<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>IMuliterno</title>
<link>https://iannimuliterno.com/index.html</link>
<atom:link href="https://iannimuliterno.com/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.2.335</generator>
<lastBuildDate>Wed, 13 Mar 2024 03:00:00 GMT</lastBuildDate>
<item>
  <title>Tech professionals vs Job interviews</title>
  <dc:creator>I. Muliterno</dc:creator>
  <link>https://iannimuliterno.com/posts/2024-03-interview_assistant/index.html</link>
  <description><![CDATA[ <p>Have you ever needed a skill at a level you didn’t expect? When I was a statistics intern, I couldn’t have imagined how important soft skills would become. Now, 8 years later, as a Senior Data Scientist, it’s crystal clear to me: the most technically proficient professional who lacks communication skills will be outperformed by someone with moderate technical expertise but who can tell a compelling story and make themselves understood without boring anyone.</p>
<section id="the-changing-landscape-of-professional-tenures" class="level3"><h3 class="anchored" data-anchor-id="the-changing-landscape-of-professional-tenures"><strong>The Changing Landscape of Professional Tenures</strong></h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="https://iannimuliterno.com/posts/2024-03-interview_assistant/genz.jpg" class="img-fluid figure-img" width="561"></p>
<p></p><figcaption class="figure-caption">Gen Z</figcaption><p></p>
</figure>
</div>
<p>The era of spending an entire career in one company is becoming a relic of the past, especially in the tech industry. Modern professionals, driven by a quest for diverse experiences and personal growth, are changing jobs more frequently than ever before. This shift is supported by data from various analyses:</p>
<ul>
<li><p>According to a turnover report from <a href="https://www.linkedin.com/business/talent/blog/talent-strategy/industries-with-the-highest-turnover-rates">Linkedin</a> the average tenure of employees in tech companies is significantly lower than in other industries, often hovering around two to three years.</p></li>
<li><p>Another <a href="https://anz.peoplemattersglobal.com/article/employee-engagement/job-loyalty-on-the-decline-are-gen-z-less-committed-to-long-term-employment-37627">report</a> highlights a growing trend among younger professionals, particularly millennials and Gen Z, who prioritize learning opportunities and workplace culture over long-term job security.</p></li>
</ul></section><section id="the-reality-of-job-interviews" class="level3"><h3 class="anchored" data-anchor-id="the-reality-of-job-interviews"><strong>The Reality of Job Interviews</strong></h3>
<p>In this dynamic career landscape, job interviews have become a critical junction in a professional’s journey. However, these interviews often come with their own set of challenges:</p>
<ul>
<li><p><strong>Short Duration</strong>: Many interviews last only about 30 minutes ( I’ve seen it last 10), providing a narrow window to make a strong impression.</p></li>
<li><p><strong>Nervousness</strong>: The pressure to perform well in such a limited time can heighten anxiety, affecting the interviewee’s ability to communicate effectively.</p></li>
<li><p><strong>The Importance of Practice</strong>: Given these constraints, practicing for interviews becomes paramount. It’s not just about rehearsing answers but also about learning to manage nerves, articulate thoughts clearly, and demonstrate those crucial soft skills.</p></li>
</ul></section><section id="the-inspiration-behind-the-interview-assistant-app" class="level3"><h3 class="anchored" data-anchor-id="the-inspiration-behind-the-interview-assistant-app"><strong>The Inspiration Behind the Interview Assistant App</strong></h3>
<p><img src="https://iannimuliterno.com/posts/2024-03-interview_assistant/app.PNG" class="img-fluid" width="677"></p>
<p>Recognizing these challenges, I was inspired to develop an [Interview Assistant app](<a href="https://ianmuliterno.shinyapps.io/interview_assistant/" class="uri">https://ianmuliterno.shinyapps.io/interview_assistant/</a>), designed to help professionals navigate the interview process more effectively. The app offers a range of features aimed at enhancing interview readiness, from simulated interview scenarios to feedback mechanisms that help users improve their performance over time. All you need to do before, if you still don’t have a bard key, is checking the recommended link in the app and generating your key ( it’s for free).</p>
</section><section id="how-the-interview-assistant-app-helps" class="level3"><h3 class="anchored" data-anchor-id="how-the-interview-assistant-app-helps"><strong>How the Interview Assistant App Helps</strong></h3>
<ul>
<li><p><strong>Simulated Interviews</strong>: Users can engage in mock interviews tailored to their industry and role, helping them practice responses to common questions.</p></li>
<li><p><strong>Feedback and Improvement</strong>: The app provides constructive feedback on users’ answers, highlighting areas for improvement and offering tips on showcasing soft skills.</p></li>
<li><p><strong>Confidence Building</strong>: By familiarizing themselves with the interview process and improving their responses, users can approach real interviews with greater confidence and poise.</p></li>
</ul></section><section id="in-summary" class="level3"><h3 class="anchored" data-anchor-id="in-summary"><strong>In Summary</strong></h3>
<ul>
<li>soft skills are as crucial as technical expertise</li>
<li>job tenures become increasingly shorter, so professionals are exposed to job interviews more often</li>
<li>The Interview Assistant app represents a step forward in empowering professionals to present their best selves during interviews, turning potential obstacles into opportunities for growth and success.</li>
</ul>


</section> ]]></description>
  <category>Data Science</category>
  <category>Shiny</category>
  <category>soft skills</category>
  <category>job</category>
  <guid>https://iannimuliterno.com/posts/2024-03-interview_assistant/index.html</guid>
  <pubDate>Wed, 13 Mar 2024 03:00:00 GMT</pubDate>
  <media:content url="https://iannimuliterno.com/posts/2024-03-interview_assistant/thumb.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Horse Race and Data Science</title>
  <dc:creator>I. Muliterno</dc:creator>
  <link>https://iannimuliterno.com/posts/2024-01-Horse_Race_and_DataScience/index.html</link>
  <description><![CDATA[ <p>Hello again my friends, I got into a very interesting project while learning how to better work with <code>BeautifulSoup</code>, and I felt like sharing with you. I saw a tutorial and it looked so easy that I decided to find a different table to extract so I could have a better challenge and ended up <a href="http://www.racebase.co.nz/jockthis.htm">here</a>. After <a href="https://github.com/IanniMuliterno/Webscrapping-lesson-with-BeautifulSoup">successfully extracting</a> the jockeys stats table, I wondered “How much people already played with horse racing data?”</p>
<p>Easy answer: not much, according to <a href="https://en.wikipedia.org/wiki/Horse_racing">wikipedia</a>, only 10 countries host horse racing, and during my research for data, I found info mostly about Australia, New Zealand, Hong Kong and Singapure races.</p>
<p>Well that means more <code>BeautifulSoup</code> practice for me, right?!</p>
<p>But before that, I need to understand the horse racing world, I’ve read so many expressions that are completely new to me. Because of that, todays post is aiming to show:</p>
<section id="what-you-need-to-know-before-going-into-a-horse-race-dataset" class="level2"><h2 class="anchored" data-anchor-id="what-you-need-to-know-before-going-into-a-horse-race-dataset"><strong>What you need to know before going into a horse race dataset</strong></h2>
<p>To effectively analyze horse race data, there are several key areas one must understand:</p>
<section id="the-jockeys" class="level3"><h3 class="anchored" data-anchor-id="the-jockeys">1. The Jockeys</h3>
<p>The jockey is much more than just a rider. Their skill, experience, and even weight play a significant role in the performance of the horse. Understanding a jockey’s track record, including wins, losses, and strategies, can provide valuable insights into future races, and at <a href="https://www.racing.com">raceing</a> I found:</p>
<p><img src="https://iannimuliterno.com/posts/2024-01-Horse_Race_and_DataScience/jockey_stats.PNG" class="img-fluid"></p>
</section><section id="the-trainers" class="level3"><h3 class="anchored" data-anchor-id="the-trainers">2. The Trainers</h3>
<p>Behind every successful horse and jockey is a trainer. Trainers are responsible for preparing the horse for the race, which includes physical training, nutrition, and strategy. Analyzing a trainer’s history, methods, and success rate is crucial for predicting how well a horse might perform.</p>
<p><img src="https://iannimuliterno.com/posts/2024-01-Horse_Race_and_DataScience/trainer_Stats.PNG" class="img-fluid"></p>
</section><section id="the-horses" class="level3"><h3 class="anchored" data-anchor-id="the-horses">3. The Horses</h3>
<p>Each horse has its unique traits - age, breed, lineage, past performance, and more. Some breeds are known for their speed, others for endurance. The lineage can also give clues about a horse’s potential. Understanding these aspects can significantly enhance the analysis of race outcomes. Also, you will see different info from the two above, that’s because each one has a <em>stats</em> and <em>form</em> tab, what you saw above was the stats tab, now lets see the other one.</p>
<p><img src="https://iannimuliterno.com/posts/2024-01-Horse_Race_and_DataScience/horse_form.PNG" class="img-fluid"></p>
</section><section id="the-tracks" class="level3"><h3 class="anchored" data-anchor-id="the-tracks">4. The Tracks</h3>
<p>Racetracks vary widely - in terms of surface (dirt, turf, synthetic), length, and weather conditions. Some horses perform better on certain surfaces or track lengths. The condition of the track on race day can also impact the results. It’s important to include these variables in any comprehensive analysis.</p>
</section><section id="race-types-and-classifications" class="level3"><h3 class="anchored" data-anchor-id="race-types-and-classifications">5. Race Types and Classifications</h3>
<p>Horse races come in different forms: flat racing, steeplechase, harness racing, etc. Within these, there are classifications based on the age and ability of the horses. Understanding the type and classification of the race is vital for contextual analysis.</p>
</section><section id="betting-odds" class="level3"><h3 class="anchored" data-anchor-id="betting-odds">6. Betting Odds</h3>
<p>While not directly related to the physical aspects of the race, betting odds are a reflection of public perception and can indicate the favorites and underdogs. These odds can sometimes offer insights into factors that may not be immediately apparent in the raw data.</p>
</section><section id="external-factors" class="level3"><h3 class="anchored" data-anchor-id="external-factors">7. External Factors</h3>
<p>Factors such as weather conditions on the day of the race, the horse’s mood and health, and even the jockey’s physical condition can influence the race outcome. While some of these are difficult to quantify, they are important to acknowledge.</p>
<p>Now that we identified the variables that can impact race results, lets get a little more specific. Because when I looked at the data I asked myself “what the heck means P.O.T? What does the rating mean in the Form tab?”</p>
<p>Well, let’s answer that, it might get a little boring of a list, but hang on, we’re almost finishing.</p>
<ul>
<li><p>CAREER: This shows the horse’s total career statistics, often in the format of starts-wins-seconds-thirds. For example, “27-10-4-4” would mean the horse has started 27 races, winning 10, finishing second 4 times, and third 4 times. The same idea goes for Season</p></li>
<li><p>GRP &amp; LISTED: This column shows the horse’s record in Group and Listed races, which are higher-class races. “7-2-1-0” means the horse has entered 7 such races, won 2, came second once, and has not placed third.</p></li>
<li><p>1ST UP: The horse’s performance when racing for the first time in a new racing season or after a break. “6-1-1-3” indicates 6 first-up starts, with 1 win, 1 second, and 3 third-place finishes. Same idea goes for 2ND UP and 3RD UP.</p></li>
<li><p>FIRM/GOOD/SOFT/HEAVY: These columns indicate the horse’s performance on different track conditions. For example, “17-7-1-4” under GOOD suggests the horse has raced 17 times on good rated tracks, with 7 wins, 1 second, and 4 thirds.</p></li>
<li><p>JUMPS: Shows the horse’s performance in jump races (if applicable), which are races involving hurdles or fences.</p></li>
<li><p>SYNTH: The horse’s record on synthetic tracks, which are artificial surfaces.</p></li>
<li><p>POS: The position the horse finished in the race.</p></li>
<li><p>HORSE/TRAINER/JOCKEY: Names of the horse, the trainer(s), and the jockey.</p></li>
<li><p>PRIZE/CLASS: The total prize money and the class of the race (e.g., G1 for Group 1, which is the highest level of race).</p></li>
<li><p>WGT: The weight carried by the horse, including the jockey and gear.</p></li>
<li><p>800M/400M: The horse’s position at 800 meters and 400 meters from the finish line.</p></li>
<li><p>MARGIN: How far the horse was from the winner, in lengths.</p></li>
<li><p>RATING: A handicap rating assigned to a horse based on its performance. It’s a number that allows horses of different abilities to compete on equal terms, with higher-rated horses carrying more weight. The exact method of calculation can vary by racing jurisdiction but generally involves the horse’s past performance, the quality of the races it has run in, and the margins of victory or defeat.</p></li>
<li><p>ODDS: The betting odds for the horse at the start of the race.</p></li>
<li><p>COM: Commentary or notes about the horse’s performance in the race.</p></li>
</ul>
<p>There three more that I found interesting:</p>
<ul>
<li><p>P.O.T: This stands for “Profit on Turnover.” It is a measure used by bettors to calculate the profitability of their bets. For a jockey, it would typically refer to the profitability of betting on all the races that the jockey has ridden. It is calculated by taking the total returns from bets on a jockey’s rides, subtracting the total amount bet, and then dividing by the total amount bet. This is often expressed as a percentage. A positive percentage means a profit, while a negative percentage indicates a loss.</p></li>
<li><p>S Rate: This is usually shorthand for “Strike Rate.” In horse racing, a jockey’s strike rate is the percentage of races won out of the total number of races ridden. A higher strike rate indicates a jockey who wins a larger proportion of their races. It’s calculated as (Number of Wins / Total Rides) x 100.</p></li>
<li><p>U.D.R: This typically stands for “Universal Driver Rating” or sometimes “Universal Jockey Rating,” depending on the context. It’s a handicapping tool used primarily in harness racing but can be applied in thoroughbred racing to assess a driver’s or jockey’s performance. The U.D.R. is calculated by a formula that takes into account wins, seconds, and thirds, giving more weight to wins. The formula is ([Wins x 9] + [Places x 5] + [Shows x 3]) / (Total Drives x 9). This gives a figure between 0 and 1, which can be interpreted as a batting average. The closer to 1, the better the performance.</p></li>
</ul>
<p>I’ll explore the main areas Jockey, Trainer, Horse and Track, sharing my findings and the challenges I encounter at my github (remember I left the link at the start). This exploration is not just about understanding data; it’s about understanding the sport of horse racing itself. So, stay tuned as we dive deeper into this fascinating world!</p>


</section></section> ]]></description>
  <category>Data Science</category>
  <category>R</category>
  <category>sports analytics</category>
  <category>race</category>
  <guid>https://iannimuliterno.com/posts/2024-01-Horse_Race_and_DataScience/index.html</guid>
  <pubDate>Thu, 25 Jan 2024 03:00:00 GMT</pubDate>
  <media:content url="https://iannimuliterno.com/posts/2024-01-Horse_Race_and_DataScience/horserace.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Text mining on Harry Potter dialogues</title>
  <dc:creator>I. Muliterno</dc:creator>
  <link>https://iannimuliterno.com/posts/2023-11-textmining/index.html</link>
  <description><![CDATA[ <p>Welcome to our latest exploration in the world of data science. Following our previous discussions on machine learning models, we now delve into the fascinating realm of text analysis using R. In this post, we’ll apply various R packages to analyze character dialogues from a fictional dataset. Our goal is to uncover insights such as the most mentioned characters, frequent greetings, and the characteristics of dialogues. Let’s dive into the script and interpret our findings.</p>
<p><strong>Understanding the Dataset</strong></p>
<p>We have a JSON file containing dialogues from different sessions, aquired <a href="https://nuochenpku.github.io/HPD.github.io/index.html">here</a> . To extract meaningful insights, we first load and process the data using <strong><code>tidyverse</code></strong> and <strong><code>tidytext</code></strong> packages in R. The dataset is a treasure trove of dialogues, each offering a unique glimpse into the interactions between characters.</p>
<p><strong>Data Preparation and Exploration</strong> With our data loaded, we proceed to clean and structure it for analysis. We utilize functions like <strong><code>str_split_fixed</code></strong> to separate the dialogues into character names and their corresponding lines. This meticulous process sets the stage for deeper analysis, allowing us to delve into the intricacies of each dialogue.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va" style="color: #111111;">hpd</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;">fromJSON</span><span class="op" style="color: #5E5E5E;">(</span>file<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"en_train_set.json"</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span></span>
<span><span class="va" style="color: #111111;">extracted_dialogue</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;">map</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">hpd</span>, <span class="va" style="color: #111111;">pluck</span>, <span class="st" style="color: #20794D;">"dialogue"</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span><span class="va" style="color: #111111;">session_names</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">extracted_dialogue</span><span class="op" style="color: #5E5E5E;">)</span>, </span>
<span>                     times <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">extracted_dialogue</span>, <span class="va" style="color: #111111;">length</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span></span>
<span> <span class="va" style="color: #111111;">dialog_tb</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;">str_split_fixed</span><span class="op" style="color: #5E5E5E;">(</span>string <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;">tibble</span><span class="op" style="color: #5E5E5E;">(</span></span>
<span>   dialogue <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/unlist.html">unlist</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">extracted_dialogue</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span> <span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">$</span><span class="va" style="color: #111111;">dialogue</span>,</span>
<span> pattern <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">':'</span>,n<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">2</span><span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">|&gt;</span> </span>
<span>   <span class="fu" style="color: #4758AB;">as_tibble</span><span class="op" style="color: #5E5E5E;">(</span><span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">|&gt;</span> </span>
<span>   <span class="fu" style="color: #4758AB;">mutate</span><span class="op" style="color: #5E5E5E;">(</span>session <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">session_names</span>,</span>
<span>          V1 <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;">str_trim</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">V1</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">|&gt;</span> </span>
<span>   <span class="fu" style="color: #4758AB;">select</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">session</span>, charac <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">V1</span>, dialog <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">V2</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
</div>
<p><strong>Insights from the Data</strong></p>
<ol type="1">
<li>
<p><strong>Most Mentioned Characters</strong>: We quantify the presence of each character in the dialogues. By counting mentions, we identify the characters that dominate the conversations, offering insights into their importance or prominence in the narrative.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span> <span class="va" style="color: #111111;">character_mentions</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/unique.html">unique</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">dialog_tb</span><span class="op" style="color: #5E5E5E;">$</span><span class="va" style="color: #111111;">charac</span><span class="op" style="color: #5E5E5E;">)</span>, <span class="kw" style="color: #003B4F;">function</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">char</span><span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span>   <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="fu" style="color: #4758AB;">str_detect</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">dialog_tb</span><span class="op" style="color: #5E5E5E;">$</span><span class="va" style="color: #111111;">dialog</span>, <span class="fu" style="color: #4758AB;">fixed</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">char</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span> <span class="op" style="color: #5E5E5E;">}</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span></span>
<span> <span class="co" style="color: #5E5E5E;"># Creating a data frame for the results</span></span>
<span> <span class="va" style="color: #111111;">mentions_df</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op" style="color: #5E5E5E;">(</span>charac <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/unique.html">unique</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">dialog_tb</span><span class="op" style="color: #5E5E5E;">$</span><span class="va" style="color: #111111;">charac</span><span class="op" style="color: #5E5E5E;">)</span>,</span>
<span>                           mentions <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">character_mentions</span><span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">|&gt;</span> </span>
<span>   <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">charac</span> <span class="op" style="color: #5E5E5E;">!=</span> <span class="st" style="color: #20794D;">"hat"</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span></span>
<span> <span class="co" style="color: #5E5E5E;"># Displaying the results</span></span>
<span> <span class="va" style="color: #111111;">mentions_df</span> <span class="op" style="color: #5E5E5E;">|&gt;</span> </span>
<span>   <span class="fu" style="color: #4758AB;">arrange</span><span class="op" style="color: #5E5E5E;">(</span><span class="fu" style="color: #4758AB;">desc</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">mentions</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">|&gt;</span> </span>
<span>   <span class="fu" style="color: #4758AB;">slice</span><span class="op" style="color: #5E5E5E;">(</span><span class="fl" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">:</span><span class="fl" style="color: #AD0000;">20</span><span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">|&gt;</span> </span>
<span>   <span class="fu" style="color: #4758AB;">ggplot</span><span class="op" style="color: #5E5E5E;">(</span><span class="fu" style="color: #4758AB;">aes</span><span class="op" style="color: #5E5E5E;">(</span>y <span class="op" style="color: #5E5E5E;">=</span>  <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/stats/reorder.factor.html">reorder</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">charac</span>, <span class="op" style="color: #5E5E5E;">-</span><span class="va" style="color: #111111;">mentions</span><span class="op" style="color: #5E5E5E;">)</span>, x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">mentions</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">+</span></span>
<span>   <span class="fu" style="color: #4758AB;">geom_bar</span><span class="op" style="color: #5E5E5E;">(</span>stat <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'identity'</span><span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">+</span></span>
<span>   <span class="fu" style="color: #4758AB;">ggtitle</span><span class="op" style="color: #5E5E5E;">(</span><span class="st" style="color: #20794D;">'Character mentions'</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="https://iannimuliterno.com/posts/2023-11-textmining/index_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</li>
<li>
<p><strong>Frequent Greetings</strong>: The essence of initial interactions is captured by analyzing common greetings like “Hello”, “Hi”, and others. This reveals the nature of interactions and the formality or informality within the dialogues.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va" style="color: #111111;">greetings</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="st" style="color: #20794D;">"Hello"</span>, <span class="st" style="color: #20794D;">"Hi"</span>, <span class="st" style="color: #20794D;">"Greetings"</span>, <span class="st" style="color: #20794D;">"Hey"</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span></span>
<span><span class="co" style="color: #5E5E5E;"># Extracting greetings from dialogues</span></span>
<span><span class="va" style="color: #111111;">greetings_found</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">greetings</span>, <span class="kw" style="color: #003B4F;">function</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">greet</span><span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">{</span></span>
<span>    <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/unlist.html">unlist</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="fu" style="color: #4758AB;">str_extract_all</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">dialog_tb</span><span class="op" style="color: #5E5E5E;">$</span><span class="va" style="color: #111111;">dialog</span>, <span class="fu" style="color: #4758AB;">fixed</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">greet</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span><span class="op" style="color: #5E5E5E;">}</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span></span>
<span><span class="co" style="color: #5E5E5E;"># Displaying the results</span></span>
<span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">greetings_found</span>,<span class="va" style="color: #111111;">length</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$Hello
[1] 41

$Hi
[1] 147

$Greetings
[1] 0

$Hey
[1] 41</code></pre>
</div>
</div>
</li>
<li>
<p><strong>What’s the longest dialogue?</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span> <span class="co" style="color: #5E5E5E;"># Calculating the length of each dialogue</span></span>
<span> <span class="va" style="color: #111111;">dialog_tb</span><span class="op" style="color: #5E5E5E;">$</span><span class="va" style="color: #111111;">length</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;">str_length</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">dialog_tb</span><span class="op" style="color: #5E5E5E;">$</span><span class="va" style="color: #111111;">dialog</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span></span>
<span> <span class="co" style="color: #5E5E5E;"># Identifying the longest dialogue</span></span>
<span> <span class="va" style="color: #111111;">longest_dialogue</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="va" style="color: #111111;">dialog_tb</span> <span class="op" style="color: #5E5E5E;">%&gt;%</span> </span>
<span>   <span class="fu" style="color: #4758AB;">arrange</span><span class="op" style="color: #5E5E5E;">(</span><span class="fu" style="color: #4758AB;">desc</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">length</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span> <span class="op" style="color: #5E5E5E;">%&gt;%</span></span>
<span>   <span class="fu" style="color: #4758AB;">slice</span><span class="op" style="color: #5E5E5E;">(</span><span class="fl" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span></span>
<span> <span class="co" style="color: #5E5E5E;"># Displaying the result</span></span>
<span> <span class="va" style="color: #111111;">longest_dialogue</span><span class="op" style="color: #5E5E5E;">$</span><span class="va" style="color: #111111;">dialog</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] " Then you should, You know the secret of my sister’s ill health, what those Muggles did, what she became. You know how my poor father sought revenge, and paid the price, died in Azkaban. You know how my mother gave up her own life to care for Ariana. Harry. I was gifted, I was brilliant. I wanted to escape. I wanted to shine. I wanted glory. Do not misunderstand me, I loved them. I loved my parents, I loved my brother and my sister, but I was selfish, Harry, more selfish than you, who are a remarkably selfless person, could possibly imagine. So that, when my mother died, and I was left the responsibility of a damaged sister and a wayward brother, I returned to my village in anger and bitterness. Trapped and wasted, I thought! And then, of course, he came. . . . Grindelwald. You cannot imagine how his ideas caught me, Harry, inflamed me. Muggles forced into subservience. We wizards triumphant. Grindelwald and I, the glorious young leaders of the revolution. Oh, I had a few scruples. I assuaged my conscience with empty words. It would all be for the greater good, and any harm done would be repaid a hundredfold in benefits for wizards. Did I know, in my heart of hearts, what Gellert Grindelwald was? I think I did, but I closed my eyes. If the plans we were making came to fruition, all my dreams would come true. And at the heart of our schemes, the Deathly Hallows! How they fascinated him, how they fascinated both of us! The unbeatable wand, the weapon that would lead us to power! The Resurrection Stone — to him, though I pretended not to know it, it meant an army of Inferi! To me, I confess, it meant the return of my parents, and the lifting of all responsibility from my shoulders. Harry. I thought that, if we ever found it, it might be useful in hiding Ariana, but our interest in the Cloak was mainly that it completed the trio, for the legend said that the man who united all three objects would then be truly master of death, which we took to mean ‘invincible. ’ Invincible masters of death, Grindelwald and Dumbledore! Two months of insanity, of cruel dreams, and neglect of the only two members of my family left to me. You know what happened. Reality returned in the form of my rough, unlettered, and infinitely more admirable brother. I did not want to hear the truths he shouted at me. I did not want to hear that I could not set forth to seek Hallows with a fragile and unstable sister in tow. The argument became a fight. Grindelwald lost control. That which I had always sensed in him, though I pretended not to, now sprang into terrible being.  And Ariana .. . after all my mother’s care and caution . . . lay dead upon the floor. Well, Grindelwald fled, as anyone but I could have predicted. He vanished, with his plans for seizing power, and his schemes for Muggle torture, and his dreams of the Deathly Hallows, dreams in which I had encouraged him and helped him. He ran, while I was left to bury my sister, and learn to live with my guilt and my terrible grief, the price of my shame. Years passed. There were rumors about him. They said he had procured a wand of immense power. I, meanwhile, was offered the post of Minister of Magic, not once, but several times. Naturally, I refused. I had learned that I was not to be trusted with power."</code></pre>
</div>
</div>
</li>
<li>
<p><strong>Dialogue Dynamics</strong>: We explore the dialogues’ nature by identifying the frequency of questions and the length of dialogues. It tells us about the conversational style and the depth of discussions among characters.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co" style="color: #5E5E5E;"># Counting dialogues that contain questions</span></span>
<span> <span class="va" style="color: #111111;">question_dialogues_count</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="fu" style="color: #4758AB;">str_detect</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">dialog_tb</span><span class="op" style="color: #5E5E5E;">$</span><span class="va" style="color: #111111;">dialog</span>, <span class="fu" style="color: #4758AB;">fixed</span><span class="op" style="color: #5E5E5E;">(</span><span class="st" style="color: #20794D;">"?"</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span></span>
<span> <span class="co" style="color: #5E5E5E;"># Displaying the count</span></span>
<span>  <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op" style="color: #5E5E5E;">(</span> <span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">question_dialogues_count</span><span class="op" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">dialog_tb</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">*</span><span class="fl" style="color: #AD0000;">100</span>, <span class="st" style="color: #20794D;">"% of all dialogues are questions"</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>35.38782 % of all dialogues are questions</code></pre>
</div>
</div>
</li>
<li>
<p><strong>Comparative Analysis</strong>: A fun aspect is comparing how often different characters mention specific terms, like how often Ron mentions “Harry” compared to Hermione. It adds a layer of relational dynamics to our analysis.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span> <span class="co" style="color: #5E5E5E;"># Count ron's mentions of "harry"</span></span>
<span> <span class="va" style="color: #111111;">ron_mentions_potter</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="fu" style="color: #4758AB;">str_detect</span><span class="op" style="color: #5E5E5E;">(</span></span>
<span>   <span class="va" style="color: #111111;">dialog_tb</span><span class="op" style="color: #5E5E5E;">$</span><span class="va" style="color: #111111;">dialog</span><span class="op" style="color: #5E5E5E;">[</span><span class="va" style="color: #111111;">dialog_tb</span><span class="op" style="color: #5E5E5E;">$</span><span class="va" style="color: #111111;">charac</span> <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"Ron"</span><span class="op" style="color: #5E5E5E;">]</span>, <span class="st" style="color: #20794D;">"Harry"</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span></span>
<span> <span class="co" style="color: #5E5E5E;"># Count Hermione's mentions of "Harry"</span></span>
<span> <span class="va" style="color: #111111;">hermione_mentions_harry</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="fu" style="color: #4758AB;">str_detect</span><span class="op" style="color: #5E5E5E;">(</span></span>
<span>   <span class="va" style="color: #111111;">dialog_tb</span><span class="op" style="color: #5E5E5E;">$</span><span class="va" style="color: #111111;">dialog</span><span class="op" style="color: #5E5E5E;">[</span><span class="va" style="color: #111111;">dialog_tb</span><span class="op" style="color: #5E5E5E;">$</span><span class="va" style="color: #111111;">charac</span> <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"Hermione"</span><span class="op" style="color: #5E5E5E;">]</span>, <span class="st" style="color: #20794D;">"Harry"</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span></span>
<span> <span class="co" style="color: #5E5E5E;"># Displaying the results</span></span>
<span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="st" style="color: #20794D;">"Ron mentions 'Harry':"</span>, <span class="va" style="color: #111111;">ron_mentions_potter</span>, <span class="st" style="color: #20794D;">"times\n"</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Ron mentions 'Harry': 149 times</code></pre>
</div>
<div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="st" style="color: #20794D;">"Hermione mentions 'Harry':"</span>, <span class="va" style="color: #111111;">hermione_mentions_harry</span>, <span class="st" style="color: #20794D;">"times"</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Hermione mentions 'Harry': 390 times</code></pre>
</div>
</div>
</li>
</ol>
<p><strong>Visual Representation and Conclusion</strong> Our findings are not just about numbers and text; we bring them to life through visualizations like bar graphs and tables. These visual aids help us to quickly grasp the essence of our analysis, making the data more accessible and comprehensible.</p>
<p>Stay tuned</p>



 ]]></description>
  <category>Data Science</category>
  <category>R</category>
  <category>Harry Potter</category>
  <category>text mining</category>
  <guid>https://iannimuliterno.com/posts/2023-11-textmining/index.html</guid>
  <pubDate>Mon, 20 Nov 2023 03:00:00 GMT</pubDate>
  <media:content url="https://iannimuliterno.com/posts/2023-11-textmining/hpd.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Model 3 - Random Forest in R with Breast Cancer Wisconsin (Diagnostic) dataset</title>
  <dc:creator>I. Muliterno</dc:creator>
  <link>https://iannimuliterno.com/posts/2023-04-daily_life_ds/index.html</link>
  <description><![CDATA[ <p>Welcome to the third instalment in our series of posts designed to introduce statistical and machine learning models. Having explored Linear Models (LM) and Decision Trees in our previous posts, we are now moving on to Random Forests.</p>
<p>In this tutorial, we will apply a Random Forest model to the Breast Cancer Wisconsin (Diagnostic) dataset. The aim is to predict whether a breast mass is malignant or benign based on measurements from cell nuclei. Let’s get started with a brief description of each attribute:</p>
<ol type="1">
<li><p><strong>Id</strong>: Each instance’s unique identifier.</p></li>
<li><p><strong>Cl.thickness</strong>: Clump thickness describes the overall thickness of the clumps. When breast cells are healthy, they group together (or “clump”) in a specific way. As cancer starts to develop, the structure starts to break down and the cells form thicker, irregular clumps.</p></li>
<li><p><strong>Cell.size</strong>: Uniformity of cell size is a measure of how similar the cells are in size. In healthy tissues, the cells tend to be of similar size. If there’s a lot of variation in cell size, it’s an indication that something may be wrong.</p></li>
<li><p><strong>Cell.shape</strong>: Uniformity of cell shape is a measure of how similarly shaped the cells are. Healthy cells have a uniform shape, while cells from malignant tumours can vary more in shape.</p></li>
<li><p><strong>Marg.adhesion</strong>: Marginal adhesion refers to how the cells stick to each other and to the tissue in which they’re located. In healthy tissues, cells have a certain level of adhesion that helps them stay together. In cancerous tissues, the cells often lose this adhesion, allowing them to invade other tissues.</p></li>
<li><p><strong>Epith.c.size</strong>: Epithelial cell size measures the size of the cells in the epithelial tissue. Epithelial tissues line the outside of organs and structures in the body. They also line the inside of hollow organs, glands, and cavities. Variations in cell size can indicate cancerous changes.</p></li>
<li><p><strong>Bare.nuclei</strong>: The bare nuclei variable refers to nuclei that are not surrounded by cytoplasm (the material within a living cell, excluding the nucleus). The presence of bare nuclei can be a sign of a malignant growth.</p></li>
<li><p><strong>Bl.cromatin</strong>: Bland chromatin describes the uniform “texture” of the nuclei in the cells when viewed under a microscope. Cancerous cells often have a more varied, “coarser” texture.</p></li>
<li><p><strong>Normal.nucleoli</strong>: Nucleoli are small structures seen in the nucleus. Typically, healthy cells contain only one or two nucleoli. An increased number or a larger size of nucleoli can be a sign of cancer cells.</p></li>
<li><p><strong>Mitoses</strong>: Mitoses is the process by which a cell divides into two daughter cells. The rate of mitosis in a cell population can indicate how quickly the cells are dividing and growing. A high mitosis rate may suggest a malignant growth.</p></li>
<li><p><strong>Class</strong>: The diagnosis of breast tissues (2 for benign, 4 for malignant).</p></li>
</ol>
<section id="exploratory-data-analysis-eda" class="level3"><h3 class="anchored" data-anchor-id="exploratory-data-analysis-eda"><strong>Exploratory Data Analysis (EDA)</strong></h3>
<p>The first step in any data analysis project is to understand the dataset. Let’s load the dataset and examine the structure:</p>
<p>This dataset includes several numerical attributes related to cell nuclei. Additionally, we have the categorical target variable <strong><code>Class</code></strong>, with two levels: ‘benign’ and ‘malignant’.</p>
</section><section id="checking-for-missing-values-and-input-problems" class="level3"><h3 class="anchored" data-anchor-id="checking-for-missing-values-and-input-problems"><strong>Checking for Missing Values and input problems</strong></h3>
<p>Next, we’ll check for missing values. In R, we use the <strong><code><a href="https://rdrr.io/r/base/NA.html">is.na()</a></code></strong> function. To check for input problems we’ll use a boxplot</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">BreastCancer</span>,<span class="kw" style="color: #003B4F;">function</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">x</span><span class="op" style="color: #5E5E5E;">)</span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">x</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>             Id    Cl.thickness       Cell.size      Cell.shape   Marg.adhesion 
              0               0               0               0               0 
   Epith.c.size     Bare.nuclei     Bl.cromatin Normal.nucleoli         Mitoses 
              0              16               0               0               0 
          Class 
              0 </code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/graphics/boxplot.html">boxplot</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">BreastCancer</span><span class="op" style="color: #5E5E5E;">[</span>, <span class="op" style="color: #5E5E5E;">-</span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="fl" style="color: #AD0000;">1</span>,<span class="fl" style="color: #AD0000;">11</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">]</span>, las <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">2</span>, notch <span class="op" style="color: #5E5E5E;">=</span> <span class="cn" style="color: #8f5902;">F</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="https://iannimuliterno.com/posts/2023-04-daily_life_ds/index_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Those 16 NA values come from Bare.nuclei, so we have few NA observations and those happen to have all the other variables properly filled. Knowing random forest can handle NA, that must not be a problem. Another thing that called my attention here was Mitoses, most is equal to 1, so maybe it’s a variable strongly related by the type of tumor, meaning that anything greater than 1 represents a malignant tumor, that’s my hypothesis, we’re going to check that out.</p>
<p><strong>Relationship with the Target Variable</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu" style="color: #4758AB;">ggpairs</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">BreastCancer</span><span class="op" style="color: #5E5E5E;">[</span>,<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">]</span> <span class="op" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">dplyr</span><span class="fu" style="color: #4758AB;">::</span><span class="fu" style="color: #4758AB;"><a href="https://dplyr.tidyverse.org/reference/mutate_all.html">mutate_all</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">as.numeric</span><span class="op" style="color: #5E5E5E;">)</span>, mapping <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;">ggplot2</span><span class="fu" style="color: #4758AB;">::</span><span class="fu" style="color: #4758AB;"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op" style="color: #5E5E5E;">(</span>colour <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">Class</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span>, progress <span class="op" style="color: #5E5E5E;">=</span> <span class="cn" style="color: #8f5902;">FALSE</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="https://iannimuliterno.com/posts/2023-04-daily_life_ds/index_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Here we see that for all the variables, bigger values are associated with malignant tumor, which confirms my hypothesis about mitosis behavior. Take into account that I converted all variables to numeric in order to have a better looking graph (try to replicate the function without converting to numeric and see what I am talking about). I did that because I know the numbers are monotonically related, in other words, I know that cell.size 1 is smaller than cell.size 2 and their difference is as big as cell size 5 versus cell size 4 for example. I say that because when a column is a factor but it’s filled with numbers, the relation between the numbers can be unexpected, for example the effects of drugs dosage. For certain medications, increasing the dose will increase the effect up to a certain point. However, beyond that point, increasing the dose may lead to harmful side effects without increasing the intended effect of the drug.</p>
</section><section id="checking-for-multicollinearity" class="level3"><h3 class="anchored" data-anchor-id="checking-for-multicollinearity"><strong>Checking for Multicollinearity</strong></h3>
<p>Next, we need to check for multicollinearity, which is when predictor variables are highly correlated with each other. This can be a problem because it undermines the statistical significance of an independent variable. We use the <strong><code>vif()</code></strong> function from the ‘car’ package to check for this.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co" style="color: #5E5E5E;"># Compute variance inflation factors (VIF)</span></span>
<span><span class="fu" style="color: #4758AB;">vif</span><span class="op" style="color: #5E5E5E;">(</span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">Class</span> <span class="op" style="color: #5E5E5E;">~</span> <span class="va" style="color: #111111;">.</span>, data <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">BreastCancer</span><span class="op" style="color: #5E5E5E;">[</span>, <span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">]</span><span class="op" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">dplyr</span><span class="fu" style="color: #4758AB;">::</span><span class="fu" style="color: #4758AB;"><a href="https://dplyr.tidyverse.org/reference/mutate_all.html">mutate_all</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">as.numeric</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   Cl.thickness       Cell.size      Cell.shape   Marg.adhesion    Epith.c.size 
       1.905223        7.194043        6.549892        2.466854        2.550007 
    Bare.nuclei     Bl.cromatin Normal.nucleoli         Mitoses 
       2.595167        2.876814        2.430306        1.400595 </code></pre>
</div>
</div>
<p>A VIF value above 5 is usually considered high and indicative of multicollinearity. Cell shape and size show high VIF, they are correlated and we could benefit from excluding one of them. I would do that by keeping the one which is stronger on the model result, here as we have a small dataset and our goal is to get familiarized with random forest, I am not going to do that.</p>
</section><section id="building-our-random-forest-model" class="level2"><h2 class="anchored" data-anchor-id="building-our-random-forest-model"><strong>Building Our Random Forest Model</strong></h2>
<p>Now that we’ve got a thorough understanding of our data, let’s move forward and build our Random Forest model. In this step, we will use the <strong><code>randomForest</code></strong> package in R.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co" style="color: #5E5E5E;"># Set seed for reproducibility</span></span>
<span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="fl" style="color: #AD0000;">123</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span></span>
<span><span class="co" style="color: #5E5E5E;"># Split the data into training and test sets</span></span>
<span><span class="va" style="color: #111111;">training_rows</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="fl" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">:</span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">BreastCancer</span><span class="op" style="color: #5E5E5E;">)</span>, <span class="fl" style="color: #AD0000;">0.7</span><span class="op" style="color: #5E5E5E;">*</span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">BreastCancer</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span><span class="va" style="color: #111111;">training_set</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="va" style="color: #111111;">BreastCancer</span><span class="op" style="color: #5E5E5E;">[</span><span class="va" style="color: #111111;">training_rows</span>, <span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">]</span></span>
<span><span class="va" style="color: #111111;">test_set</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="va" style="color: #111111;">BreastCancer</span><span class="op" style="color: #5E5E5E;">[</span><span class="op" style="color: #5E5E5E;">-</span><span class="va" style="color: #111111;">training_rows</span>,<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">1</span> <span class="op" style="color: #5E5E5E;">]</span></span>
<span></span>
<span><span class="co" style="color: #5E5E5E;"># Train the model</span></span>
<span> <span class="va" style="color: #111111;">rf_model</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;">cforest</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">Class</span> <span class="op" style="color: #5E5E5E;">~</span> <span class="va" style="color: #111111;">.</span>, data <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">training_set</span>, controls<span class="op" style="color: #5E5E5E;">=</span><span class="fu" style="color: #4758AB;">cforest_unbiased</span><span class="op" style="color: #5E5E5E;">(</span>ntree<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">5</span>, mtry<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">3</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span></span>
<span><span class="co" style="color: #5E5E5E;"># Print the model</span></span>
<span><span class="va" style="color: #111111;">rf_model</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
     Random Forest using Conditional Inference Trees

Number of trees:  5 

Response:  Class 
Inputs:  Cl.thickness, Cell.size, Cell.shape, Marg.adhesion, Epith.c.size, Bare.nuclei, Bl.cromatin, Normal.nucleoli, Mitoses 
Number of observations:  489 </code></pre>
</div>
</div>
<p>The <strong><code>randomForest()</code></strong> function builds the Random Forest model, with the formula <strong><code>Class ~ .</code></strong> telling it to use all other variables to predict <strong><code>Class</code></strong>. The <strong><code>ntree</code></strong> argument is the number of trees to grow in the forest - in this case, 10. The <strong><code>importance</code></strong> argument, when set to <strong><code>TRUE</code></strong>, computes a measure of variable importance.</p>
<p>Now, let’s evaluate our model’s performance on the test data.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co" style="color: #5E5E5E;"># Make predictions on the test data</span></span>
<span><span class="va" style="color: #111111;">predictions</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">rf_model</span>, newdata <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">test_set</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span></span>
<span><span class="co" style="color: #5E5E5E;"># Print confusion matrix</span></span>
<span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op" style="color: #5E5E5E;">(</span>observed <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">test_set</span><span class="op" style="color: #5E5E5E;">$</span><span class="va" style="color: #111111;">Class</span>, predicted <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">predictions</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           predicted
observed    benign malignant
  benign       143         7
  malignant      0        60</code></pre>
</div>
</div>
<p>The confusion matrix gives us a detailed breakdown of the model’s performance. The diagonal elements represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabelled by the classifier.</p>
<p>In addition to a confusion matrix, there are several other metrics and methods that can provide a more detailed understanding of a model’s performance. These include:</p>
<ol type="1">
<li>
<strong>ROC Curve and AUC</strong>: Receiver Operating Characteristic (ROC) curve is a plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold is varied. Area Under the Curve (AUC) provides an aggregate measure of performance across all possible classification thresholds. An AUC of 1 indicates a perfect classifier.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va" style="color: #111111;">pred_prob</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">rf_model</span>, newdata <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">test_set</span>, type <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"prob"</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span><span class="va" style="color: #111111;">pred_prob</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">pred_prob</span>, <span class="va" style="color: #111111;">`[[`</span>, <span class="fl" style="color: #AD0000;">2</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span><span class="va" style="color: #111111;">roc_obj</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;">roc</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">test_set</span><span class="op" style="color: #5E5E5E;">$</span><span class="va" style="color: #111111;">Class</span>, <span class="va" style="color: #111111;">pred_prob</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Setting levels: control = benign, case = malignant</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Setting direction: controls &lt; cases</code></pre>
</div>
<div class="sourceCode" id="cb14" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="st" style="color: #20794D;">"AUC: "</span>, <span class="fu" style="color: #4758AB;">auc</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">roc_obj</span><span class="op" style="color: #5E5E5E;">)</span>, <span class="st" style="color: #20794D;">"\n"</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>AUC:  0.9866667 </code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb16" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">roc_obj</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="https://iannimuliterno.com/posts/2023-04-daily_life_ds/index_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<ol start="2" type="1">
<li>
<strong>Cross-validation</strong>: This technique provides a better assessment of the model’s expected performance on unseen data by reducing the variance associated with a single train-test split.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb17" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co" style="color: #5E5E5E;"># Load necessary library</span></span>
<span><span class="kw" style="color: #003B4F;"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;"><a href="https://github.com/topepo/caret/">caret</a></span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Carregando pacotes exigidos: lattice</code></pre>
</div>
<div class="sourceCode" id="cb19" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co" style="color: #5E5E5E;"># Define training control</span></span>
<span><span class="va" style="color: #111111;">train_control</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;">trainControl</span><span class="op" style="color: #5E5E5E;">(</span>method <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"cv"</span>, number <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">10</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span></span>
<span><span class="co" style="color: #5E5E5E;"># Train the model using cross-validation</span></span>
<span><span class="va" style="color: #111111;">rf_model2</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;">train</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">Class</span> <span class="op" style="color: #5E5E5E;">~</span> <span class="va" style="color: #111111;">.</span>, data <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">BreastCancer</span><span class="op" style="color: #5E5E5E;">[</span>,<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">]</span> <span class="op" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/stats/na.fail.html">na.omit</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="op" style="color: #5E5E5E;">)</span>, trControl <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">train_control</span>, method <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"rf"</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span></span>
<span><span class="co" style="color: #5E5E5E;"># Print the model</span></span>
<span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">rf_model2</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Random Forest 

683 samples
  9 predictor
  2 classes: 'benign', 'malignant' 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 615, 615, 614, 614, 615, 615, ... 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa    
   2    0.9647679  0.9232216
  41    0.9633186  0.9200402
  80    0.9574576  0.9065592

Accuracy was used to select the optimal model using the largest value.
The final value used for the model was mtry = 2.</code></pre>
</div>
</div>
<p>Lastly, we can inspect the importance of each feature in making predictions.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb21" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co" style="color: #5E5E5E;"># Get variable importance</span></span>
<span><span class="fu" style="color: #4758AB;">varimp</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">rf_model</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   Cl.thickness       Cell.size      Cell.shape   Marg.adhesion    Epith.c.size 
    0.000000000     0.054748603     0.005586592     0.000000000     0.011173184 
    Bare.nuclei     Bl.cromatin Normal.nucleoli         Mitoses 
    0.026815642     0.079329609     0.016759777     0.000000000 </code></pre>
</div>
</div>
<p>Each row of the output corresponds to one of the predictor variables. The higher the value, the more important the predictor variable in classifying the tumour as benign or malignant.</p>
<p>Variable importance gives us insight into which features are most influential in predicting the target variable in our Random Forest model. It’s an important tool for understanding and interpreting the model. So cell size and shape are the strongest variables for prediction.</p>
<p>And there we have it - a complete Random Forest model in R, built on the Breast Cancer Wisconsin (Diagnostic) dataset. In this tutorial, we’ve covered a lot of ground, from the initial exploratory data analysis through to the creation and evaluation of our model. We hope you’ve found it helpful and enlightening!</p>
<p>In our upcoming posts, we’ll dive into more sophisticated machine learning algorithms and their applications. Stay tuned!</p>


</section> ]]></description>
  <category>Data Science</category>
  <category>R</category>
  <category>random forest</category>
  <category>Machine Learning</category>
  <guid>https://iannimuliterno.com/posts/2023-04-daily_life_ds/index.html</guid>
  <pubDate>Sun, 28 May 2023 03:00:00 GMT</pubDate>
  <media:content url="https://iannimuliterno.com/posts/2023-04-daily_life_ds/dataset-cover.png" medium="image" type="image/png"/>
</item>
<item>
  <title>DDC: Tackling Missing or Inconsistent Data</title>
  <dc:creator>I. Muliterno</dc:creator>
  <link>https://iannimuliterno.com/posts/2023-04-daily_life_ds2/index.html</link>
  <description><![CDATA[ <p>Hello and welcome to the Data Driven Chronicles (DDC) series, where I share small doses of my life as a data professional. Today we will talk about a common task: dealing with missing or inconsistent data.</p>
<p>In the world of data science, dealing with missing or inconsistent data is an everyday challenge. The quality of your insights, predictions, and models heavily depends on the quality of the data you use. In this second post of our series on data science daily life challenges, we’ll explore various strategies for handling missing or inconsistent data using R, and how to make informed decisions about the best approach for your specific situation.</p>
<ol type="1">
<li>Understand the nature of the missing or inconsistent data</li>
</ol>
<p>Before diving into any solutions, it’s essential to understand the nature of the missing or inconsistent data you’re dealing with. In R, you can use the <strong><code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code></strong> function to get an overview of your dataset, including the number of missing values:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co" style="color: #5E5E5E;"># load these packages</span></span>
<span><span class="co" style="color: #5E5E5E;">#library(dplyr)</span></span>
<span><span class="co" style="color: #5E5E5E;">#library(stringdist)</span></span>
<span><span class="co" style="color: #5E5E5E;">#library(tidyverse)</span></span>
<span><span class="co" style="color: #5E5E5E;">#library(mice)</span></span>
<span><span class="co" style="color: #5E5E5E;">#library(DMwR2)</span></span>
<span></span>
<span><span class="va" style="color: #111111;">dataset</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op" style="color: #5E5E5E;">(</span>col1 <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="fl" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">:</span><span class="fl" style="color: #AD0000;">3</span>, <span class="cn" style="color: #8f5902;">NA</span><span class="op" style="color: #5E5E5E;">)</span>,</span>
<span>                 col2 <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="st" style="color: #20794D;">"one"</span>, <span class="cn" style="color: #8f5902;">NA</span>,<span class="st" style="color: #20794D;">"cool"</span>, <span class="st" style="color: #20794D;">"text"</span><span class="op" style="color: #5E5E5E;">)</span>, </span>
<span>                 col3 <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="cn" style="color: #8f5902;">TRUE</span>, <span class="cn" style="color: #8f5902;">FALSE</span>, <span class="cn" style="color: #8f5902;">TRUE</span>, <span class="cn" style="color: #8f5902;">TRUE</span><span class="op" style="color: #5E5E5E;">)</span>, </span>
<span>                 col4 <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="fl" style="color: #AD0000;">0.5</span>, <span class="fl" style="color: #AD0000;">4.7</span>, <span class="fl" style="color: #AD0000;">3.2</span>, <span class="cn" style="color: #8f5902;">NA</span><span class="op" style="color: #5E5E5E;">)</span>,</span>
<span>                 date_column <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="st" style="color: #20794D;">"2000/1/1"</span>,<span class="st" style="color: #20794D;">"2000/2/1"</span> ,<span class="st" style="color: #20794D;">"2000/3/1"</span> ,<span class="st" style="color: #20794D;">"2023/13/40"</span><span class="op" style="color: #5E5E5E;">)</span>,                 stringsAsFactors <span class="op" style="color: #5E5E5E;">=</span> <span class="cn" style="color: #8f5902;">FALSE</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span></span>
<span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">dataset</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      col1         col2              col3              col4     
 Min.   :1.0   Length:4           Mode :logical   Min.   :0.50  
 1st Qu.:1.5   Class :character   FALSE:1         1st Qu.:1.85  
 Median :2.0   Mode  :character   TRUE :3         Median :3.20  
 Mean   :2.0                                      Mean   :2.80  
 3rd Qu.:2.5                                      3rd Qu.:3.95  
 Max.   :3.0                                      Max.   :4.70  
 NA's   :1                                        NA's   :1     
 date_column       
 Length:4          
 Class :character  
 Mode  :character  
                   
                   
                   
                   </code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Data Imputation</li>
</ol>
<p>One common approach for dealing with missing data is imputation. Imputation involves estimating the missing values based on other available data. Some popular imputation methods in R include:</p>
<ul>
<li>Mean, median, or mode imputation: Replace missing values with the mean, median, or mode of the column.</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va" style="color: #111111;">dataset</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="va" style="color: #111111;">dataset</span> <span class="op" style="color: #5E5E5E;">%&gt;%</span></span>
<span>  <span class="fu" style="color: #4758AB;">mutate</span><span class="op" style="color: #5E5E5E;">(</span>col4 <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;">if_else</span><span class="op" style="color: #5E5E5E;">(</span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">col4</span><span class="op" style="color: #5E5E5E;">)</span>, <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">col4</span>, na.rm <span class="op" style="color: #5E5E5E;">=</span> <span class="cn" style="color: #8f5902;">TRUE</span><span class="op" style="color: #5E5E5E;">)</span>, <span class="va" style="color: #111111;">col4</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
</div>
<ul>
<li>Linear regression imputation: Use a linear regression model to estimate missing values based on other variables in the dataset.</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va" style="color: #111111;">imputed_data</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;">mice</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">dataset</span>, method <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'norm.predict'</span>, m <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">5</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
 iter imp variable
  1   1  col1
  1   2  col1
  1   3  col1
  1   4  col1
  1   5  col1
  2   1  col1
  2   2  col1
  2   3  col1
  2   4  col1
  2   5  col1
  3   1  col1
  3   2  col1
  3   3  col1
  3   4  col1
  3   5  col1
  4   1  col1
  4   2  col1
  4   3  col1
  4   4  col1
  4   5  col1
  5   1  col1
  5   2  col1
  5   3  col1
  5   4  col1
  5   5  col1</code></pre>
</div>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va" style="color: #111111;">complete_data</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;">complete</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">imputed_data</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span><span class="va" style="color: #111111;">complete_data</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      col1 col2  col3 col4 date_column
1 1.000000  one  TRUE  0.5    2000/1/1
2 2.000000 &lt;NA&gt; FALSE  4.7    2000/2/1
3 3.000000 cool  TRUE  3.2    2000/3/1
4 2.703704 text  TRUE  2.8  2023/13/40</code></pre>
</div>
</div>
<ul>
<li>K-Nearest Neighbours (KNN) imputation: Fill in missing values by averaging the values of the k-nearest neighbours. I will give an example of the code below, but you need a bigger dataset for that approach, that’s why the code is commented.</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co" style="color: #5E5E5E;">#imputed_data &lt;- knnImputation(dataset, k = 5)</span></span></code></pre></div>
</div>
<p>It’s important to note that imputation can introduce bias or distort the underlying data distribution, so it should be used with caution.</p>
<ol start="3" type="1">
<li>Removing missing or inconsistent data</li>
</ol>
<p>In some cases, it may be appropriate to remove rows or columns containing missing or inconsistent data. This can be done using techniques such as:</p>
<ul>
<li>Listwise deletion: Remove any rows containing missing values.</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va" style="color: #111111;">dataset</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/stats/na.fail.html">na.omit</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">dataset</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
</div>
<ul>
<li>
<p>Dropping columns: Remove columns with a high proportion of missing or inconsistent data.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va" style="color: #111111;">column_with_missing_data</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">dataset</span>,<span class="kw" style="color: #003B4F;">function</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">x</span><span class="op" style="color: #5E5E5E;">)</span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">x</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span><span class="va" style="color: #111111;">column_with_missing_data</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="va" style="color: #111111;">column_with_missing_data</span><span class="op" style="color: #5E5E5E;">[</span><span class="va" style="color: #111111;">column_with_missing_data</span> <span class="op" style="color: #5E5E5E;">==</span> <span class="fl" style="color: #AD0000;">0</span><span class="op" style="color: #5E5E5E;">]</span></span>
<span></span>
<span><span class="va" style="color: #111111;">dataset</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="va" style="color: #111111;">dataset</span> <span class="op" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">select</span><span class="op" style="color: #5E5E5E;">(</span><span class="op" style="color: #5E5E5E;">-</span><span class="va" style="color: #111111;">column_with_missing_data</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span><span class="va" style="color: #111111;">dataset</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  col1 col2 col3 col4 date_column
1    1  one TRUE  0.5    2000/1/1
3    3 cool TRUE  3.2    2000/3/1</code></pre>
</div>
</div>
<p>Keep in mind that removing data can lead to loss of information and may introduce bias if the data is not missing at random.</p>
<ol start="4" type="1">
<li>Data Standardisation and Transformation</li>
</ol>
<p>Inconsistent data often results from variations in data entry, formats, or units. To address this issue, you can standardise and transform the data using R functions like:</p>
<ul>
<li>Establishing consistent formats for dates ( in case it is of type character and there’s inconsistences like “13/40/2023” the return would be NA, so it will help you to recognise inconsistences.</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va" style="color: #111111;">dataset</span><span class="op" style="color: #5E5E5E;">$</span><span class="va" style="color: #111111;">date_column</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/as.Date.html">as.Date</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">dataset</span><span class="op" style="color: #5E5E5E;">$</span><span class="va" style="color: #111111;">date_column</span>, format <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"%Y/%m/%d"</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span><span class="va" style="color: #111111;">dataset</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  col1 col2 col3 col4 date_column
1    1  one TRUE  0.5  2000-01-01
3    3 cool TRUE  3.2  2000-03-01</code></pre>
</div>
</div>
<p>Dealing with missing or inconsistent data is a common challenge for data scientists, but it’s also an opportunity to refine your skills and improve your dataset’s quality. By using R to understand the nature of the missing or inconsistent data and applying appropriate strategies, you can make more informed decisions and produce more accurate and reliable insights. In the next post of our series on data science daily life challenges, we’ll explore the intricacies of handling high-dimensional data and the techniques used to simplify analyses using R. Stay tuned!</p>
</li>
</ul>



 ]]></description>
  <category>Data Science</category>
  <category>R</category>
  <category>DDC</category>
  <category>missing</category>
  <category>inconsistent</category>
  <guid>https://iannimuliterno.com/posts/2023-04-daily_life_ds2/index.html</guid>
  <pubDate>Mon, 24 Apr 2023 03:00:00 GMT</pubDate>
  <media:content url="https://iannimuliterno.com/posts/2023-04-daily_life_ds2/DDC.PNG" medium="image"/>
</item>
<item>
  <title>DDC: Tackling Missing or Inconsistent Data</title>
  <dc:creator>I. Muliterno</dc:creator>
  <link>https://iannimuliterno.com/posts/2023-05-randomforest/index.html</link>
  <description><![CDATA[ <p>In the world of data science, dealing with missing or inconsistent data is an everyday challenge. The quality of your insights, predictions, and models heavily depends on the quality of the data you use. In this second post of our series on data science daily life challenges, we’ll explore various strategies for handling missing or inconsistent data using R, and how to make informed decisions about the best approach for your specific situation.</p>
<ol type="1">
<li>Understand the nature of the missing or inconsistent data</li>
</ol>
<p>Before diving into any solutions, it’s essential to understand the nature of the missing or inconsistent data you’re dealing with. In R, you can use the <strong><code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code></strong> function to get an overview of your dataset, including the number of missing values:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co" style="color: #5E5E5E;"># load these packages</span></span>
<span><span class="co" style="color: #5E5E5E;">#library(dplyr)</span></span>
<span><span class="co" style="color: #5E5E5E;">#library(stringdist)</span></span>
<span><span class="co" style="color: #5E5E5E;">#library(tidyverse)</span></span>
<span><span class="co" style="color: #5E5E5E;">#library(mice)</span></span>
<span><span class="co" style="color: #5E5E5E;">#library(DMwR2)</span></span>
<span></span>
<span><span class="va" style="color: #111111;">dataset</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op" style="color: #5E5E5E;">(</span>col1 <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="fl" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">:</span><span class="fl" style="color: #AD0000;">3</span>, <span class="cn" style="color: #8f5902;">NA</span><span class="op" style="color: #5E5E5E;">)</span>,</span>
<span>                 col2 <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="st" style="color: #20794D;">"one"</span>, <span class="cn" style="color: #8f5902;">NA</span>,<span class="st" style="color: #20794D;">"cool"</span>, <span class="st" style="color: #20794D;">"text"</span><span class="op" style="color: #5E5E5E;">)</span>, </span>
<span>                 col3 <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="cn" style="color: #8f5902;">TRUE</span>, <span class="cn" style="color: #8f5902;">FALSE</span>, <span class="cn" style="color: #8f5902;">TRUE</span>, <span class="cn" style="color: #8f5902;">TRUE</span><span class="op" style="color: #5E5E5E;">)</span>, </span>
<span>                 col4 <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="fl" style="color: #AD0000;">0.5</span>, <span class="fl" style="color: #AD0000;">4.7</span>, <span class="fl" style="color: #AD0000;">3.2</span>, <span class="cn" style="color: #8f5902;">NA</span><span class="op" style="color: #5E5E5E;">)</span>,</span>
<span>                 date_column <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="st" style="color: #20794D;">"2000/1/1"</span>,<span class="st" style="color: #20794D;">"2000/2/1"</span> ,<span class="st" style="color: #20794D;">"2000/3/1"</span> ,<span class="st" style="color: #20794D;">"2023/13/40"</span><span class="op" style="color: #5E5E5E;">)</span>,                 stringsAsFactors <span class="op" style="color: #5E5E5E;">=</span> <span class="cn" style="color: #8f5902;">FALSE</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span></span>
<span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">dataset</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      col1         col2              col3              col4     
 Min.   :1.0   Length:4           Mode :logical   Min.   :0.50  
 1st Qu.:1.5   Class :character   FALSE:1         1st Qu.:1.85  
 Median :2.0   Mode  :character   TRUE :3         Median :3.20  
 Mean   :2.0                                      Mean   :2.80  
 3rd Qu.:2.5                                      3rd Qu.:3.95  
 Max.   :3.0                                      Max.   :4.70  
 NA's   :1                                        NA's   :1     
 date_column       
 Length:4          
 Class :character  
 Mode  :character  
                   
                   
                   
                   </code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Data Imputation</li>
</ol>
<p>One common approach for dealing with missing data is imputation. Imputation involves estimating the missing values based on other available data. Some popular imputation methods in R include:</p>
<ul>
<li>Mean, median, or mode imputation: Replace missing values with the mean, median, or mode of the column.</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va" style="color: #111111;">dataset</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="va" style="color: #111111;">dataset</span> <span class="op" style="color: #5E5E5E;">%&gt;%</span></span>
<span>  <span class="fu" style="color: #4758AB;">mutate</span><span class="op" style="color: #5E5E5E;">(</span>col4 <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;">if_else</span><span class="op" style="color: #5E5E5E;">(</span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">col4</span><span class="op" style="color: #5E5E5E;">)</span>, <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">col4</span>, na.rm <span class="op" style="color: #5E5E5E;">=</span> <span class="cn" style="color: #8f5902;">TRUE</span><span class="op" style="color: #5E5E5E;">)</span>, <span class="va" style="color: #111111;">col4</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
</div>
<ul>
<li>Linear regression imputation: Use a linear regression model to estimate missing values based on other variables in the dataset.</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va" style="color: #111111;">imputed_data</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;">mice</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">dataset</span>, method <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'norm.predict'</span>, m <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">5</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
 iter imp variable
  1   1  col1
  1   2  col1
  1   3  col1
  1   4  col1
  1   5  col1
  2   1  col1
  2   2  col1
  2   3  col1
  2   4  col1
  2   5  col1
  3   1  col1
  3   2  col1
  3   3  col1
  3   4  col1
  3   5  col1
  4   1  col1
  4   2  col1
  4   3  col1
  4   4  col1
  4   5  col1
  5   1  col1
  5   2  col1
  5   3  col1
  5   4  col1
  5   5  col1</code></pre>
</div>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va" style="color: #111111;">complete_data</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;">complete</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">imputed_data</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span><span class="va" style="color: #111111;">complete_data</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      col1 col2  col3 col4 date_column
1 1.000000  one  TRUE  0.5    2000/1/1
2 2.000000 &lt;NA&gt; FALSE  4.7    2000/2/1
3 3.000000 cool  TRUE  3.2    2000/3/1
4 2.703704 text  TRUE  2.8  2023/13/40</code></pre>
</div>
</div>
<ul>
<li>K-Nearest Neighbours (KNN) imputation: Fill in missing values by averaging the values of the k-nearest neighbours. I will give an example of the code below, but you need a bigger dataset for that approach, that’s why the code is commented.</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co" style="color: #5E5E5E;">#imputed_data &lt;- knnImputation(dataset, k = 5)</span></span></code></pre></div>
</div>
<p>It’s important to note that imputation can introduce bias or distort the underlying data distribution, so it should be used with caution.</p>
<ol start="3" type="1">
<li>Removing missing or inconsistent data</li>
</ol>
<p>In some cases, it may be appropriate to remove rows or columns containing missing or inconsistent data. This can be done using techniques such as:</p>
<ul>
<li>Listwise deletion: Remove any rows containing missing values.</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va" style="color: #111111;">dataset</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/stats/na.fail.html">na.omit</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">dataset</span><span class="op" style="color: #5E5E5E;">)</span></span></code></pre></div>
</div>
<ul>
<li>
<p>Dropping columns: Remove columns with a high proportion of missing or inconsistent data.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va" style="color: #111111;">column_with_missing_data</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">dataset</span>,<span class="kw" style="color: #003B4F;">function</span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">x</span><span class="op" style="color: #5E5E5E;">)</span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">x</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span><span class="va" style="color: #111111;">column_with_missing_data</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="va" style="color: #111111;">column_with_missing_data</span><span class="op" style="color: #5E5E5E;">[</span><span class="va" style="color: #111111;">column_with_missing_data</span> <span class="op" style="color: #5E5E5E;">==</span> <span class="fl" style="color: #AD0000;">0</span><span class="op" style="color: #5E5E5E;">]</span></span>
<span></span>
<span><span class="va" style="color: #111111;">dataset</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="va" style="color: #111111;">dataset</span> <span class="op" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">select</span><span class="op" style="color: #5E5E5E;">(</span><span class="op" style="color: #5E5E5E;">-</span><span class="va" style="color: #111111;">column_with_missing_data</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span><span class="va" style="color: #111111;">dataset</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  col1 col2 col3 col4 date_column
1    1  one TRUE  0.5    2000/1/1
3    3 cool TRUE  3.2    2000/3/1</code></pre>
</div>
</div>
<p>Keep in mind that removing data can lead to loss of information and may introduce bias if the data is not missing at random.</p>
<ol start="4" type="1">
<li>Data Standardisation and Transformation</li>
</ol>
<p>Inconsistent data often results from variations in data entry, formats, or units. To address this issue, you can standardise and transform the data using R functions like:</p>
<ul>
<li>Establishing consistent formats for dates ( in case it is of type character and there’s inconsistences like “13/40/2023” the return would be NA, so it will help you to recognise inconsistences.</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va" style="color: #111111;">dataset</span><span class="op" style="color: #5E5E5E;">$</span><span class="va" style="color: #111111;">date_column</span> <span class="op" style="color: #5E5E5E;">&lt;-</span> <span class="fu" style="color: #4758AB;"><a href="https://rdrr.io/r/base/as.Date.html">as.Date</a></span><span class="op" style="color: #5E5E5E;">(</span><span class="va" style="color: #111111;">dataset</span><span class="op" style="color: #5E5E5E;">$</span><span class="va" style="color: #111111;">date_column</span>, format <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"%Y/%m/%d"</span><span class="op" style="color: #5E5E5E;">)</span></span>
<span><span class="va" style="color: #111111;">dataset</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  col1 col2 col3 col4 date_column
1    1  one TRUE  0.5  2000-01-01
3    3 cool TRUE  3.2  2000-03-01</code></pre>
</div>
</div>
<p>Dealing with missing or inconsistent data is a common challenge for data scientists, but it’s also an opportunity to refine your skills and improve your dataset’s quality. By using R to understand the nature of the missing or inconsistent data and applying appropriate strategies, you can make more informed decisions and produce more accurate and reliable insights. In the next post of our series on data science daily life challenges, we’ll explore the intricacies of handling high-dimensional data and the techniques used to simplify analyses using R. Stay tuned!</p>
</li>
</ul>



 ]]></description>
  <category>Data Science</category>
  <category>R</category>
  <category>DDC</category>
  <category>missing</category>
  <category>inconsistent</category>
  <guid>https://iannimuliterno.com/posts/2023-05-randomforest/index.html</guid>
  <pubDate>Mon, 24 Apr 2023 03:00:00 GMT</pubDate>
  <media:content url="https://iannimuliterno.com/posts/2023-05-randomforest/DDC.PNG" medium="image"/>
</item>
<item>
  <title>Model 2 - Decision Trees in Python with Penguin dataset</title>
  <dc:creator>I. Muliterno</dc:creator>
  <link>https://iannimuliterno.com/posts/2023-03-dectree_python/index.html</link>
  <description><![CDATA[ <p>In our previous post, we discussed linear regression. Today, we will get familiar with decisions trees, but before that, you gotta understand what’s a <strong>classification</strong> <strong>problem</strong>.</p>
<p>Classification is an important task in machine learning, with numerous applications in fields such as healthcare, finance, and marketing. One popular classification algorithm is decision trees, which use a tree-like model of decisions and their possible consequences to predict the target variable. In this post, we will provide a step-by-step guide to implementing decision trees for classification using Python and the Penguin dataset.</p>
<section id="decision-trees" class="level3"><h3 class="anchored" data-anchor-id="decision-trees">Decision Trees</h3>
<p>Decision trees are a type of classification algorithm that use a hierarchical structure to model decisions and their outcomes. They work by partitioning the data into subsets based on the values of the input features, and then recursively dividing each subset into smaller subsets until the target variable is predicted with a high degree of accuracy. Decision trees have several advantages over other classification algorithms, including their interpretability, ability to handle both categorical and numerical data, and flexibility in handling missing values.</p>
</section><section id="dataset-description" class="level2"><h2 class="anchored" data-anchor-id="dataset-description">Dataset Description</h2>
<p>The Penguin dataset is a well-known dataset that contains information about the physical characteristics and species of penguins. It consists of 344 samples with 8 input features and 1 target variable (the species of the penguin). In this section, we will describe the structure of the dataset and the importance of data preprocessing in the context of decision trees.</p>
<p>The first step in any machine learning project is to load and explore the data. To load the Palmer Penguins dataset, we will use the <strong><code>load_penguins</code></strong> function from the <strong><code>palmerpenguins</code></strong> library. We will also import <strong><code>pandas</code></strong> to create a dataframe to store the data, and <strong><code>seaborn</code></strong> and <strong><code>matplotlib</code></strong> to visualize the data.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> pandas <span class="im" style="color: #00769E;">as</span> pd</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">from</span> sklearn.tree <span class="im" style="color: #00769E;">import</span> DecisionTreeClassifier</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;">import</span> train_test_split</span>
<span id="cb1-5"><span class="im" style="color: #00769E;">from</span> sklearn.metrics <span class="im" style="color: #00769E;">import</span> accuracy_score, precision_score, recall_score, confusion_matrix</span>
<span id="cb1-6"><span class="im" style="color: #00769E;">from</span> sklearn.metrics <span class="im" style="color: #00769E;">import</span> f1_score</span>
<span id="cb1-7"><span class="im" style="color: #00769E;">from</span> sklearn.tree <span class="im" style="color: #00769E;">import</span> export_graphviz</span>
<span id="cb1-8"><span class="im" style="color: #00769E;">import</span> graphviz</span>
<span id="cb1-9"><span class="im" style="color: #00769E;">import</span> seaborn <span class="im" style="color: #00769E;">as</span> sns </span>
<span id="cb1-10"><span class="im" style="color: #00769E;">from</span> palmerpenguins <span class="im" style="color: #00769E;">import</span> load_penguins</span>
<span id="cb1-11"><span class="im" style="color: #00769E;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;">import</span> StandardScaler</span>
<span id="cb1-12"></span>
<span id="cb1-13"></span>
<span id="cb1-14"><span class="im" style="color: #00769E;">import</span> pandas <span class="im" style="color: #00769E;">as</span> pd</span>
<span id="cb1-15"><span class="im" style="color: #00769E;">import</span> seaborn <span class="im" style="color: #00769E;">as</span> sns </span>
<span id="cb1-16"><span class="im" style="color: #00769E;">from</span> palmerpenguins <span class="im" style="color: #00769E;">import</span> load_penguins</span>
<span id="cb1-17">sns.set_style(<span class="st" style="color: #20794D;">'whitegrid'</span>)</span>
<span id="cb1-18">penguins <span class="op" style="color: #5E5E5E;">=</span> load_penguins()</span>
<span id="cb1-19">penguins.head()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  species     island  bill_length_mm  ...  body_mass_g     sex  year
0  Adelie  Torgersen            39.1  ...       3750.0    male  2007
1  Adelie  Torgersen            39.5  ...       3800.0  female  2007
2  Adelie  Torgersen            40.3  ...       3250.0  female  2007
3  Adelie  Torgersen             NaN  ...          NaN     NaN  2007
4  Adelie  Torgersen            36.7  ...       3450.0  female  2007

[5 rows x 8 columns]</code></pre>
</div>
</div>
<p>The <strong><code>head</code></strong> function is used to display the first few rows of the dataset. This is useful to check that the data has been loaded correctly and to get a quick overview of the data. Now let’s look for missing values.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"></span>
<span id="cb3-2"><span class="co" style="color: #5E5E5E;"># Check for missing values</span></span>
<span id="cb3-3"><span class="bu" style="color: null;">print</span>(penguins.isnull().<span class="bu" style="color: null;">sum</span>())</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>species               0
island                0
bill_length_mm        2
bill_depth_mm         2
flipper_length_mm     2
body_mass_g           2
sex                  11
year                  0
dtype: int64</code></pre>
</div>
</div>
<p>we don’t always just drop na, but sckitlearn classifiers aren’t able to handle missing values plus we will lose only a few rows, so let’s take the easy way out here. An alternative could be `filling NA`, but it can be dangerous specially if you rely in simple methods like, just using the average value to fill the gaps. Note that, in the real world, we usually deal with many more missing values and the answer could be trying to enrich the dataset with external information, testing another classification model which can deal with missing values or checking more advanced methods to fill the gaps, how would you like a post about that?</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;"># drop missing values </span></span>
<span id="cb5-2">penguins <span class="op" style="color: #5E5E5E;">=</span> penguins.dropna()</span>
<span id="cb5-3"><span class="co" style="color: #5E5E5E;"># Double Check for missing values</span></span>
<span id="cb5-4"><span class="bu" style="color: null;">print</span>(penguins.isnull().<span class="bu" style="color: null;">sum</span>())</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>species              0
island               0
bill_length_mm       0
bill_depth_mm        0
flipper_length_mm    0
body_mass_g          0
sex                  0
year                 0
dtype: int64</code></pre>
</div>
</div>
<p>Now that we have checked for missing values, we can visualize the distribution of the target variable using a histogram. In this case, the target variable is the species of the penguin.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="co" style="color: #5E5E5E;"># Plot the distribution of the target variable</span></span>
<span id="cb7-2">plt.hist(penguins[<span class="st" style="color: #20794D;">'species'</span>])</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(array([146.,   0.,   0.,   0.,   0., 119.,   0.,   0.,   0.,  68.]), array([0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8, 2. ]), &lt;BarContainer object of 10 artists&gt;)</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">plt.show()</span></code></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="https://iannimuliterno.com/posts/2023-03-dectree_python/index_files/figure-html/targeteda-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>We can see that the dataset contains three species of penguins: Adelie, Chinstrap, and Gentoo.</p>
<p>Finally, we can use a scatter matrix plot to visualize the pairwise relationships between the features. This is useful to understand the relationships between the features and to identify any potential correlations, or even if there’s a variable worth removing.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="co" style="color: #5E5E5E;"># Plot the pairwise relationships between the features</span></span>
<span id="cb10-2">sns.pairplot(data<span class="op" style="color: #5E5E5E;">=</span>penguins, hue<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'species'</span>)</span></code></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="https://iannimuliterno.com/posts/2023-03-dectree_python/index_files/figure-html/correlation-3.png" class="img-fluid figure-img" width="676"></p>
</figure>
</div>
</div>
</div>
<p>This is a very revealing plot, we can see, for example, that some features are very correlated, meaning that we could probably remove flipper_length_mm for example, and leave body_mass, because they are correlated, you can see what I’m talking about if you look to the graph at row 3 column 4. We also can see how the features interact with the species of the penguins, look at row 4 column 4, for example and you will find out that most of Gentoo penguins are heavier than the others, this indicates that body mass may be important predictor for our decision tree model.</p>
<p>Ok, now we’re read to prep the data</p>
</section><section id="data-preprocessing" class="level1"><h1><strong>Data Preprocessing</strong></h1>
<p>In this section, we will preprocess our dataset before we move on to the modeling phase. Preprocessing is an important step in machine learning as it involves cleaning, transforming, and engineering the data to make it suitable for modeling.</p>
<p>First, we make a copy of the original <strong><code>penguins</code></strong> DataFrame, since it’s good practice to keep the original data intact, and define our categorical variables. In our case, the categorical variables are <strong><code>island</code></strong> and <strong><code>sex</code></strong>. We then iterate through each categorical variable and create dummy variables using the <strong><code>pd.get_dummies()</code></strong> function. This creates a binary indicator variable for each possible category. Next, we concatenate the dummy variables with the original DataFrame and drop the original categorical variables.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"></span>
<span id="cb11-2"><span class="co" style="color: #5E5E5E;"># Make a copy of data_size DataFrame</span></span>
<span id="cb11-3">data_df <span class="op" style="color: #5E5E5E;">=</span> penguins.copy()</span>
<span id="cb11-4"></span>
<span id="cb11-5"><span class="co" style="color: #5E5E5E;"># Define categorical variables</span></span>
<span id="cb11-6">cat_vars <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">'island'</span>, <span class="st" style="color: #20794D;">'sex'</span>]</span>
<span id="cb11-7"></span>
<span id="cb11-8"><span class="co" style="color: #5E5E5E;"># Iterate through categorical variables and create dummy variables</span></span>
<span id="cb11-9">dummy_vars <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb11-10"><span class="cf" style="color: #003B4F;">for</span> var <span class="kw" style="color: #003B4F;">in</span> cat_vars:</span>
<span id="cb11-11">    dummy_var <span class="op" style="color: #5E5E5E;">=</span> pd.get_dummies(data_df[var], prefix<span class="op" style="color: #5E5E5E;">=</span>var)</span>
<span id="cb11-12">    dummy_vars.append(dummy_var)</span>
<span id="cb11-13">    </span>
<span id="cb11-14">dummy_df <span class="op" style="color: #5E5E5E;">=</span> pd.concat(dummy_vars, axis<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb11-15">data_df <span class="op" style="color: #5E5E5E;">=</span> pd.concat([data_df, dummy_df], axis<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb11-16">data_df <span class="op" style="color: #5E5E5E;">=</span> data_df.drop(cat_vars, axis<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)    </span>
<span id="cb11-17"></span>
<span id="cb11-18"><span class="co" style="color: #5E5E5E;"># Define final list of variables</span></span>
<span id="cb11-19">data_final_vars <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">'bill_length_mm'</span>,<span class="st" style="color: #20794D;">'bill_depth_mm'</span>, <span class="st" style="color: #20794D;">'flipper_length_mm'</span>, <span class="st" style="color: #20794D;">'body_mass_g'</span>, <span class="st" style="color: #20794D;">'island_Biscoe'</span>,</span>
<span id="cb11-20"> <span class="st" style="color: #20794D;">'island_Dream'</span>, <span class="st" style="color: #20794D;">'sex_female'</span>]</span>
<span id="cb11-21">y <span class="op" style="color: #5E5E5E;">=</span> penguins[<span class="st" style="color: #20794D;">'species'</span>].values</span>
<span id="cb11-22">X <span class="op" style="color: #5E5E5E;">=</span> data_df[data_final_vars].values</span></code></pre></div>
</div>
<p>We also define our final list of variables, which are the numeric features we will use in our decision tree model. The target variable is defined as <strong><code>y</code></strong> and the independent variables are defined as <strong><code>X</code></strong>.</p>
<p>Now let us perform scaling on our numeric features using <strong><code>StandardScaler()</code></strong>. This standardizes our numeric features so that each feature has a mean of 0 and standard deviation of 1. This is an important step as decision trees are sensitive to the scale of the features.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"></span>
<span id="cb12-2"></span>
<span id="cb12-3"><span class="co" style="color: #5E5E5E;"># Scale the numeric features</span></span>
<span id="cb12-4">scaler <span class="op" style="color: #5E5E5E;">=</span> StandardScaler()</span>
<span id="cb12-5">scaled <span class="op" style="color: #5E5E5E;">=</span> scaler.fit_transform(penguins[[<span class="st" style="color: #20794D;">'bill_length_mm'</span>,<span class="st" style="color: #20794D;">'bill_depth_mm'</span>, <span class="st" style="color: #20794D;">'flipper_length_mm'</span>, <span class="st" style="color: #20794D;">'body_mass_g'</span>]])</span>
<span id="cb12-6">scaled_df <span class="op" style="color: #5E5E5E;">=</span> pd.DataFrame(scaled, columns <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">'bill_length_mm'</span>,<span class="st" style="color: #20794D;">'bill_depth_mm'</span>, <span class="st" style="color: #20794D;">'flipper_length_mm'</span>, <span class="st" style="color: #20794D;">'body_mass_g'</span>])</span>
<span id="cb12-7">scaled_df.head()</span>
<span id="cb12-8"></span>
<span id="cb12-9"><span class="co" style="color: #5E5E5E;"># Copy the scaled data back to the main dataframe</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g
0       -0.896042       0.780732          -1.426752    -0.568475
1       -0.822788       0.119584          -1.069474    -0.506286
2       -0.676280       0.424729          -0.426373    -1.190361
3       -1.335566       1.085877          -0.569284    -0.941606
4       -0.859415       1.747026          -0.783651    -0.692852</code></pre>
</div>
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">data_df[[<span class="st" style="color: #20794D;">'bill_length_mm'</span>,<span class="st" style="color: #20794D;">'bill_depth_mm'</span>, <span class="st" style="color: #20794D;">'flipper_length_mm'</span>, <span class="st" style="color: #20794D;">'body_mass_g'</span>]] <span class="op" style="color: #5E5E5E;">=</span> scaled_df[[<span class="st" style="color: #20794D;">'bill_length_mm'</span>,<span class="st" style="color: #20794D;">'bill_depth_mm'</span>, <span class="st" style="color: #20794D;">'flipper_length_mm'</span>, <span class="st" style="color: #20794D;">'body_mass_g'</span>]]</span>
<span id="cb14-2">penguins_final <span class="op" style="color: #5E5E5E;">=</span> data_df</span></code></pre></div>
</div>
<p>Now we are all set, let’s get to the model.</p>
<section id="modeling-using-decision-trees" class="level2"><h2 class="anchored" data-anchor-id="modeling-using-decision-trees">Modeling using Decision Trees</h2>
<p>Now let’s train our decision tree model to predict the species of penguins based on the independent variables we identified in the previous section.</p>
<p>The first step in model training is to split the dataset into training and testing sets. We used the <strong><code>train_test_split</code></strong> function from scikit-learn library to randomly split the data into 80% for training and 20% for testing. This step is important to ensure that the model is not overfitting to the data, meaning that it is not just memorizing the training data but can generalize well to new, unseen data.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="co" style="color: #5E5E5E;"># Split the data into training and testing sets</span></span>
<span id="cb15-2">X_train, X_test, y_train, y_test <span class="op" style="color: #5E5E5E;">=</span> train_test_split(X, y, test_size<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.2</span>, random_state<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">42</span>)</span></code></pre></div>
</div>
<p>Next, we built the decision tree model using the <strong><code>DecisionTreeClassifier</code></strong> function from scikit-learn library. We set the maximum depth of the tree to 3 and used a random state of 42 to ensure that our results are reproducible. The <strong><code>max_depth</code></strong> parameter controls the complexity of the tree, and we chose 3 to balance between underfitting and overfitting.</p>
<p>After building the model, we made predictions on the test set using the <strong><code>predict</code></strong> function of the decision tree classifier. These predictions will be used to evaluate the performance of our model in the next section.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"></span>
<span id="cb16-2"><span class="co" style="color: #5E5E5E;"># Build the decision tree model</span></span>
<span id="cb16-3">tree <span class="op" style="color: #5E5E5E;">=</span> DecisionTreeClassifier(max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">3</span>, random_state<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">42</span>)</span>
<span id="cb16-4">tree.fit(X_train, y_train)</span>
<span id="cb16-5"></span>
<span id="cb16-6"><span class="co" style="color: #5E5E5E;"># Make predictions on the test set</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>DecisionTreeClassifier(max_depth=3, random_state=42)</code></pre>
</div>
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">y_pred <span class="op" style="color: #5E5E5E;">=</span> tree.predict(X_test)</span></code></pre></div>
</div>
</section><section id="performance" class="level2"><h2 class="anchored" data-anchor-id="performance">Performance</h2>
<p>After building our decision tree model, it is important to evaluate its performance. We can use various metrics to evaluate the model, including accuracy, precision, recall, and the confusion matrix. Before getting to the code, let’s understand the metrics.</p>
<p>The accuracy score tells us the percentage of correctly classified instances out of all instances.</p>
<p><strong><code>accuracy = (number of correct predictions) / (total number of predictions)</code></strong></p>
<p>The precision score tells us the percentage of instances that were correctly classified as a certain class out of all instances classified as that class.</p>
<p><code>Precision = (true positives) / (true positives + false positives)</code></p>
<p>The recall score tells us the percentage of instances that were correctly classified as a certain class out of all instances of that class.</p>
<p><code>Recall = (true positives) / (true positives + false negatives)</code></p>
<p>F1 score is the harmonic mean of precision and recall. It is a measure of the balance between precision and recall and is a useful metric when dealing with imbalanced datasets.</p>
<p><code>F1 score = 2 * ((precision * recall) / (precision + recall))</code></p>
<p>In these formulas, true positives are the number of correctly predicted instances of a particular class, false positives are the number of instances that were incorrectly predicted as that class, and false negatives are the number of instances of that class that were incorrectly predicted as another class.</p>
<p>The confusion matrix is a table that shows the number of true positives, true negatives, false positives, and false negatives for each class.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"></span>
<span id="cb19-2"><span class="co" style="color: #5E5E5E;"># Evaluate the model</span></span>
<span id="cb19-3"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Accuracy: </span><span class="sc" style="color: #5E5E5E;">{:.2f}</span><span class="st" style="color: #20794D;">%"</span>.<span class="bu" style="color: null;">format</span>(<span class="dv" style="color: #AD0000;">100</span> <span class="op" style="color: #5E5E5E;">*</span> accuracy_score(y_test, y_pred)))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 97.01%</code></pre>
</div>
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Precision: </span><span class="sc" style="color: #5E5E5E;">{:.2f}</span><span class="st" style="color: #20794D;">%"</span>.<span class="bu" style="color: null;">format</span>(<span class="dv" style="color: #AD0000;">100</span> <span class="op" style="color: #5E5E5E;">*</span> precision_score(y_test, y_pred, average<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'weighted'</span>)))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Precision: 97.20%</code></pre>
</div>
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Recall: </span><span class="sc" style="color: #5E5E5E;">{:.2f}</span><span class="st" style="color: #20794D;">%"</span>.<span class="bu" style="color: null;">format</span>(<span class="dv" style="color: #AD0000;">100</span> <span class="op" style="color: #5E5E5E;">*</span> recall_score(y_test, y_pred, average<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'weighted'</span>)))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Recall: 97.01%</code></pre>
</div>
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"F1 score: </span><span class="sc" style="color: #5E5E5E;">{:.2f}</span><span class="st" style="color: #20794D;">%"</span>.<span class="bu" style="color: null;">format</span>(<span class="dv" style="color: #AD0000;">100</span> <span class="op" style="color: #5E5E5E;">*</span> f1_score(y_test, y_pred, average<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'macro'</span>)))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>F1 score: 97.00%</code></pre>
</div>
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Confusion Matrix:</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span>, confusion_matrix(y_test, y_pred))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix:
 [[31  0  0]
 [ 2 16  0]
 [ 0  0 18]]</code></pre>
</div>
</div>
<p>We can see that our model has an accuracy of 97%, which means it correctly classified 97% of instances. The precision and recall scores are also high, indicating that our model performed well in classifying instances for each class. The confusion matrix provides us with more detailed information about the number of correctly and incorrectly classified instances for each class.</p>
<p>Let’s take a look at the decision tree we made.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"></span>
<span id="cb29-2"><span class="co" style="color: #5E5E5E;"># Visualize the decision tree</span></span>
<span id="cb29-3">dot_data <span class="op" style="color: #5E5E5E;">=</span> export_graphviz(tree, out_file<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, </span>
<span id="cb29-4">                           feature_names<span class="op" style="color: #5E5E5E;">=</span>data_final_vars,  </span>
<span id="cb29-5">                           class_names<span class="op" style="color: #5E5E5E;">=</span>[<span class="st" style="color: #20794D;">'Adelie'</span>, <span class="st" style="color: #20794D;">'Chinstrap'</span>, <span class="st" style="color: #20794D;">'Gentoo'</span>],  </span>
<span id="cb29-6">                           filled<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, rounded<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>,  </span>
<span id="cb29-7">                           special_characters<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)  </span>
<span id="cb29-8"></span>
<span id="cb29-9">graph <span class="op" style="color: #5E5E5E;">=</span> graphviz.Source(dot_data)  </span>
<span id="cb29-10"></span>
<span id="cb29-11"><span class="co" style="color: #5E5E5E;"># create the plot as a png file on your directory</span></span>
<span id="cb29-12"><span class="co" style="color: #5E5E5E;">#graph.render("penguins_decision_tree.png") </span></span></code></pre></div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="https://iannimuliterno.com/posts/2023-03-dectree_python/penguins_decision_tree.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Each box is called ‘node’, the node shows the rules and the subsets created, it comes together with Gini index, so we can track how pure each node gets.</p>
<p>Let’s take a better look at the Gini index: <img src="https://latex.codecogs.com/png.latex?%0AGini%20=%201%20-%20%5Csum_%7Bi=1%7D%5Ec%20(p_i)%5E2%0A"></p>
<p>Where:</p>
<ul>
<li><p><img src="https://latex.codecogs.com/png.latex?c"> is the number of classes in the target variable</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?p_i"> is the proportion of observations belonging to class i in the node</p></li>
</ul>
<p>The Gini index ranges from 0 to 1, where a Gini index of 0 means all observations in the node belong to the same class (pure node), and a Gini index of 1 means that the node contains an equal proportion of observations from each class (impure node). In general, a lower Gini index indicates a better split for the decision tree.</p>
<p>You got it! We’ve covered a lot of ground in this post, from exploring our dataset and identifying key features, to preprocessing our data and training a decision tree model to predict penguin species. And the best part? Our model performed really well. Next we will talk about random forest, until then, keep coding!</p>


</section></section> ]]></description>
  <category>Machine learning</category>
  <category>Statistics</category>
  <category>Data Science</category>
  <category>Python</category>
  <category>Classification</category>
  <guid>https://iannimuliterno.com/posts/2023-03-dectree_python/index.html</guid>
  <pubDate>Tue, 07 Mar 2023 03:00:00 GMT</pubDate>
  <media:content url="https://iannimuliterno.com/posts/2023-03-dectree_python/penguins_logo.png" medium="image" type="image/png" height="86" width="144"/>
</item>
<item>
  <title>Model 1 - Linear Regression in Python with Kaggle Data</title>
  <dc:creator>I. Muliterno</dc:creator>
  <link>https://iannimuliterno.com/posts/2023-03-lm_python/index.html</link>
  <description><![CDATA[ 




<p>In our previous post, we discussed different modeling approaches and their applications. Today, we will delve deeper into linear regression, one of the most commonly used modeling techniques in data science. By the end, you will learn when to use linear regression and how to code it from start to finish using a dataset from Kaggle.</p>
<section id="getting-to-know-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="getting-to-know-linear-regression">1. Getting to know Linear Regression</h2>
<p>Linear regression is a popular modeling approach used for predicting continuous values. It is a simple yet powerful modeling approach that <strong>works well when the relationship between the predictor and response variable is linear</strong>. It is important to ensure that the assumptions of linear regression are met, including linearity, independence, normality, and equal variance. These assumptions can be tested using various techniques, such as residual plots and statistical tests. Linear regression models are easy to interpret and can be used for both simple and complex problems.</p>
<p>Simple linear regression involves a single independent variable, while multiple linear regression involves two or more independent variables. Examples of where linear regression is commonly used include predicting housing prices, analyzing stock prices, and estimating crop yields.</p>
</section>
<section id="data-cleaning-and-visualization" class="level2">
<h2 class="anchored" data-anchor-id="data-cleaning-and-visualization">2. Data Cleaning and Visualization</h2>
<p>When working with real-world data, it is common to encounter missing or erroneous values, inconsistent formatting, and other issues. Data cleaning is the process of detecting and correcting these problems in the data to ensure that it is accurate and reliable for analysis. Visualization, on the other hand, is the process of representing data in a visual format such as graphs, charts, or maps, to help analysts identify patterns and trends.</p>
<p>For this blog post, we will be using a data set from Kaggle’s House Prices: Advanced Regression Techniques competition. This data set contains information on various attributes of residential homes in Ames, Iowa, including their sale prices. The goal of the competition is to build a model that can accurately predict the sale prices of homes based on these attributes.</p>
<p>To start, we will import the necessary libraries in Python, including Pandas for data manipulation and Matplotlib for visualization. We will then load the data set using the read_csv() function from Pandas.</p>
<div class="cell" data-layout-align="center">

</div>
<p>Now that we have loaded the data, we can begin the data cleaning process. The first step is to check for missing values in the data. We can use the isnull() function from Pandas to check for missing values and the sum() function to count the number of missing values in each column.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> pandas <span class="im" style="color: #00769E;">as</span> pd</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="co" style="color: #5E5E5E;"># remove the comment to load the dataset</span></span>
<span id="cb1-5"><span class="co" style="color: #5E5E5E;">#df = pd.read_csv("train.csv")</span></span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="co" style="color: #5E5E5E;"># Check for missing values</span></span>
<span id="cb1-8">missing_values <span class="op" style="color: #5E5E5E;">=</span> df.isnull().<span class="bu" style="color: null;">sum</span>()</span>
<span id="cb1-9"><span class="bu" style="color: null;">print</span>(missing_values)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Id                 0
MSSubClass         0
MSZoning           0
LotFrontage      259
LotArea            0
                ... 
MoSold             0
YrSold             0
SaleType           0
SaleCondition      0
SalePrice          0
Length: 81, dtype: int64</code></pre>
</div>
</div>
<p>This will give us a count of missing values in each column of the data set. We can then decide how to handle these missing values, depending on the amount of missing data and the nature of the problem. In this case, we will simply drop the columns with more than 50% missing values.</p>
<p>Next, we can check for any duplicate rows in the data set using the duplicated() function from Pandas. If there are any duplicate rows, we can drop them using the drop_duplicates() function.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"></span>
<span id="cb3-2"></span>
<span id="cb3-3"><span class="co" style="color: #5E5E5E;"># Drop columns with more than 50% missing values</span></span>
<span id="cb3-4">df <span class="op" style="color: #5E5E5E;">=</span> df.dropna(thresh<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">len</span>(df)<span class="op" style="color: #5E5E5E;">*</span><span class="fl" style="color: #AD0000;">0.5</span>, axis<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb3-5"></span>
<span id="cb3-6"><span class="co" style="color: #5E5E5E;"># Check for duplicates</span></span>
<span id="cb3-7">duplicates <span class="op" style="color: #5E5E5E;">=</span> df.duplicated()</span>
<span id="cb3-8"><span class="bu" style="color: null;">print</span>(duplicates.<span class="bu" style="color: null;">sum</span>())</span>
<span id="cb3-9"></span>
<span id="cb3-10"><span class="co" style="color: #5E5E5E;"># Drop duplicates</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">df <span class="op" style="color: #5E5E5E;">=</span> df.drop_duplicates()</span></code></pre></div>
</div>
<p>Now that we have cleaned the data, we can move on to visualization. One common visualization for exploring the relationship between two variables is a scatter plot. We can create a scatter plot of the sale prices and the living area of the homes using Matplotlib.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="co" style="color: #5E5E5E;"># Create a histogram of sale prices</span></span>
<span id="cb6-2">plt.hist(df[<span class="st" style="color: #20794D;">"SalePrice"</span>], bins<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">20</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(array([ 22., 126., 380., 343., 229., 144.,  86.,  49.,  28.,  23.,  12.,
         7.,   3.,   1.,   2.,   1.,   2.,   0.,   0.,   2.]), array([ 34900.,  70905., 106910., 142915., 178920., 214925., 250930.,
       286935., 322940., 358945., 394950., 430955., 466960., 502965.,
       538970., 574975., 610980., 646985., 682990., 718995., 755000.]), &lt;BarContainer object of 20 artists&gt;)</code></pre>
</div>
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">plt.xlabel(<span class="st" style="color: #20794D;">"Sale Price ($)"</span>)</span>
<span id="cb8-2">plt.ylabel(<span class="st" style="color: #20794D;">"Frequency"</span>)</span>
<span id="cb8-3">plt.show()</span></code></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://iannimuliterno.com/posts/2023-03-lm_python/index_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>This will give us a visual representation of the distribution of sale prices in the data set. We can see that the distribution is skewed to the right, with a few homes having very high sale prices.</p>
<p>By cleaning and visualizing the data, we can gain a better understanding of its properties and identify any potential issues that may need to be addressed before building a linear regression model.</p>
</section>
<section id="building-a-linear-regression-model" class="level2">
<h2 class="anchored" data-anchor-id="building-a-linear-regression-model">3. Building a Linear Regression Model</h2>
<p>Now that we have cleaned and visualized the data, we can start building a linear regression model to predict the sale prices of homes based on their attributes. Linear regression is a statistical technique that is commonly used for predicting a numeric value based on one or more input variables. In this case, we will use the input variables in the data set to predict the sale price of each home.</p>
<p>To start, we will split the data set into a training set and a validation set using the train_test_split() function from Scikit-learn. The training set will be used to train the model, while the validation set will be used to evaluate the model’s performance.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"></span>
<span id="cb9-2"><span class="im" style="color: #00769E;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;">import</span> train_test_split</span>
<span id="cb9-3"></span>
<span id="cb9-4"><span class="co" style="color: #5E5E5E;"># Split the data set into training and validation sets</span></span>
<span id="cb9-5">train, val <span class="op" style="color: #5E5E5E;">=</span> train_test_split(df, test_size<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.2</span>, random_state<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">42</span>)</span></code></pre></div>
</div>
<p>Next, we will select the input variables that we want to use in the model. In this case, we will use the living area, number of bedrooms, and number of bathrooms as input variables.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="co" style="color: #5E5E5E;"># Select the input variables</span></span>
<span id="cb10-2">X_train <span class="op" style="color: #5E5E5E;">=</span> train[[<span class="st" style="color: #20794D;">"GrLivArea"</span>, <span class="st" style="color: #20794D;">"BedroomAbvGr"</span>, <span class="st" style="color: #20794D;">"FullBath"</span>]]</span>
<span id="cb10-3">y_train <span class="op" style="color: #5E5E5E;">=</span> train[<span class="st" style="color: #20794D;">"SalePrice"</span>]</span>
<span id="cb10-4"></span>
<span id="cb10-5">X_val <span class="op" style="color: #5E5E5E;">=</span> val[[<span class="st" style="color: #20794D;">"GrLivArea"</span>, <span class="st" style="color: #20794D;">"BedroomAbvGr"</span>, <span class="st" style="color: #20794D;">"FullBath"</span>]]</span>
<span id="cb10-6">y_val <span class="op" style="color: #5E5E5E;">=</span> val[<span class="st" style="color: #20794D;">"SalePrice"</span>]</span></code></pre></div>
</div>
<p>We can then build a linear regression model using the LinearRegression() function from Scikit-learn. We can use it to predict the sale prices of homes in the validation set using the predict() function.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"></span>
<span id="cb11-2"><span class="im" style="color: #00769E;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;">import</span> LinearRegression</span>
<span id="cb11-3"></span>
<span id="cb11-4"><span class="co" style="color: #5E5E5E;"># Build the linear regression model</span></span>
<span id="cb11-5">model <span class="op" style="color: #5E5E5E;">=</span> LinearRegression()</span>
<span id="cb11-6">model.fit(X_train, y_train)</span>
<span id="cb11-7"></span>
<span id="cb11-8"><span class="co" style="color: #5E5E5E;"># Predict the sale prices of homes in the validation set</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>LinearRegression()</code></pre>
</div>
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">y_pred <span class="op" style="color: #5E5E5E;">=</span> model.predict(X_val)</span></code></pre></div>
</div>
<p>To evaluate the performance of the model, we can calculate the mean squared error (MSE) and the coefficient of determination (R-squared) between the predicted sale prices and the actual sale prices in the validation set using the mean_squared_error() and r2_score() functions from Scikit-learn.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="im" style="color: #00769E;">from</span> sklearn.metrics <span class="im" style="color: #00769E;">import</span> mean_squared_error, r2_score</span>
<span id="cb14-2"></span>
<span id="cb14-3"><span class="co" style="color: #5E5E5E;"># Calculate the mean squared error and R-squared</span></span>
<span id="cb14-4">mse <span class="op" style="color: #5E5E5E;">=</span> mean_squared_error(y_val, y_pred)</span>
<span id="cb14-5">r2 <span class="op" style="color: #5E5E5E;">=</span> r2_score(y_val, y_pred)</span>
<span id="cb14-6"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"MSE:"</span>, mse)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE: 2806426667.247853</code></pre>
</div>
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"R-squared:"</span>, r2)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>R-squared: 0.6341189942328371</code></pre>
</div>
</div>
<p>A low mean squared error and a high coefficient of determination indicate that the model is accurately predicting the sale prices of homes based on their attributes.</p>
<p>By building a linear regression model, we can use the input variables in the data set to predict the sale prices of homes with a high degree of accuracy. This information can be useful for real estate professionals, home buyers, and sellers looking to estimate the value of a residential property.</p>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>In this post, we have explored the process of using linear regression to predict the sale prices of homes based on their attributes. We started by cleaning and visualizing the data to gain insights into the relationships between the input variables and the sale prices. We then built a linear regression model using the Scikit-learn library and evaluated its performance using the mean squared error and coefficient of determination.</p>
<p>By following this process, we can make accurate predictions about the sale prices of homes based on their attributes. This information can be valuable for a variety of applications, including real estate valuation, mortgage underwriting, and investment analysis.</p>
<p>As with any predictive model, it is important to continually evaluate and refine the model over time to ensure that it is accurately predicting the outcome of interest. With continued effort, we can refine our understanding of the relationship between the input variables and the sale prices of homes, and improve the accuracy of our predictions.</p>


</section>

 ]]></description>
  <category>Machine learning</category>
  <category>Statistics</category>
  <category>Data Science</category>
  <category>Linear Regression</category>
  <category>Python</category>
  <guid>https://iannimuliterno.com/posts/2023-03-lm_python/index.html</guid>
  <pubDate>Mon, 06 Mar 2023 03:00:00 GMT</pubDate>
  <media:content url="https://iannimuliterno.com/posts/2023-03-lm_python/housesbanner.png" medium="image" type="image/png" height="25" width="144"/>
</item>
<item>
  <title>Modeling Approaches</title>
  <dc:creator>Ianní Muliterno</dc:creator>
  <link>https://iannimuliterno.com/posts/2023-03-models_and_applications/index.html</link>
  <description><![CDATA[ <p>As a senior data scientist, one of the key responsibilities is to identify the right modeling approach for a given problem. Different modeling approaches have different strengths and weaknesses, and choosing the right approach is crucial to building an effective and accurate model. This post is the first in a series of posts on modeling approaches, I will discuss the strengths and weaknesses of some popular modeling approaches and when to apply different combinations.</p>
<p>This post is the first in a series of posts on modeling approaches</p>
<section id="linear-regression" class="level2"><h2 class="anchored" data-anchor-id="linear-regression">1. Linear Regression</h2>
<p>Linear regression is a popular modeling approach used for predicting continuous values. It is a simple yet powerful modeling approach that works well when the relationship between the predictor and response variable is linear. Linear regression models are easy to interpret and can be used for both simple and complex problems.</p>
<section id="strengths" class="level3"><h3 class="anchored" data-anchor-id="strengths">Strengths:</h3>
<ul>
<li><p>Easy to interpret and explain.</p></li>
<li><p>Works well when the relationship between predictor and response variable is linear.</p></li>
<li><p>Can handle both simple and complex problems.</p></li>
</ul></section><section id="weaknesses" class="level3"><h3 class="anchored" data-anchor-id="weaknesses">Weaknesses:</h3>
<ul>
<li><p>Assumes a linear relationship between predictor and response variable.</p></li>
<li><p>Sensitive to outliers and multicollinearity.</p></li>
</ul></section><section id="when-to-use" class="level3"><h3 class="anchored" data-anchor-id="when-to-use">When to use:</h3>
<ul>
<li><p>When the relationship between predictor and response variable is linear.</p></li>
<li><p>When the data has few predictors.</p></li>
<li><p>When the data has no multicollinearity or outliers.</p></li>
</ul></section></section><section id="decision-trees" class="level2"><h2 class="anchored" data-anchor-id="decision-trees">2. Decision Trees</h2>
<p>Decision trees are a popular modeling approach used for classification and regression. They work by partitioning the data into smaller subsets based on the values of the predictors. Decision trees are easy to interpret and can handle both categorical and continuous predictors. Personally, I prefer to apply only for categorical predictors.</p>
<section id="strengths-1" class="level3"><h3 class="anchored" data-anchor-id="strengths-1">Strengths:</h3>
<ul>
<li><p>Easy to interpret and explain.</p></li>
<li><p>Can handle both categorical and continuous predictors.</p></li>
<li><p>Can handle interactions between predictors.</p></li>
</ul></section><section id="weaknesses-1" class="level3"><h3 class="anchored" data-anchor-id="weaknesses-1">Weaknesses:</h3>
<ul>
<li><p>Can overfit the data.</p></li>
<li><p>Sensitive to small changes in the data.</p></li>
</ul></section><section id="when-to-use-1" class="level3"><h3 class="anchored" data-anchor-id="when-to-use-1">When to use:</h3>
<ul>
<li><p>When the data has many predictors.</p></li>
<li><p>When the data has interactions between predictors.</p></li>
<li><p>When the data has both categorical and continuous predictors.</p></li>
</ul></section></section><section id="random-forests" class="level2"><h2 class="anchored" data-anchor-id="random-forests">3. Random Forests</h2>
<p>Random forests are an extension of decision trees that work by combining multiple decision trees to make a final prediction. They are a popular modeling approach used for classification and regression. Random forests are robust to overfitting and can handle both categorical and continuous predictors.</p>
<section id="strengths-2" class="level3"><h3 class="anchored" data-anchor-id="strengths-2">Strengths:</h3>
<ul>
<li><p>Robust to overfitting.</p></li>
<li><p>Can handle both categorical and continuous predictors.</p></li>
<li><p>Can handle interactions between predictors.</p></li>
</ul></section><section id="weaknesses-2" class="level3"><h3 class="anchored" data-anchor-id="weaknesses-2">Weaknesses:</h3>
<ul>
<li><p>Can be slow to train and predict.</p></li>
<li><p>Can be difficult to interpret.</p></li>
</ul></section><section id="when-to-use-2" class="level3"><h3 class="anchored" data-anchor-id="when-to-use-2">When to use:</h3>
<ul>
<li><p>When the data has many predictors.</p></li>
<li><p>When the data has interactions between predictors.</p></li>
<li><p>When the data has both categorical and continuous predictors.</p></li>
<li><p>When the data has outliers or missing values.</p></li>
</ul></section></section><section id="neural-networks" class="level2"><h2 class="anchored" data-anchor-id="neural-networks">4. Neural Networks</h2>
<p>Neural networks are a popular modeling approach used for classification and regression. They work by mimicking the structure of the human brain to identify complex patterns in the data. Neural networks are powerful and can handle both linear and nonlinear relationships between predictors and response variables.</p>
<section id="strengths-3" class="level3"><h3 class="anchored" data-anchor-id="strengths-3">Strengths:</h3>
<ul>
<li><p>Can handle both linear and nonlinear relationships between predictors and response variables.</p></li>
<li><p>Can identify complex patterns in the data.</p></li>
<li><p>Can handle large and complex datasets.</p></li>
</ul></section><section id="weaknesses-3" class="level3"><h3 class="anchored" data-anchor-id="weaknesses-3">Weaknesses:</h3>
<ul>
<li><p>Can be slow to train and predict.</p></li>
<li><p>Can overfit the data.</p></li>
</ul></section><section id="when-to-use-3" class="level3"><h3 class="anchored" data-anchor-id="when-to-use-3">When to use:</h3>
<ul>
<li><p>When the data has many predictors.</p></li>
<li><p>When the data has complex relationships between predictors and response variables.</p></li>
<li><p>When the data has large and complex datasets.</p></li>
</ul></section></section><section id="support-vector-machines" class="level2"><h2 class="anchored" data-anchor-id="support-vector-machines">5. Support Vector Machines</h2>
<p>Support Vector Machines (SVMs) are a popular modeling approach used for classification and regression. They work by finding the hyperplane that best separates the data into different classes. SVMs are powerful and can handle both linear and nonlinear relationships between predictors and response variables.</p>
<section id="strengths-4" class="level3"><h3 class="anchored" data-anchor-id="strengths-4">Strengths:</h3>
<ul>
<li><p>Can handle both linear and nonlinear relationships between predictors and response variables.</p></li>
<li><p>Can handle high-dimensional datasets.</p></li>
<li><p>Can handle outliers.</p></li>
</ul></section><section id="weaknesses-4" class="level3"><h3 class="anchored" data-anchor-id="weaknesses-4">Weaknesses:</h3>
<ul>
<li><p>Can be sensitive to the choice of kernel function.</p></li>
<li><p>Can be slow to train and predict.</p></li>
</ul></section><section id="when-to-use-4" class="level3"><h3 class="anchored" data-anchor-id="when-to-use-4">When to use:</h3>
<ul>
<li>When the data has many predictors.</li>
</ul>
<!-- --><ul>
<li><p>When the data has both linear and nonlinear relationships between predictors and response variables.</p></li>
<li><p>When the data has outliers.</p></li>
</ul>
<p>In practice, different modeling approaches may be combined to build more accurate and robust models. For example, a decision tree model may be combined with a random forest model to improve accuracy and reduce overfitting. Or, a linear regression model may be combined with a support vector machine model to handle both linear and nonlinear relationships between predictors and response variables.</p>
<p>When choosing a modeling approach or combination of approaches, it is important to consider the strengths and weaknesses of each approach in relation to the specific problem at hand. It is also important to consider factors such as the size and complexity of the data, the level of interpretability required, and the computational resources available.</p>
<p>In summary, there are a variety of modeling approaches that can be used in data science, each with its own strengths and weaknesses. By carefully considering the specific problem at hand and choosing the right combination of modeling approaches, data scientists can build accurate and robust models that provide valuable insights and predictions.</p>


</section></section> ]]></description>
  <category>Machine learning</category>
  <category>Statistics</category>
  <category>Data Science</category>
  <guid>https://iannimuliterno.com/posts/2023-03-models_and_applications/index.html</guid>
  <pubDate>Wed, 01 Mar 2023 03:00:00 GMT</pubDate>
</item>
</channel>
</rss>
