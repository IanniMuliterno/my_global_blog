[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IMuliterno",
    "section": "",
    "text": "Data Science\n\n\nR\n\n\nsports analytics\n\n\nrace\n\n\n \n\n\n\n\nJan 25, 2024\n\n\nI. Muliterno\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nData Science\n\n\nR\n\n\nHarry Potter\n\n\ntext mining\n\n\n \n\n\n\n\nNov 20, 2023\n\n\nI. Muliterno\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nData Science\n\n\nR\n\n\nrandom forest\n\n\nMachine Learning\n\n\n \n\n\n\n\nMay 28, 2023\n\n\nI. Muliterno\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nData Science\n\n\nR\n\n\nDDC\n\n\nmissing\n\n\ninconsistent\n\n\n \n\n\n\n\nApr 24, 2023\n\n\nI. Muliterno\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nData Science\n\n\nR\n\n\nDDC\n\n\nmissing\n\n\ninconsistent\n\n\n \n\n\n\n\nApr 24, 2023\n\n\nI. Muliterno\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMachine learning\n\n\nStatistics\n\n\nData Science\n\n\nPython\n\n\nClassification\n\n\n \n\n\n\n\nMar 7, 2023\n\n\nI. Muliterno\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMachine learning\n\n\nStatistics\n\n\nData Science\n\n\nLinear Regression\n\n\nPython\n\n\n \n\n\n\n\nMar 6, 2023\n\n\nI. Muliterno\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine learning\n\n\nStatistics\n\n\nData Science\n\n\n \n\n\n\n\nMar 1, 2023\n\n\nIanní Muliterno\n\n\n4 min\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n See all"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a data scientist and this is an extension of my portfolio. You can check more of my work here\nMy expertise lies in Machine learning, AI engineering and developing and implementing data-driven solutions to drive business growth and enhance performance. If you speak portuguese, you might enjoy my substrack and instagram (@muliterno_) as well."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nHorse Race and Data Science\n\n\n\nJan 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText mining on Harry Potter dialogues\n\n\n\nNov 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 3 - Random Forest in R with Breast Cancer Wisconsin (Diagnostic) dataset\n\n\n\nMay 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDDC: Tackling Missing or Inconsistent Data\n\n\n\nApr 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDDC: Tackling Missing or Inconsistent Data\n\n\n\nApr 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 2 - Decision Trees in Python with Penguin dataset\n\n\n\nMar 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 1 - Linear Regression in Python with Kaggle Data\n\n\n\nMar 6, 2023\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nModeling Approaches\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-03-models_and_applications/index.html",
    "href": "posts/2023-03-models_and_applications/index.html",
    "title": "Modeling Approaches",
    "section": "",
    "text": "As a senior data scientist, one of the key responsibilities is to identify the right modeling approach for a given problem. Different modeling approaches have different strengths and weaknesses, and choosing the right approach is crucial to building an effective and accurate model. This post is the first in a series of posts on modeling approaches, I will discuss the strengths and weaknesses of some popular modeling approaches and when to apply different combinations.\nThis post is the first in a series of posts on modeling approaches"
  },
  {
    "objectID": "posts/2023-03-models_and_applications/index.html#linear-regression",
    "href": "posts/2023-03-models_and_applications/index.html#linear-regression",
    "title": "Modeling Approaches",
    "section": "1. Linear Regression",
    "text": "1. Linear Regression\nLinear regression is a popular modeling approach used for predicting continuous values. It is a simple yet powerful modeling approach that works well when the relationship between the predictor and response variable is linear. Linear regression models are easy to interpret and can be used for both simple and complex problems.\nStrengths:\n\nEasy to interpret and explain.\nWorks well when the relationship between predictor and response variable is linear.\nCan handle both simple and complex problems.\nWeaknesses:\n\nAssumes a linear relationship between predictor and response variable.\nSensitive to outliers and multicollinearity.\nWhen to use:\n\nWhen the relationship between predictor and response variable is linear.\nWhen the data has few predictors.\nWhen the data has no multicollinearity or outliers."
  },
  {
    "objectID": "posts/2023-03-models_and_applications/index.html#decision-trees",
    "href": "posts/2023-03-models_and_applications/index.html#decision-trees",
    "title": "Modeling Approaches",
    "section": "2. Decision Trees",
    "text": "2. Decision Trees\nDecision trees are a popular modeling approach used for classification and regression. They work by partitioning the data into smaller subsets based on the values of the predictors. Decision trees are easy to interpret and can handle both categorical and continuous predictors. Personally, I prefer to apply only for categorical predictors.\nStrengths:\n\nEasy to interpret and explain.\nCan handle both categorical and continuous predictors.\nCan handle interactions between predictors.\nWeaknesses:\n\nCan overfit the data.\nSensitive to small changes in the data.\nWhen to use:\n\nWhen the data has many predictors.\nWhen the data has interactions between predictors.\nWhen the data has both categorical and continuous predictors."
  },
  {
    "objectID": "posts/2023-03-models_and_applications/index.html#random-forests",
    "href": "posts/2023-03-models_and_applications/index.html#random-forests",
    "title": "Modeling Approaches",
    "section": "3. Random Forests",
    "text": "3. Random Forests\nRandom forests are an extension of decision trees that work by combining multiple decision trees to make a final prediction. They are a popular modeling approach used for classification and regression. Random forests are robust to overfitting and can handle both categorical and continuous predictors.\nStrengths:\n\nRobust to overfitting.\nCan handle both categorical and continuous predictors.\nCan handle interactions between predictors.\nWeaknesses:\n\nCan be slow to train and predict.\nCan be difficult to interpret.\nWhen to use:\n\nWhen the data has many predictors.\nWhen the data has interactions between predictors.\nWhen the data has both categorical and continuous predictors.\nWhen the data has outliers or missing values."
  },
  {
    "objectID": "posts/2023-03-models_and_applications/index.html#neural-networks",
    "href": "posts/2023-03-models_and_applications/index.html#neural-networks",
    "title": "Modeling Approaches",
    "section": "4. Neural Networks",
    "text": "4. Neural Networks\nNeural networks are a popular modeling approach used for classification and regression. They work by mimicking the structure of the human brain to identify complex patterns in the data. Neural networks are powerful and can handle both linear and nonlinear relationships between predictors and response variables.\nStrengths:\n\nCan handle both linear and nonlinear relationships between predictors and response variables.\nCan identify complex patterns in the data.\nCan handle large and complex datasets.\nWeaknesses:\n\nCan be slow to train and predict.\nCan overfit the data.\nWhen to use:\n\nWhen the data has many predictors.\nWhen the data has complex relationships between predictors and response variables.\nWhen the data has large and complex datasets."
  },
  {
    "objectID": "posts/2023-03-models_and_applications/index.html#support-vector-machines",
    "href": "posts/2023-03-models_and_applications/index.html#support-vector-machines",
    "title": "Modeling Approaches",
    "section": "5. Support Vector Machines",
    "text": "5. Support Vector Machines\nSupport Vector Machines (SVMs) are a popular modeling approach used for classification and regression. They work by finding the hyperplane that best separates the data into different classes. SVMs are powerful and can handle both linear and nonlinear relationships between predictors and response variables.\nStrengths:\n\nCan handle both linear and nonlinear relationships between predictors and response variables.\nCan handle high-dimensional datasets.\nCan handle outliers.\nWeaknesses:\n\nCan be sensitive to the choice of kernel function.\nCan be slow to train and predict.\nWhen to use:\n\nWhen the data has many predictors.\n\n\nWhen the data has both linear and nonlinear relationships between predictors and response variables.\nWhen the data has outliers.\n\nIn practice, different modeling approaches may be combined to build more accurate and robust models. For example, a decision tree model may be combined with a random forest model to improve accuracy and reduce overfitting. Or, a linear regression model may be combined with a support vector machine model to handle both linear and nonlinear relationships between predictors and response variables.\nWhen choosing a modeling approach or combination of approaches, it is important to consider the strengths and weaknesses of each approach in relation to the specific problem at hand. It is also important to consider factors such as the size and complexity of the data, the level of interpretability required, and the computational resources available.\nIn summary, there are a variety of modeling approaches that can be used in data science, each with its own strengths and weaknesses. By carefully considering the specific problem at hand and choosing the right combination of modeling approaches, data scientists can build accurate and robust models that provide valuable insights and predictions."
  },
  {
    "objectID": "posts/2023-03-lm_python/index.html",
    "href": "posts/2023-03-lm_python/index.html",
    "title": "Model 1 - Linear Regression in Python with Kaggle Data",
    "section": "",
    "text": "In our previous post, we discussed different modeling approaches and their applications. Today, we will delve deeper into linear regression, one of the most commonly used modeling techniques in data science. By the end, you will learn when to use linear regression and how to code it from start to finish using a dataset from Kaggle."
  },
  {
    "objectID": "posts/2023-03-lm_python/index.html#getting-to-know-linear-regression",
    "href": "posts/2023-03-lm_python/index.html#getting-to-know-linear-regression",
    "title": "Model 1 - Linear Regression in Python with Kaggle Data",
    "section": "1. Getting to know Linear Regression",
    "text": "1. Getting to know Linear Regression\nLinear regression is a popular modeling approach used for predicting continuous values. It is a simple yet powerful modeling approach that works well when the relationship between the predictor and response variable is linear. It is important to ensure that the assumptions of linear regression are met, including linearity, independence, normality, and equal variance. These assumptions can be tested using various techniques, such as residual plots and statistical tests. Linear regression models are easy to interpret and can be used for both simple and complex problems.\nSimple linear regression involves a single independent variable, while multiple linear regression involves two or more independent variables. Examples of where linear regression is commonly used include predicting housing prices, analyzing stock prices, and estimating crop yields."
  },
  {
    "objectID": "posts/2023-03-lm_python/index.html#data-cleaning-and-visualization",
    "href": "posts/2023-03-lm_python/index.html#data-cleaning-and-visualization",
    "title": "Model 1 - Linear Regression in Python with Kaggle Data",
    "section": "2. Data Cleaning and Visualization",
    "text": "2. Data Cleaning and Visualization\nWhen working with real-world data, it is common to encounter missing or erroneous values, inconsistent formatting, and other issues. Data cleaning is the process of detecting and correcting these problems in the data to ensure that it is accurate and reliable for analysis. Visualization, on the other hand, is the process of representing data in a visual format such as graphs, charts, or maps, to help analysts identify patterns and trends.\nFor this blog post, we will be using a data set from Kaggle’s House Prices: Advanced Regression Techniques competition. This data set contains information on various attributes of residential homes in Ames, Iowa, including their sale prices. The goal of the competition is to build a model that can accurately predict the sale prices of homes based on these attributes.\nTo start, we will import the necessary libraries in Python, including Pandas for data manipulation and Matplotlib for visualization. We will then load the data set using the read_csv() function from Pandas.\n\n\n\nNow that we have loaded the data, we can begin the data cleaning process. The first step is to check for missing values in the data. We can use the isnull() function from Pandas to check for missing values and the sum() function to count the number of missing values in each column.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# remove the comment to load the dataset\n#df = pd.read_csv(\"train.csv\")\n\n# Check for missing values\nmissing_values = df.isnull().sum()\nprint(missing_values)\n\nId                 0\nMSSubClass         0\nMSZoning           0\nLotFrontage      259\nLotArea            0\n                ... \nMoSold             0\nYrSold             0\nSaleType           0\nSaleCondition      0\nSalePrice          0\nLength: 81, dtype: int64\n\n\nThis will give us a count of missing values in each column of the data set. We can then decide how to handle these missing values, depending on the amount of missing data and the nature of the problem. In this case, we will simply drop the columns with more than 50% missing values.\nNext, we can check for any duplicate rows in the data set using the duplicated() function from Pandas. If there are any duplicate rows, we can drop them using the drop_duplicates() function.\n\n\n\n# Drop columns with more than 50% missing values\ndf = df.dropna(thresh=len(df)*0.5, axis=1)\n\n# Check for duplicates\nduplicates = df.duplicated()\nprint(duplicates.sum())\n\n# Drop duplicates\n\n0\n\ndf = df.drop_duplicates()\n\nNow that we have cleaned the data, we can move on to visualization. One common visualization for exploring the relationship between two variables is a scatter plot. We can create a scatter plot of the sale prices and the living area of the homes using Matplotlib.\n\n# Create a histogram of sale prices\nplt.hist(df[\"SalePrice\"], bins=20)\n\n(array([ 22., 126., 380., 343., 229., 144.,  86.,  49.,  28.,  23.,  12.,\n         7.,   3.,   1.,   2.,   1.,   2.,   0.,   0.,   2.]), array([ 34900.,  70905., 106910., 142915., 178920., 214925., 250930.,\n       286935., 322940., 358945., 394950., 430955., 466960., 502965.,\n       538970., 574975., 610980., 646985., 682990., 718995., 755000.]), <BarContainer object of 20 artists>)\n\nplt.xlabel(\"Sale Price ($)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\n\n\n\n\nThis will give us a visual representation of the distribution of sale prices in the data set. We can see that the distribution is skewed to the right, with a few homes having very high sale prices.\nBy cleaning and visualizing the data, we can gain a better understanding of its properties and identify any potential issues that may need to be addressed before building a linear regression model."
  },
  {
    "objectID": "posts/2023-03-lm_python/index.html#summary",
    "href": "posts/2023-03-lm_python/index.html#summary",
    "title": "Model 1 - Linear Regression in Python with Kaggle Data",
    "section": "Summary",
    "text": "Summary\nIn this post, we have explored the process of using linear regression to predict the sale prices of homes based on their attributes. We started by cleaning and visualizing the data to gain insights into the relationships between the input variables and the sale prices. We then built a linear regression model using the Scikit-learn library and evaluated its performance using the mean squared error and coefficient of determination.\nBy following this process, we can make accurate predictions about the sale prices of homes based on their attributes. This information can be valuable for a variety of applications, including real estate valuation, mortgage underwriting, and investment analysis.\nAs with any predictive model, it is important to continually evaluate and refine the model over time to ensure that it is accurately predicting the outcome of interest. With continued effort, we can refine our understanding of the relationship between the input variables and the sale prices of homes, and improve the accuracy of our predictions."
  },
  {
    "objectID": "posts/2023-03-lm_python/index.html#building-a-linear-regression-model",
    "href": "posts/2023-03-lm_python/index.html#building-a-linear-regression-model",
    "title": "Model 1 - Linear Regression in Python with Kaggle Data",
    "section": "3. Building a Linear Regression Model",
    "text": "3. Building a Linear Regression Model\nNow that we have cleaned and visualized the data, we can start building a linear regression model to predict the sale prices of homes based on their attributes. Linear regression is a statistical technique that is commonly used for predicting a numeric value based on one or more input variables. In this case, we will use the input variables in the data set to predict the sale price of each home.\nTo start, we will split the data set into a training set and a validation set using the train_test_split() function from Scikit-learn. The training set will be used to train the model, while the validation set will be used to evaluate the model’s performance.\n\n\nfrom sklearn.model_selection import train_test_split\n\n# Split the data set into training and validation sets\ntrain, val = train_test_split(df, test_size=0.2, random_state=42)\n\nNext, we will select the input variables that we want to use in the model. In this case, we will use the living area, number of bedrooms, and number of bathrooms as input variables.\n\n# Select the input variables\nX_train = train[[\"GrLivArea\", \"BedroomAbvGr\", \"FullBath\"]]\ny_train = train[\"SalePrice\"]\n\nX_val = val[[\"GrLivArea\", \"BedroomAbvGr\", \"FullBath\"]]\ny_val = val[\"SalePrice\"]\n\nWe can then build a linear regression model using the LinearRegression() function from Scikit-learn. We can use it to predict the sale prices of homes in the validation set using the predict() function.\n\n\nfrom sklearn.linear_model import LinearRegression\n\n# Build the linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predict the sale prices of homes in the validation set\n\nLinearRegression()\n\ny_pred = model.predict(X_val)\n\nTo evaluate the performance of the model, we can calculate the mean squared error (MSE) and the coefficient of determination (R-squared) between the predicted sale prices and the actual sale prices in the validation set using the mean_squared_error() and r2_score() functions from Scikit-learn.\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Calculate the mean squared error and R-squared\nmse = mean_squared_error(y_val, y_pred)\nr2 = r2_score(y_val, y_pred)\nprint(\"MSE:\", mse)\n\nMSE: 2806426667.247853\n\nprint(\"R-squared:\", r2)\n\nR-squared: 0.6341189942328371\n\n\nA low mean squared error and a high coefficient of determination indicate that the model is accurately predicting the sale prices of homes based on their attributes.\nBy building a linear regression model, we can use the input variables in the data set to predict the sale prices of homes with a high degree of accuracy. This information can be useful for real estate professionals, home buyers, and sellers looking to estimate the value of a residential property."
  },
  {
    "objectID": "posts/2023-03-dectree_python/index.html",
    "href": "posts/2023-03-dectree_python/index.html",
    "title": "Model 2 - Decision Trees in Python with Penguin dataset",
    "section": "",
    "text": "In our previous post, we discussed linear regression. Today, we will get familiar with decisions trees, but before that, you gotta understand what’s a classification problem.\nClassification is an important task in machine learning, with numerous applications in fields such as healthcare, finance, and marketing. One popular classification algorithm is decision trees, which use a tree-like model of decisions and their possible consequences to predict the target variable. In this post, we will provide a step-by-step guide to implementing decision trees for classification using Python and the Penguin dataset."
  },
  {
    "objectID": "posts/2023-03-dectree_python/index.html#modeling-using-decision-trees",
    "href": "posts/2023-03-dectree_python/index.html#modeling-using-decision-trees",
    "title": "Model 2 - Decision Trees in Python with Penguin dataset",
    "section": "Modeling using Decision Trees",
    "text": "Modeling using Decision Trees\nNow let’s train our decision tree model to predict the species of penguins based on the independent variables we identified in the previous section.\nThe first step in model training is to split the dataset into training and testing sets. We used the train_test_split function from scikit-learn library to randomly split the data into 80% for training and 20% for testing. This step is important to ensure that the model is not overfitting to the data, meaning that it is not just memorizing the training data but can generalize well to new, unseen data.\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nNext, we built the decision tree model using the DecisionTreeClassifier function from scikit-learn library. We set the maximum depth of the tree to 3 and used a random state of 42 to ensure that our results are reproducible. The max_depth parameter controls the complexity of the tree, and we chose 3 to balance between underfitting and overfitting.\nAfter building the model, we made predictions on the test set using the predict function of the decision tree classifier. These predictions will be used to evaluate the performance of our model in the next section.\n\n\n# Build the decision tree model\ntree = DecisionTreeClassifier(max_depth=3, random_state=42)\ntree.fit(X_train, y_train)\n\n# Make predictions on the test set\n\nDecisionTreeClassifier(max_depth=3, random_state=42)\n\ny_pred = tree.predict(X_test)"
  },
  {
    "objectID": "posts/2023-03-dectree_python/index.html#performance",
    "href": "posts/2023-03-dectree_python/index.html#performance",
    "title": "Model 2 - Decision Trees in Python with Penguin dataset",
    "section": "Performance",
    "text": "Performance\nAfter building our decision tree model, it is important to evaluate its performance. We can use various metrics to evaluate the model, including accuracy, precision, recall, and the confusion matrix. Before getting to the code, let’s understand the metrics.\nThe accuracy score tells us the percentage of correctly classified instances out of all instances.\naccuracy = (number of correct predictions) / (total number of predictions)\nThe precision score tells us the percentage of instances that were correctly classified as a certain class out of all instances classified as that class.\nPrecision = (true positives) / (true positives + false positives)\nThe recall score tells us the percentage of instances that were correctly classified as a certain class out of all instances of that class.\nRecall = (true positives) / (true positives + false negatives)\nF1 score is the harmonic mean of precision and recall. It is a measure of the balance between precision and recall and is a useful metric when dealing with imbalanced datasets.\nF1 score = 2 * ((precision * recall) / (precision + recall))\nIn these formulas, true positives are the number of correctly predicted instances of a particular class, false positives are the number of instances that were incorrectly predicted as that class, and false negatives are the number of instances of that class that were incorrectly predicted as another class.\nThe confusion matrix is a table that shows the number of true positives, true negatives, false positives, and false negatives for each class.\n\n\n# Evaluate the model\nprint(\"Accuracy: {:.2f}%\".format(100 * accuracy_score(y_test, y_pred)))\n\nAccuracy: 97.01%\n\nprint(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred, average='weighted')))\n\nPrecision: 97.20%\n\nprint(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred, average='weighted')))\n\nRecall: 97.01%\n\nprint(\"F1 score: {:.2f}%\".format(100 * f1_score(y_test, y_pred, average='macro')))\n\nF1 score: 97.00%\n\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n\nConfusion Matrix:\n [[31  0  0]\n [ 2 16  0]\n [ 0  0 18]]\n\n\nWe can see that our model has an accuracy of 97%, which means it correctly classified 97% of instances. The precision and recall scores are also high, indicating that our model performed well in classifying instances for each class. The confusion matrix provides us with more detailed information about the number of correctly and incorrectly classified instances for each class.\nLet’s take a look at the decision tree we made.\n\n\n# Visualize the decision tree\ndot_data = export_graphviz(tree, out_file=None, \n                           feature_names=data_final_vars,  \n                           class_names=['Adelie', 'Chinstrap', 'Gentoo'],  \n                           filled=True, rounded=True,  \n                           special_characters=True)  \n\ngraph = graphviz.Source(dot_data)  \n\n# create the plot as a png file on your directory\n#graph.render(\"penguins_decision_tree.png\") \n\n\n\n\n\nEach box is called ‘node’, the node shows the rules and the subsets created, it comes together with Gini index, so we can track how pure each node gets.\nLet’s take a better look at the Gini index: \\[\nGini = 1 - \\sum_{i=1}^c (p_i)^2\n\\]\nWhere:\n\n\\(c\\) is the number of classes in the target variable\n\\(p_i\\) is the proportion of observations belonging to class i in the node\n\nThe Gini index ranges from 0 to 1, where a Gini index of 0 means all observations in the node belong to the same class (pure node), and a Gini index of 1 means that the node contains an equal proportion of observations from each class (impure node). In general, a lower Gini index indicates a better split for the decision tree.\nYou got it! We’ve covered a lot of ground in this post, from exploring our dataset and identifying key features, to preprocessing our data and training a decision tree model to predict penguin species. And the best part? Our model performed really well. Next we will talk about random forest, until then, keep coding!"
  },
  {
    "objectID": "posts/2023-03-dectree_python/index.html#dataset-description",
    "href": "posts/2023-03-dectree_python/index.html#dataset-description",
    "title": "Model 2 - Decision Trees in Python with Penguin dataset",
    "section": "Dataset Description",
    "text": "Dataset Description\nThe Penguin dataset is a well-known dataset that contains information about the physical characteristics and species of penguins. It consists of 344 samples with 8 input features and 1 target variable (the species of the penguin). In this section, we will describe the structure of the dataset and the importance of data preprocessing in the context of decision trees.\nThe first step in any machine learning project is to load and explore the data. To load the Palmer Penguins dataset, we will use the load_penguins function from the palmerpenguins library. We will also import pandas to create a dataframe to store the data, and seaborn and matplotlib to visualize the data.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.tree import export_graphviz\nimport graphviz\nimport seaborn as sns \nfrom palmerpenguins import load_penguins\nfrom sklearn.preprocessing import StandardScaler\n\n\nimport pandas as pd\nimport seaborn as sns \nfrom palmerpenguins import load_penguins\nsns.set_style('whitegrid')\npenguins = load_penguins()\npenguins.head()\n\n  species     island  bill_length_mm  ...  body_mass_g     sex  year\n0  Adelie  Torgersen            39.1  ...       3750.0    male  2007\n1  Adelie  Torgersen            39.5  ...       3800.0  female  2007\n2  Adelie  Torgersen            40.3  ...       3250.0  female  2007\n3  Adelie  Torgersen             NaN  ...          NaN     NaN  2007\n4  Adelie  Torgersen            36.7  ...       3450.0  female  2007\n\n[5 rows x 8 columns]\n\n\nThe head function is used to display the first few rows of the dataset. This is useful to check that the data has been loaded correctly and to get a quick overview of the data. Now let’s look for missing values.\n\n\n# Check for missing values\nprint(penguins.isnull().sum())\n\nspecies               0\nisland                0\nbill_length_mm        2\nbill_depth_mm         2\nflipper_length_mm     2\nbody_mass_g           2\nsex                  11\nyear                  0\ndtype: int64\n\n\nwe don’t always just drop na, but sckitlearn classifiers aren’t able to handle missing values plus we will lose only a few rows, so let’s take the easy way out here. An alternative could be `filling NA`, but it can be dangerous specially if you rely in simple methods like, just using the average value to fill the gaps. Note that, in the real world, we usually deal with many more missing values and the answer could be trying to enrich the dataset with external information, testing another classification model which can deal with missing values or checking more advanced methods to fill the gaps, how would you like a post about that?\n\n# drop missing values \npenguins = penguins.dropna()\n# Double Check for missing values\nprint(penguins.isnull().sum())\n\nspecies              0\nisland               0\nbill_length_mm       0\nbill_depth_mm        0\nflipper_length_mm    0\nbody_mass_g          0\nsex                  0\nyear                 0\ndtype: int64\n\n\nNow that we have checked for missing values, we can visualize the distribution of the target variable using a histogram. In this case, the target variable is the species of the penguin.\n\n# Plot the distribution of the target variable\nplt.hist(penguins['species'])\n\n(array([146.,   0.,   0.,   0.,   0., 119.,   0.,   0.,   0.,  68.]), array([0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8, 2. ]), <BarContainer object of 10 artists>)\n\nplt.show()\n\n\n\n\n\n\n\nWe can see that the dataset contains three species of penguins: Adelie, Chinstrap, and Gentoo.\nFinally, we can use a scatter matrix plot to visualize the pairwise relationships between the features. This is useful to understand the relationships between the features and to identify any potential correlations, or even if there’s a variable worth removing.\n\n# Plot the pairwise relationships between the features\nsns.pairplot(data=penguins, hue='species')\n\n\n\n\n\n\n\nThis is a very revealing plot, we can see, for example, that some features are very correlated, meaning that we could probably remove flipper_length_mm for example, and leave body_mass, because they are correlated, you can see what I’m talking about if you look to the graph at row 3 column 4. We also can see how the features interact with the species of the penguins, look at row 4 column 4, for example and you will find out that most of Gentoo penguins are heavier than the others, this indicates that body mass may be important predictor for our decision tree model.\nOk, now we’re read to prep the data"
  },
  {
    "objectID": "posts/2023-04-daily_life_ds/index.html",
    "href": "posts/2023-04-daily_life_ds/index.html",
    "title": "Model 3 - Random Forest in R with Breast Cancer Wisconsin (Diagnostic) dataset",
    "section": "",
    "text": "Welcome to the third instalment in our series of posts designed to introduce statistical and machine learning models. Having explored Linear Models (LM) and Decision Trees in our previous posts, we are now moving on to Random Forests.\nIn this tutorial, we will apply a Random Forest model to the Breast Cancer Wisconsin (Diagnostic) dataset. The aim is to predict whether a breast mass is malignant or benign based on measurements from cell nuclei. Let’s get started with a brief description of each attribute:"
  },
  {
    "objectID": "posts/2023-04-daily_life_ds/index.html#conclusion",
    "href": "posts/2023-04-daily_life_ds/index.html#conclusion",
    "title": "Data Detective Chronicles: Solving the Puzzles of Daily Data Science",
    "section": "Conclusion",
    "text": "Conclusion\nJoining datasets without a proper key column can be a challenging task for data scientists. However, by using string similarity measures like the Jaro-Winkler distance, you can tackle this issue and find the best matches between columns with slight discrepancies. Stay tuned for the next instalment of our series, where we will continue to explore the daily challenges faced by data scientists and share\nSee ya!"
  },
  {
    "objectID": "posts/2023-04-daily_life_ds2/index.html",
    "href": "posts/2023-04-daily_life_ds2/index.html",
    "title": "DDC: Tackling Missing or Inconsistent Data",
    "section": "",
    "text": "In the world of data science, dealing with missing or inconsistent data is an everyday challenge. The quality of your insights, predictions, and models heavily depends on the quality of the data you use. In this second post of our series on data science daily life challenges, we’ll explore various strategies for handling missing or inconsistent data using R, and how to make informed decisions about the best approach for your specific situation.\n\nUnderstand the nature of the missing or inconsistent data\n\nBefore diving into any solutions, it’s essential to understand the nature of the missing or inconsistent data you’re dealing with. In R, you can use the summary() function to get an overview of your dataset, including the number of missing values:\n\n# load these packages\n#library(dplyr)\n#library(stringdist)\n#library(tidyverse)\n#library(mice)\n#library(DMwR2)\n\ndataset <- data.frame(col1 = c(1:3, NA),\n                 col2 = c(\"one\", NA,\"cool\", \"text\"), \n                 col3 = c(TRUE, FALSE, TRUE, TRUE), \n                 col4 = c(0.5, 4.7, 3.2, NA),\n                 date_column = c(\"2000/1/1\",\"2000/2/1\" ,\"2000/3/1\" ,\"2023/13/40\"),                 stringsAsFactors = FALSE)\n\nsummary(dataset)\n\n      col1         col2              col3              col4     \n Min.   :1.0   Length:4           Mode :logical   Min.   :0.50  \n 1st Qu.:1.5   Class :character   FALSE:1         1st Qu.:1.85  \n Median :2.0   Mode  :character   TRUE :3         Median :3.20  \n Mean   :2.0                                      Mean   :2.80  \n 3rd Qu.:2.5                                      3rd Qu.:3.95  \n Max.   :3.0                                      Max.   :4.70  \n NA's   :1                                        NA's   :1     \n date_column       \n Length:4          \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n\n\nData Imputation\n\nOne common approach for dealing with missing data is imputation. Imputation involves estimating the missing values based on other available data. Some popular imputation methods in R include:\n\nMean, median, or mode imputation: Replace missing values with the mean, median, or mode of the column.\n\n\ndataset <- dataset %>%\n  mutate(col4 = if_else(is.na(col4), mean(col4, na.rm = TRUE), col4))\n\n\nLinear regression imputation: Use a linear regression model to estimate missing values based on other variables in the dataset.\n\n\nimputed_data <- mice(dataset, method = 'norm.predict', m = 5)\n\n\n iter imp variable\n  1   1  col1\n  1   2  col1\n  1   3  col1\n  1   4  col1\n  1   5  col1\n  2   1  col1\n  2   2  col1\n  2   3  col1\n  2   4  col1\n  2   5  col1\n  3   1  col1\n  3   2  col1\n  3   3  col1\n  3   4  col1\n  3   5  col1\n  4   1  col1\n  4   2  col1\n  4   3  col1\n  4   4  col1\n  4   5  col1\n  5   1  col1\n  5   2  col1\n  5   3  col1\n  5   4  col1\n  5   5  col1\n\ncomplete_data <- complete(imputed_data)\ncomplete_data\n\n      col1 col2  col3 col4 date_column\n1 1.000000  one  TRUE  0.5    2000/1/1\n2 2.000000 <NA> FALSE  4.7    2000/2/1\n3 3.000000 cool  TRUE  3.2    2000/3/1\n4 2.703704 text  TRUE  2.8  2023/13/40\n\n\n\nK-Nearest Neighbours (KNN) imputation: Fill in missing values by averaging the values of the k-nearest neighbours. I will give an example of the code below, but you need a bigger dataset for that approach, that’s why the code is commented.\n\n\n#imputed_data <- knnImputation(dataset, k = 5)\n\nIt’s important to note that imputation can introduce bias or distort the underlying data distribution, so it should be used with caution.\n\nRemoving missing or inconsistent data\n\nIn some cases, it may be appropriate to remove rows or columns containing missing or inconsistent data. This can be done using techniques such as:\n\nListwise deletion: Remove any rows containing missing values.\n\n\ndataset <- na.omit(dataset)\n\n\n\nDropping columns: Remove columns with a high proportion of missing or inconsistent data.\n\ncolumn_with_missing_data <- sapply(dataset,function(x)sum(is.na(x)))\ncolumn_with_missing_data <- column_with_missing_data[column_with_missing_data == 0]\n\ndataset <- dataset %>% select(-column_with_missing_data)\ndataset\n\n  col1 col2 col3 col4 date_column\n1    1  one TRUE  0.5    2000/1/1\n3    3 cool TRUE  3.2    2000/3/1\n\n\nKeep in mind that removing data can lead to loss of information and may introduce bias if the data is not missing at random.\n\nData Standardisation and Transformation\n\nInconsistent data often results from variations in data entry, formats, or units. To address this issue, you can standardise and transform the data using R functions like:\n\nEstablishing consistent formats for dates ( in case it is of type character and there’s inconsistences like “13/40/2023” the return would be NA, so it will help you to recognise inconsistences.\n\n\ndataset$date_column <- as.Date(dataset$date_column, format = \"%Y/%m/%d\")\ndataset\n\n  col1 col2 col3 col4 date_column\n1    1  one TRUE  0.5  2000-01-01\n3    3 cool TRUE  3.2  2000-03-01\n\n\nDealing with missing or inconsistent data is a common challenge for data scientists, but it’s also an opportunity to refine your skills and improve your dataset’s quality. By using R to understand the nature of the missing or inconsistent data and applying appropriate strategies, you can make more informed decisions and produce more accurate and reliable insights. In the next post of our series on data science daily life challenges, we’ll explore the intricacies of handling high-dimensional data and the techniques used to simplify analyses using R. Stay tuned!"
  },
  {
    "objectID": "posts/2023-04-daily_life_ds2/index.html#conclusion",
    "href": "posts/2023-04-daily_life_ds2/index.html#conclusion",
    "title": "Data Detective Chronicles: Solving the Puzzles of Daily Data Science",
    "section": "Conclusion",
    "text": "Conclusion\nJoining datasets without a proper key column can be a challenging task for data scientists. However, by using string similarity measures like the Jaro-Winkler distance, you can tackle this issue and find the best matches between columns with slight discrepancies. Stay tuned for the next instalment of our series, where we will continue to explore the daily challenges faced by data scientists and share\nSee ya!"
  },
  {
    "objectID": "posts/2023-04-daily_life_ds/index.html#building-our-random-forest-model",
    "href": "posts/2023-04-daily_life_ds/index.html#building-our-random-forest-model",
    "title": "Model 3 - Random Forest in R with Breast Cancer Wisconsin (Diagnostic) dataset",
    "section": "Building Our Random Forest Model",
    "text": "Building Our Random Forest Model\nNow that we’ve got a thorough understanding of our data, let’s move forward and build our Random Forest model. In this step, we will use the randomForest package in R.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Split the data into training and test sets\ntraining_rows <- sample(1:nrow(BreastCancer), 0.7*nrow(BreastCancer))\ntraining_set <- BreastCancer[training_rows, -1]\ntest_set <- BreastCancer[-training_rows,-1 ]\n\n# Train the model\n rf_model <- cforest(Class ~ ., data = training_set, controls=cforest_unbiased(ntree=5, mtry=3))\n\n# Print the model\nrf_model\n\n\n     Random Forest using Conditional Inference Trees\n\nNumber of trees:  5 \n\nResponse:  Class \nInputs:  Cl.thickness, Cell.size, Cell.shape, Marg.adhesion, Epith.c.size, Bare.nuclei, Bl.cromatin, Normal.nucleoli, Mitoses \nNumber of observations:  489 \n\n\nThe randomForest() function builds the Random Forest model, with the formula Class ~ . telling it to use all other variables to predict Class. The ntree argument is the number of trees to grow in the forest - in this case, 10. The importance argument, when set to TRUE, computes a measure of variable importance.\nNow, let’s evaluate our model’s performance on the test data.\n\n# Make predictions on the test data\npredictions <- predict(rf_model, newdata = test_set)\n\n# Print confusion matrix\ntable(observed = test_set$Class, predicted = predictions)\n\n           predicted\nobserved    benign malignant\n  benign       143         7\n  malignant      0        60\n\n\nThe confusion matrix gives us a detailed breakdown of the model’s performance. The diagonal elements represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabelled by the classifier.\nIn addition to a confusion matrix, there are several other metrics and methods that can provide a more detailed understanding of a model’s performance. These include:\n\n\nROC Curve and AUC: Receiver Operating Characteristic (ROC) curve is a plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold is varied. Area Under the Curve (AUC) provides an aggregate measure of performance across all possible classification thresholds. An AUC of 1 indicates a perfect classifier.\n\n\npred_prob <- predict(rf_model, newdata = test_set, type = \"prob\")\npred_prob <- sapply(pred_prob, `[[`, 2)\nroc_obj <- roc(test_set$Class, pred_prob)\n\nSetting levels: control = benign, case = malignant\n\n\nSetting direction: controls < cases\n\ncat(\"AUC: \", auc(roc_obj), \"\\n\")\n\nAUC:  0.9866667 \n\n\n\nplot(roc_obj)\n\n\n\n\n\n\n\n\n\nCross-validation: This technique provides a better assessment of the model’s expected performance on unseen data by reducing the variance associated with a single train-test split.\n\n\n# Load necessary library\nlibrary(caret)\n\nCarregando pacotes exigidos: lattice\n\n# Define training control\ntrain_control <- trainControl(method = \"cv\", number = 10)\n\n# Train the model using cross-validation\nrf_model2 <- train(Class ~ ., data = BreastCancer[,-1] %>% na.omit(), trControl = train_control, method = \"rf\")\n\n# Print the model\nprint(rf_model2)\n\nRandom Forest \n\n683 samples\n  9 predictor\n  2 classes: 'benign', 'malignant' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 615, 615, 614, 614, 615, 615, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n   2    0.9647679  0.9232216\n  41    0.9633186  0.9200402\n  80    0.9574576  0.9065592\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.\n\n\nLastly, we can inspect the importance of each feature in making predictions.\n\n# Get variable importance\nvarimp(rf_model)\n\n   Cl.thickness       Cell.size      Cell.shape   Marg.adhesion    Epith.c.size \n    0.000000000     0.054748603     0.005586592     0.000000000     0.011173184 \n    Bare.nuclei     Bl.cromatin Normal.nucleoli         Mitoses \n    0.026815642     0.079329609     0.016759777     0.000000000 \n\n\nEach row of the output corresponds to one of the predictor variables. The higher the value, the more important the predictor variable in classifying the tumour as benign or malignant.\nVariable importance gives us insight into which features are most influential in predicting the target variable in our Random Forest model. It’s an important tool for understanding and interpreting the model. So cell size and shape are the strongest variables for prediction.\nAnd there we have it - a complete Random Forest model in R, built on the Breast Cancer Wisconsin (Diagnostic) dataset. In this tutorial, we’ve covered a lot of ground, from the initial exploratory data analysis through to the creation and evaluation of our model. We hope you’ve found it helpful and enlightening!\nIn our upcoming posts, we’ll dive into more sophisticated machine learning algorithms and their applications. Stay tuned!"
  },
  {
    "objectID": "posts/2023-05-randomforest/index.html",
    "href": "posts/2023-05-randomforest/index.html",
    "title": "DDC: Tackling Missing or Inconsistent Data",
    "section": "",
    "text": "In the world of data science, dealing with missing or inconsistent data is an everyday challenge. The quality of your insights, predictions, and models heavily depends on the quality of the data you use. In this second post of our series on data science daily life challenges, we’ll explore various strategies for handling missing or inconsistent data using R, and how to make informed decisions about the best approach for your specific situation.\n\nUnderstand the nature of the missing or inconsistent data\n\nBefore diving into any solutions, it’s essential to understand the nature of the missing or inconsistent data you’re dealing with. In R, you can use the summary() function to get an overview of your dataset, including the number of missing values:\n\n# load these packages\n#library(dplyr)\n#library(stringdist)\n#library(tidyverse)\n#library(mice)\n#library(DMwR2)\n\ndataset <- data.frame(col1 = c(1:3, NA),\n                 col2 = c(\"one\", NA,\"cool\", \"text\"), \n                 col3 = c(TRUE, FALSE, TRUE, TRUE), \n                 col4 = c(0.5, 4.7, 3.2, NA),\n                 date_column = c(\"2000/1/1\",\"2000/2/1\" ,\"2000/3/1\" ,\"2023/13/40\"),                 stringsAsFactors = FALSE)\n\nsummary(dataset)\n\n      col1         col2              col3              col4     \n Min.   :1.0   Length:4           Mode :logical   Min.   :0.50  \n 1st Qu.:1.5   Class :character   FALSE:1         1st Qu.:1.85  \n Median :2.0   Mode  :character   TRUE :3         Median :3.20  \n Mean   :2.0                                      Mean   :2.80  \n 3rd Qu.:2.5                                      3rd Qu.:3.95  \n Max.   :3.0                                      Max.   :4.70  \n NA's   :1                                        NA's   :1     \n date_column       \n Length:4          \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n\n\nData Imputation\n\nOne common approach for dealing with missing data is imputation. Imputation involves estimating the missing values based on other available data. Some popular imputation methods in R include:\n\nMean, median, or mode imputation: Replace missing values with the mean, median, or mode of the column.\n\n\ndataset <- dataset %>%\n  mutate(col4 = if_else(is.na(col4), mean(col4, na.rm = TRUE), col4))\n\n\nLinear regression imputation: Use a linear regression model to estimate missing values based on other variables in the dataset.\n\n\nimputed_data <- mice(dataset, method = 'norm.predict', m = 5)\n\n\n iter imp variable\n  1   1  col1\n  1   2  col1\n  1   3  col1\n  1   4  col1\n  1   5  col1\n  2   1  col1\n  2   2  col1\n  2   3  col1\n  2   4  col1\n  2   5  col1\n  3   1  col1\n  3   2  col1\n  3   3  col1\n  3   4  col1\n  3   5  col1\n  4   1  col1\n  4   2  col1\n  4   3  col1\n  4   4  col1\n  4   5  col1\n  5   1  col1\n  5   2  col1\n  5   3  col1\n  5   4  col1\n  5   5  col1\n\ncomplete_data <- complete(imputed_data)\ncomplete_data\n\n      col1 col2  col3 col4 date_column\n1 1.000000  one  TRUE  0.5    2000/1/1\n2 2.000000 <NA> FALSE  4.7    2000/2/1\n3 3.000000 cool  TRUE  3.2    2000/3/1\n4 2.703704 text  TRUE  2.8  2023/13/40\n\n\n\nK-Nearest Neighbours (KNN) imputation: Fill in missing values by averaging the values of the k-nearest neighbours. I will give an example of the code below, but you need a bigger dataset for that approach, that’s why the code is commented.\n\n\n#imputed_data <- knnImputation(dataset, k = 5)\n\nIt’s important to note that imputation can introduce bias or distort the underlying data distribution, so it should be used with caution.\n\nRemoving missing or inconsistent data\n\nIn some cases, it may be appropriate to remove rows or columns containing missing or inconsistent data. This can be done using techniques such as:\n\nListwise deletion: Remove any rows containing missing values.\n\n\ndataset <- na.omit(dataset)\n\n\n\nDropping columns: Remove columns with a high proportion of missing or inconsistent data.\n\ncolumn_with_missing_data <- sapply(dataset,function(x)sum(is.na(x)))\ncolumn_with_missing_data <- column_with_missing_data[column_with_missing_data == 0]\n\ndataset <- dataset %>% select(-column_with_missing_data)\ndataset\n\n  col1 col2 col3 col4 date_column\n1    1  one TRUE  0.5    2000/1/1\n3    3 cool TRUE  3.2    2000/3/1\n\n\nKeep in mind that removing data can lead to loss of information and may introduce bias if the data is not missing at random.\n\nData Standardisation and Transformation\n\nInconsistent data often results from variations in data entry, formats, or units. To address this issue, you can standardise and transform the data using R functions like:\n\nEstablishing consistent formats for dates ( in case it is of type character and there’s inconsistences like “13/40/2023” the return would be NA, so it will help you to recognise inconsistences.\n\n\ndataset$date_column <- as.Date(dataset$date_column, format = \"%Y/%m/%d\")\ndataset\n\n  col1 col2 col3 col4 date_column\n1    1  one TRUE  0.5  2000-01-01\n3    3 cool TRUE  3.2  2000-03-01\n\n\nDealing with missing or inconsistent data is a common challenge for data scientists, but it’s also an opportunity to refine your skills and improve your dataset’s quality. By using R to understand the nature of the missing or inconsistent data and applying appropriate strategies, you can make more informed decisions and produce more accurate and reliable insights. In the next post of our series on data science daily life challenges, we’ll explore the intricacies of handling high-dimensional data and the techniques used to simplify analyses using R. Stay tuned!"
  },
  {
    "objectID": "posts/2023-11-textmining/index.html",
    "href": "posts/2023-11-textmining/index.html",
    "title": "Text mining on Harry Potter dialogues",
    "section": "",
    "text": "Welcome to our latest exploration in the world of data science. Following our previous discussions on machine learning models, we now delve into the fascinating realm of text analysis using R. In this post, we’ll apply various R packages to analyze character dialogues from a fictional dataset. Our goal is to uncover insights such as the most mentioned characters, frequent greetings, and the characteristics of dialogues. Let’s dive into the script and interpret our findings.\nUnderstanding the Dataset\nWe have a JSON file containing dialogues from different sessions, aquired here . To extract meaningful insights, we first load and process the data using tidyverse and tidytext packages in R. The dataset is a treasure trove of dialogues, each offering a unique glimpse into the interactions between characters.\nData Preparation and Exploration With our data loaded, we proceed to clean and structure it for analysis. We utilize functions like str_split_fixed to separate the dialogues into character names and their corresponding lines. This meticulous process sets the stage for deeper analysis, allowing us to delve into the intricacies of each dialogue.\n\nhpd <- fromJSON(file=\"en_train_set.json\")\n\nextracted_dialogue <- map(hpd, pluck, \"dialogue\")\nsession_names <- rep(names(extracted_dialogue), \n                     times = sapply(extracted_dialogue, length))\n\n dialog_tb <- str_split_fixed(string = tibble(\n   dialogue = unlist(extracted_dialogue)\n )$dialogue,\n pattern = ':',n=2) |> \n   as_tibble() |> \n   mutate(session = session_names,\n          V1 = str_trim(V1)) |> \n   select(session, charac = V1, dialog = V2)\n\nInsights from the Data\n\n\nMost Mentioned Characters: We quantify the presence of each character in the dialogues. By counting mentions, we identify the characters that dominate the conversations, offering insights into their importance or prominence in the narrative.\n\n character_mentions <- sapply(unique(dialog_tb$charac), function(char) {\n   sum(str_detect(dialog_tb$dialog, fixed(char)))\n })\n\n # Creating a data frame for the results\n mentions_df <- data.frame(charac = unique(dialog_tb$charac),\n                           mentions = character_mentions) |> \n   filter(charac != \"hat\")\n\n # Displaying the results\n mentions_df |> \n   arrange(desc(mentions)) |> \n   slice(1:20) |> \n   ggplot(aes(y =  reorder(charac, -mentions), x = mentions)) +\n   geom_bar(stat = 'identity') +\n   ggtitle('Character mentions')\n\n\n\n\n\n\n\n\n\nFrequent Greetings: The essence of initial interactions is captured by analyzing common greetings like “Hello”, “Hi”, and others. This reveals the nature of interactions and the formality or informality within the dialogues.\n\ngreetings <- c(\"Hello\", \"Hi\", \"Greetings\", \"Hey\")\n\n# Extracting greetings from dialogues\ngreetings_found <- sapply(greetings, function(greet) {\n    unlist(str_extract_all(dialog_tb$dialog, fixed(greet)))\n})\n\n# Displaying the results\nlapply(greetings_found,length)\n\n$Hello\n[1] 41\n\n$Hi\n[1] 147\n\n$Greetings\n[1] 0\n\n$Hey\n[1] 41\n\n\n\n\nWhat’s the longest dialogue?\n\n # Calculating the length of each dialogue\n dialog_tb$length <- str_length(dialog_tb$dialog)\n\n # Identifying the longest dialogue\n longest_dialogue <- dialog_tb %>% \n   arrange(desc(length)) %>%\n   slice(1)\n\n # Displaying the result\n longest_dialogue$dialog\n\n[1] \" Then you should, You know the secret of my sister’s ill health, what those Muggles did, what she became. You know how my poor father sought revenge, and paid the price, died in Azkaban. You know how my mother gave up her own life to care for Ariana. Harry. I was gifted, I was brilliant. I wanted to escape. I wanted to shine. I wanted glory. Do not misunderstand me, I loved them. I loved my parents, I loved my brother and my sister, but I was selfish, Harry, more selfish than you, who are a remarkably selfless person, could possibly imagine. So that, when my mother died, and I was left the responsibility of a damaged sister and a wayward brother, I returned to my village in anger and bitterness. Trapped and wasted, I thought! And then, of course, he came. . . . Grindelwald. You cannot imagine how his ideas caught me, Harry, inflamed me. Muggles forced into subservience. We wizards triumphant. Grindelwald and I, the glorious young leaders of the revolution. Oh, I had a few scruples. I assuaged my conscience with empty words. It would all be for the greater good, and any harm done would be repaid a hundredfold in benefits for wizards. Did I know, in my heart of hearts, what Gellert Grindelwald was? I think I did, but I closed my eyes. If the plans we were making came to fruition, all my dreams would come true. And at the heart of our schemes, the Deathly Hallows! How they fascinated him, how they fascinated both of us! The unbeatable wand, the weapon that would lead us to power! The Resurrection Stone — to him, though I pretended not to know it, it meant an army of Inferi! To me, I confess, it meant the return of my parents, and the lifting of all responsibility from my shoulders. Harry. I thought that, if we ever found it, it might be useful in hiding Ariana, but our interest in the Cloak was mainly that it completed the trio, for the legend said that the man who united all three objects would then be truly master of death, which we took to mean ‘invincible. ’ Invincible masters of death, Grindelwald and Dumbledore! Two months of insanity, of cruel dreams, and neglect of the only two members of my family left to me. You know what happened. Reality returned in the form of my rough, unlettered, and infinitely more admirable brother. I did not want to hear the truths he shouted at me. I did not want to hear that I could not set forth to seek Hallows with a fragile and unstable sister in tow. The argument became a fight. Grindelwald lost control. That which I had always sensed in him, though I pretended not to, now sprang into terrible being.  And Ariana .. . after all my mother’s care and caution . . . lay dead upon the floor. Well, Grindelwald fled, as anyone but I could have predicted. He vanished, with his plans for seizing power, and his schemes for Muggle torture, and his dreams of the Deathly Hallows, dreams in which I had encouraged him and helped him. He ran, while I was left to bury my sister, and learn to live with my guilt and my terrible grief, the price of my shame. Years passed. There were rumors about him. They said he had procured a wand of immense power. I, meanwhile, was offered the post of Minister of Magic, not once, but several times. Naturally, I refused. I had learned that I was not to be trusted with power.\"\n\n\n\n\nDialogue Dynamics: We explore the dialogues’ nature by identifying the frequency of questions and the length of dialogues. It tells us about the conversational style and the depth of discussions among characters.\n\n# Counting dialogues that contain questions\n question_dialogues_count <- sum(str_detect(dialog_tb$dialog, fixed(\"?\")))\n\n # Displaying the count\n  cat( (question_dialogues_count/nrow(dialog_tb))*100, \"% of all dialogues are questions\")\n\n35.38782 % of all dialogues are questions\n\n\n\n\nComparative Analysis: A fun aspect is comparing how often different characters mention specific terms, like how often Ron mentions “Harry” compared to Hermione. It adds a layer of relational dynamics to our analysis.\n\n # Count ron's mentions of \"harry\"\n ron_mentions_potter <- sum(str_detect(\n   dialog_tb$dialog[dialog_tb$charac == \"Ron\"], \"Harry\"))\n\n # Count Hermione's mentions of \"Harry\"\n hermione_mentions_harry <- sum(str_detect(\n   dialog_tb$dialog[dialog_tb$charac == \"Hermione\"], \"Harry\"))\n\n # Displaying the results\n cat(\"Ron mentions 'Harry':\", ron_mentions_potter, \"times\\n\")\n\nRon mentions 'Harry': 149 times\n\n cat(\"Hermione mentions 'Harry':\", hermione_mentions_harry, \"times\")\n\nHermione mentions 'Harry': 390 times\n\n\n\n\nVisual Representation and Conclusion Our findings are not just about numbers and text; we bring them to life through visualizations like bar graphs and tables. These visual aids help us to quickly grasp the essence of our analysis, making the data more accessible and comprehensible.\nStay tuned"
  },
  {
    "objectID": "posts/2024-01-Horse_Race_and_DataScience/index.html",
    "href": "posts/2024-01-Horse_Race_and_DataScience/index.html",
    "title": "Horse Race and Data Science",
    "section": "",
    "text": "Hello again my friends, I got into a very interesting project while learning how to better work with BeautifulSoup, and I felt like sharing with you. I saw a tutorial and it looked so easy that I decided to find a different table to extract so I could have a better challenge and ended up here. After successfully extracting the jockeys stats table, I wondered “How much people already played with horse racing data?”\nEasy answer: not much, according to wikipedia, only 10 countries host horse racing, and during my research for data, I found info mostly about Australia, New Zealand, Hong Kong and Singapure races.\nWell that means more BeautifulSoup practice for me, right?!\nBut before that, I need to understand the horse racing world, I’ve read so many expressions that are completely new to me. Because of that, todays post is aiming to show:"
  },
  {
    "objectID": "posts/2024-01-Horse_Race_and_DataScience/index.html#what-you-need-to-know-before-going-into-a-horse-race-dataset",
    "href": "posts/2024-01-Horse_Race_and_DataScience/index.html#what-you-need-to-know-before-going-into-a-horse-race-dataset",
    "title": "Horse Race and Data Science",
    "section": "What you need to know before going into a horse race dataset",
    "text": "What you need to know before going into a horse race dataset\nTo effectively analyze horse race data, there are several key areas one must understand:\n1. The Jockeys\nThe jockey is much more than just a rider. Their skill, experience, and even weight play a significant role in the performance of the horse. Understanding a jockey’s track record, including wins, losses, and strategies, can provide valuable insights into future races, and at raceing I found:\n\n2. The Trainers\nBehind every successful horse and jockey is a trainer. Trainers are responsible for preparing the horse for the race, which includes physical training, nutrition, and strategy. Analyzing a trainer’s history, methods, and success rate is crucial for predicting how well a horse might perform.\n\n3. The Horses\nEach horse has its unique traits - age, breed, lineage, past performance, and more. Some breeds are known for their speed, others for endurance. The lineage can also give clues about a horse’s potential. Understanding these aspects can significantly enhance the analysis of race outcomes. Also, you will see different info from the two above, that’s because each one has a stats and form tab, what you saw above was the stats tab, now lets see the other one.\n\n4. The Tracks\nRacetracks vary widely - in terms of surface (dirt, turf, synthetic), length, and weather conditions. Some horses perform better on certain surfaces or track lengths. The condition of the track on race day can also impact the results. It’s important to include these variables in any comprehensive analysis.\n5. Race Types and Classifications\nHorse races come in different forms: flat racing, steeplechase, harness racing, etc. Within these, there are classifications based on the age and ability of the horses. Understanding the type and classification of the race is vital for contextual analysis.\n6. Betting Odds\nWhile not directly related to the physical aspects of the race, betting odds are a reflection of public perception and can indicate the favorites and underdogs. These odds can sometimes offer insights into factors that may not be immediately apparent in the raw data.\n7. External Factors\nFactors such as weather conditions on the day of the race, the horse’s mood and health, and even the jockey’s physical condition can influence the race outcome. While some of these are difficult to quantify, they are important to acknowledge.\nNow that we identified the variables that can impact race results, lets get a little more specific. Because when I looked at the data I asked myself “what the heck means P.O.T? What does the rating mean in the Form tab?”\nWell, let’s answer that, it might get a little boring of a list, but hang on, we’re almost finishing.\n\nCAREER: This shows the horse’s total career statistics, often in the format of starts-wins-seconds-thirds. For example, “27-10-4-4” would mean the horse has started 27 races, winning 10, finishing second 4 times, and third 4 times. The same idea goes for Season\nGRP & LISTED: This column shows the horse’s record in Group and Listed races, which are higher-class races. “7-2-1-0” means the horse has entered 7 such races, won 2, came second once, and has not placed third.\n1ST UP: The horse’s performance when racing for the first time in a new racing season or after a break. “6-1-1-3” indicates 6 first-up starts, with 1 win, 1 second, and 3 third-place finishes. Same idea goes for 2ND UP and 3RD UP.\nFIRM/GOOD/SOFT/HEAVY: These columns indicate the horse’s performance on different track conditions. For example, “17-7-1-4” under GOOD suggests the horse has raced 17 times on good rated tracks, with 7 wins, 1 second, and 4 thirds.\nJUMPS: Shows the horse’s performance in jump races (if applicable), which are races involving hurdles or fences.\nSYNTH: The horse’s record on synthetic tracks, which are artificial surfaces.\nPOS: The position the horse finished in the race.\nHORSE/TRAINER/JOCKEY: Names of the horse, the trainer(s), and the jockey.\nPRIZE/CLASS: The total prize money and the class of the race (e.g., G1 for Group 1, which is the highest level of race).\nWGT: The weight carried by the horse, including the jockey and gear.\n800M/400M: The horse’s position at 800 meters and 400 meters from the finish line.\nMARGIN: How far the horse was from the winner, in lengths.\nRATING: A handicap rating assigned to a horse based on its performance. It’s a number that allows horses of different abilities to compete on equal terms, with higher-rated horses carrying more weight. The exact method of calculation can vary by racing jurisdiction but generally involves the horse’s past performance, the quality of the races it has run in, and the margins of victory or defeat.\nODDS: The betting odds for the horse at the start of the race.\nCOM: Commentary or notes about the horse’s performance in the race.\n\nThere three more that I found interesting:\n\nP.O.T: This stands for “Profit on Turnover.” It is a measure used by bettors to calculate the profitability of their bets. For a jockey, it would typically refer to the profitability of betting on all the races that the jockey has ridden. It is calculated by taking the total returns from bets on a jockey’s rides, subtracting the total amount bet, and then dividing by the total amount bet. This is often expressed as a percentage. A positive percentage means a profit, while a negative percentage indicates a loss.\nS Rate: This is usually shorthand for “Strike Rate.” In horse racing, a jockey’s strike rate is the percentage of races won out of the total number of races ridden. A higher strike rate indicates a jockey who wins a larger proportion of their races. It’s calculated as (Number of Wins / Total Rides) x 100.\nU.D.R: This typically stands for “Universal Driver Rating” or sometimes “Universal Jockey Rating,” depending on the context. It’s a handicapping tool used primarily in harness racing but can be applied in thoroughbred racing to assess a driver’s or jockey’s performance. The U.D.R. is calculated by a formula that takes into account wins, seconds, and thirds, giving more weight to wins. The formula is ([Wins x 9] + [Places x 5] + [Shows x 3]) / (Total Drives x 9). This gives a figure between 0 and 1, which can be interpreted as a batting average. The closer to 1, the better the performance.\n\nI’ll explore the main areas Jockey, Trainer, Horse and Track, sharing my findings and the challenges I encounter at my github (remember I left the link at the start). This exploration is not just about understanding data; it’s about understanding the sport of horse racing itself. So, stay tuned as we dive deeper into this fascinating world!"
  }
]