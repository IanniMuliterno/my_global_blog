[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IMuliterno",
    "section": "",
    "text": "Data Science\n\n\nR\n\n\nrandom forest\n\n\nmachine learning\n\n\n \n\n\n\n\nMay 28, 2023\n\n\nI. Muliterno\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nData Science\n\n\nR\n\n\nDDC\n\n\nmissing\n\n\ninconsistent\n\n\n \n\n\n\n\nApr 24, 2023\n\n\nI. Muliterno\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nData Science\n\n\nR\n\n\nDDC\n\n\nmissing\n\n\ninconsistent\n\n\n \n\n\n\n\nApr 24, 2023\n\n\nI. Muliterno\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMachine learning\n\n\nStatistics\n\n\nData Science\n\n\nPython\n\n\nClassification\n\n\n \n\n\n\n\nMar 7, 2023\n\n\nI. Muliterno\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMachine learning\n\n\nStatistics\n\n\nData Science\n\n\nLinear Regression\n\n\nPython\n\n\n \n\n\n\n\nMar 6, 2023\n\n\nI. Muliterno\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine learning\n\n\nStatistics\n\n\nData Science\n\n\n \n\n\n\n\nMar 1, 2023\n\n\nIanní Muliterno\n\n\n4 min\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n See all"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a data scientist and here I practice.\nMy expertise lies in developing and implementing data-driven solutions to drive business growth and enhance performance. My main programming are R, SQL and Python."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nModel 3 - Random Forest in R with Breast Cancer Wisconsin (Diagnostic) dataset\n\n\n\nMay 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDDC: Tackling Missing or Inconsistent Data\n\n\n\nApr 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDDC: Tackling Missing or Inconsistent Data\n\n\n\nApr 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 2 - Decision Trees in Python with Penguin dataset\n\n\n\nMar 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 1 - Linear Regression in Python with Kaggle Data\n\n\n\nMar 6, 2023\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nModeling Approaches\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-03-models_and_applications/index.html",
    "href": "posts/2023-03-models_and_applications/index.html",
    "title": "Modeling Approaches",
    "section": "",
    "text": "As a senior data scientist, one of the key responsibilities is to identify the right modeling approach for a given problem. Different modeling approaches have different strengths and weaknesses, and choosing the right approach is crucial to building an effective and accurate model. This post is the first in a series of posts on modeling approaches, I will discuss the strengths and weaknesses of some popular modeling approaches and when to apply different combinations.\nThis post is the first in a series of posts on modeling approaches"
  },
  {
    "objectID": "posts/2023-03-models_and_applications/index.html#linear-regression",
    "href": "posts/2023-03-models_and_applications/index.html#linear-regression",
    "title": "Modeling Approaches",
    "section": "1. Linear Regression",
    "text": "1. Linear Regression\nLinear regression is a popular modeling approach used for predicting continuous values. It is a simple yet powerful modeling approach that works well when the relationship between the predictor and response variable is linear. Linear regression models are easy to interpret and can be used for both simple and complex problems.\n\nStrengths:\n\nEasy to interpret and explain.\nWorks well when the relationship between predictor and response variable is linear.\nCan handle both simple and complex problems.\n\n\n\nWeaknesses:\n\nAssumes a linear relationship between predictor and response variable.\nSensitive to outliers and multicollinearity.\n\n\n\nWhen to use:\n\nWhen the relationship between predictor and response variable is linear.\nWhen the data has few predictors.\nWhen the data has no multicollinearity or outliers."
  },
  {
    "objectID": "posts/2023-03-models_and_applications/index.html#decision-trees",
    "href": "posts/2023-03-models_and_applications/index.html#decision-trees",
    "title": "Modeling Approaches",
    "section": "2. Decision Trees",
    "text": "2. Decision Trees\nDecision trees are a popular modeling approach used for classification and regression. They work by partitioning the data into smaller subsets based on the values of the predictors. Decision trees are easy to interpret and can handle both categorical and continuous predictors. Personally, I prefer to apply only for categorical predictors.\n\nStrengths:\n\nEasy to interpret and explain.\nCan handle both categorical and continuous predictors.\nCan handle interactions between predictors.\n\n\n\nWeaknesses:\n\nCan overfit the data.\nSensitive to small changes in the data.\n\n\n\nWhen to use:\n\nWhen the data has many predictors.\nWhen the data has interactions between predictors.\nWhen the data has both categorical and continuous predictors."
  },
  {
    "objectID": "posts/2023-03-models_and_applications/index.html#random-forests",
    "href": "posts/2023-03-models_and_applications/index.html#random-forests",
    "title": "Modeling Approaches",
    "section": "3. Random Forests",
    "text": "3. Random Forests\nRandom forests are an extension of decision trees that work by combining multiple decision trees to make a final prediction. They are a popular modeling approach used for classification and regression. Random forests are robust to overfitting and can handle both categorical and continuous predictors.\n\nStrengths:\n\nRobust to overfitting.\nCan handle both categorical and continuous predictors.\nCan handle interactions between predictors.\n\n\n\nWeaknesses:\n\nCan be slow to train and predict.\nCan be difficult to interpret.\n\n\n\nWhen to use:\n\nWhen the data has many predictors.\nWhen the data has interactions between predictors.\nWhen the data has both categorical and continuous predictors.\nWhen the data has outliers or missing values."
  },
  {
    "objectID": "posts/2023-03-models_and_applications/index.html#neural-networks",
    "href": "posts/2023-03-models_and_applications/index.html#neural-networks",
    "title": "Modeling Approaches",
    "section": "4. Neural Networks",
    "text": "4. Neural Networks\nNeural networks are a popular modeling approach used for classification and regression. They work by mimicking the structure of the human brain to identify complex patterns in the data. Neural networks are powerful and can handle both linear and nonlinear relationships between predictors and response variables.\n\nStrengths:\n\nCan handle both linear and nonlinear relationships between predictors and response variables.\nCan identify complex patterns in the data.\nCan handle large and complex datasets.\n\n\n\nWeaknesses:\n\nCan be slow to train and predict.\nCan overfit the data.\n\n\n\nWhen to use:\n\nWhen the data has many predictors.\nWhen the data has complex relationships between predictors and response variables.\nWhen the data has large and complex datasets."
  },
  {
    "objectID": "posts/2023-03-models_and_applications/index.html#support-vector-machines",
    "href": "posts/2023-03-models_and_applications/index.html#support-vector-machines",
    "title": "Modeling Approaches",
    "section": "5. Support Vector Machines",
    "text": "5. Support Vector Machines\nSupport Vector Machines (SVMs) are a popular modeling approach used for classification and regression. They work by finding the hyperplane that best separates the data into different classes. SVMs are powerful and can handle both linear and nonlinear relationships between predictors and response variables.\n\nStrengths:\n\nCan handle both linear and nonlinear relationships between predictors and response variables.\nCan handle high-dimensional datasets.\nCan handle outliers.\n\n\n\nWeaknesses:\n\nCan be sensitive to the choice of kernel function.\nCan be slow to train and predict.\n\n\n\nWhen to use:\n\nWhen the data has many predictors.\n\n\n\nWhen the data has both linear and nonlinear relationships between predictors and response variables.\nWhen the data has outliers.\n\nIn practice, different modeling approaches may be combined to build more accurate and robust models. For example, a decision tree model may be combined with a random forest model to improve accuracy and reduce overfitting. Or, a linear regression model may be combined with a support vector machine model to handle both linear and nonlinear relationships between predictors and response variables.\nWhen choosing a modeling approach or combination of approaches, it is important to consider the strengths and weaknesses of each approach in relation to the specific problem at hand. It is also important to consider factors such as the size and complexity of the data, the level of interpretability required, and the computational resources available.\nIn summary, there are a variety of modeling approaches that can be used in data science, each with its own strengths and weaknesses. By carefully considering the specific problem at hand and choosing the right combination of modeling approaches, data scientists can build accurate and robust models that provide valuable insights and predictions."
  },
  {
    "objectID": "posts/2023-03-lm_python/index.html",
    "href": "posts/2023-03-lm_python/index.html",
    "title": "Model 1 - Linear Regression in Python with Kaggle Data",
    "section": "",
    "text": "In our previous post, we discussed different modeling approaches and their applications. Today, we will delve deeper into linear regression, one of the most commonly used modeling techniques in data science. By the end, you will learn when to use linear regression and how to code it from start to finish using a dataset from Kaggle."
  },
  {
    "objectID": "posts/2023-03-lm_python/index.html#getting-to-know-linear-regression",
    "href": "posts/2023-03-lm_python/index.html#getting-to-know-linear-regression",
    "title": "Model 1 - Linear Regression in Python with Kaggle Data",
    "section": "1. Getting to know Linear Regression",
    "text": "1. Getting to know Linear Regression\nLinear regression is a popular modeling approach used for predicting continuous values. It is a simple yet powerful modeling approach that works well when the relationship between the predictor and response variable is linear. It is important to ensure that the assumptions of linear regression are met, including linearity, independence, normality, and equal variance. These assumptions can be tested using various techniques, such as residual plots and statistical tests. Linear regression models are easy to interpret and can be used for both simple and complex problems.\nSimple linear regression involves a single independent variable, while multiple linear regression involves two or more independent variables. Examples of where linear regression is commonly used include predicting housing prices, analyzing stock prices, and estimating crop yields."
  },
  {
    "objectID": "posts/2023-03-lm_python/index.html#data-cleaning-and-visualization",
    "href": "posts/2023-03-lm_python/index.html#data-cleaning-and-visualization",
    "title": "Model 1 - Linear Regression in Python with Kaggle Data",
    "section": "2. Data Cleaning and Visualization",
    "text": "2. Data Cleaning and Visualization\nWhen working with real-world data, it is common to encounter missing or erroneous values, inconsistent formatting, and other issues. Data cleaning is the process of detecting and correcting these problems in the data to ensure that it is accurate and reliable for analysis. Visualization, on the other hand, is the process of representing data in a visual format such as graphs, charts, or maps, to help analysts identify patterns and trends.\nFor this blog post, we will be using a data set from Kaggle’s House Prices: Advanced Regression Techniques competition. This data set contains information on various attributes of residential homes in Ames, Iowa, including their sale prices. The goal of the competition is to build a model that can accurately predict the sale prices of homes based on these attributes.\nTo start, we will import the necessary libraries in Python, including Pandas for data manipulation and Matplotlib for visualization. We will then load the data set using the read_csv() function from Pandas.\n\n\n\nNow that we have loaded the data, we can begin the data cleaning process. The first step is to check for missing values in the data. We can use the isnull() function from Pandas to check for missing values and the sum() function to count the number of missing values in each column.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# remove the comment to load the dataset\n#df = pd.read_csv(\"train.csv\")\n\n# Check for missing values\nmissing_values = df.isnull().sum()\nprint(missing_values)\n\nId                 0\nMSSubClass         0\nMSZoning           0\nLotFrontage      259\nLotArea            0\n                ... \nMoSold             0\nYrSold             0\nSaleType           0\nSaleCondition      0\nSalePrice          0\nLength: 81, dtype: int64\n\n\nThis will give us a count of missing values in each column of the data set. We can then decide how to handle these missing values, depending on the amount of missing data and the nature of the problem. In this case, we will simply drop the columns with more than 50% missing values.\nNext, we can check for any duplicate rows in the data set using the duplicated() function from Pandas. If there are any duplicate rows, we can drop them using the drop_duplicates() function.\n\n\n\n# Drop columns with more than 50% missing values\ndf = df.dropna(thresh=len(df)*0.5, axis=1)\n\n# Check for duplicates\nduplicates = df.duplicated()\nprint(duplicates.sum())\n\n# Drop duplicates\n\n0\n\ndf = df.drop_duplicates()\n\nNow that we have cleaned the data, we can move on to visualization. One common visualization for exploring the relationship between two variables is a scatter plot. We can create a scatter plot of the sale prices and the living area of the homes using Matplotlib.\n\n# Create a histogram of sale prices\nplt.hist(df[\"SalePrice\"], bins=20)\n\n(array([ 22., 126., 380., 343., 229., 144.,  86.,  49.,  28.,  23.,  12.,\n         7.,   3.,   1.,   2.,   1.,   2.,   0.,   0.,   2.]), array([ 34900.,  70905., 106910., 142915., 178920., 214925., 250930.,\n       286935., 322940., 358945., 394950., 430955., 466960., 502965.,\n       538970., 574975., 610980., 646985., 682990., 718995., 755000.]), <BarContainer object of 20 artists>)\n\nplt.xlabel(\"Sale Price ($)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\n\n\n\n\nThis will give us a visual representation of the distribution of sale prices in the data set. We can see that the distribution is skewed to the right, with a few homes having very high sale prices.\nBy cleaning and visualizing the data, we can gain a better understanding of its properties and identify any potential issues that may need to be addressed before building a linear regression model."
  },
  {
    "objectID": "posts/2023-03-lm_python/index.html#summary",
    "href": "posts/2023-03-lm_python/index.html#summary",
    "title": "Model 1 - Linear Regression in Python with Kaggle Data",
    "section": "Summary",
    "text": "Summary\nIn this post, we have explored the process of using linear regression to predict the sale prices of homes based on their attributes. We started by cleaning and visualizing the data to gain insights into the relationships between the input variables and the sale prices. We then built a linear regression model using the Scikit-learn library and evaluated its performance using the mean squared error and coefficient of determination.\nBy following this process, we can make accurate predictions about the sale prices of homes based on their attributes. This information can be valuable for a variety of applications, including real estate valuation, mortgage underwriting, and investment analysis.\nAs with any predictive model, it is important to continually evaluate and refine the model over time to ensure that it is accurately predicting the outcome of interest. With continued effort, we can refine our understanding of the relationship between the input variables and the sale prices of homes, and improve the accuracy of our predictions."
  },
  {
    "objectID": "posts/2023-03-lm_python/index.html#building-a-linear-regression-model",
    "href": "posts/2023-03-lm_python/index.html#building-a-linear-regression-model",
    "title": "Model 1 - Linear Regression in Python with Kaggle Data",
    "section": "3. Building a Linear Regression Model",
    "text": "3. Building a Linear Regression Model\nNow that we have cleaned and visualized the data, we can start building a linear regression model to predict the sale prices of homes based on their attributes. Linear regression is a statistical technique that is commonly used for predicting a numeric value based on one or more input variables. In this case, we will use the input variables in the data set to predict the sale price of each home.\nTo start, we will split the data set into a training set and a validation set using the train_test_split() function from Scikit-learn. The training set will be used to train the model, while the validation set will be used to evaluate the model’s performance.\n\n\nfrom sklearn.model_selection import train_test_split\n\n# Split the data set into training and validation sets\ntrain, val = train_test_split(df, test_size=0.2, random_state=42)\n\nNext, we will select the input variables that we want to use in the model. In this case, we will use the living area, number of bedrooms, and number of bathrooms as input variables.\n\n# Select the input variables\nX_train = train[[\"GrLivArea\", \"BedroomAbvGr\", \"FullBath\"]]\ny_train = train[\"SalePrice\"]\n\nX_val = val[[\"GrLivArea\", \"BedroomAbvGr\", \"FullBath\"]]\ny_val = val[\"SalePrice\"]\n\nWe can then build a linear regression model using the LinearRegression() function from Scikit-learn. We can use it to predict the sale prices of homes in the validation set using the predict() function.\n\n\nfrom sklearn.linear_model import LinearRegression\n\n# Build the linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predict the sale prices of homes in the validation set\n\nLinearRegression()\n\ny_pred = model.predict(X_val)\n\nTo evaluate the performance of the model, we can calculate the mean squared error (MSE) and the coefficient of determination (R-squared) between the predicted sale prices and the actual sale prices in the validation set using the mean_squared_error() and r2_score() functions from Scikit-learn.\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Calculate the mean squared error and R-squared\nmse = mean_squared_error(y_val, y_pred)\nr2 = r2_score(y_val, y_pred)\nprint(\"MSE:\", mse)\n\nMSE: 2806426667.247853\n\nprint(\"R-squared:\", r2)\n\nR-squared: 0.6341189942328371\n\n\nA low mean squared error and a high coefficient of determination indicate that the model is accurately predicting the sale prices of homes based on their attributes.\nBy building a linear regression model, we can use the input variables in the data set to predict the sale prices of homes with a high degree of accuracy. This information can be useful for real estate professionals, home buyers, and sellers looking to estimate the value of a residential property."
  },
  {
    "objectID": "posts/2023-03-dectree_python/index.html",
    "href": "posts/2023-03-dectree_python/index.html",
    "title": "Model 2 - Decision Trees in Python with Penguin dataset",
    "section": "",
    "text": "In our previous post, we discussed linear regression. Today, we will get familiar with decisions trees, but before that, you gotta understand what’s a classification problem.\nClassification is an important task in machine learning, with numerous applications in fields such as healthcare, finance, and marketing. One popular classification algorithm is decision trees, which use a tree-like model of decisions and their possible consequences to predict the target variable. In this post, we will provide a step-by-step guide to implementing decision trees for classification using Python and the Penguin dataset."
  },
  {
    "objectID": "posts/2023-03-dectree_python/index.html#modeling-using-decision-trees",
    "href": "posts/2023-03-dectree_python/index.html#modeling-using-decision-trees",
    "title": "Model 2 - Decision Trees in Python with Penguin dataset",
    "section": "Modeling using Decision Trees",
    "text": "Modeling using Decision Trees\nNow let’s train our decision tree model to predict the species of penguins based on the independent variables we identified in the previous section.\nThe first step in model training is to split the dataset into training and testing sets. We used the train_test_split function from scikit-learn library to randomly split the data into 80% for training and 20% for testing. This step is important to ensure that the model is not overfitting to the data, meaning that it is not just memorizing the training data but can generalize well to new, unseen data.\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nNext, we built the decision tree model using the DecisionTreeClassifier function from scikit-learn library. We set the maximum depth of the tree to 3 and used a random state of 42 to ensure that our results are reproducible. The max_depth parameter controls the complexity of the tree, and we chose 3 to balance between underfitting and overfitting.\nAfter building the model, we made predictions on the test set using the predict function of the decision tree classifier. These predictions will be used to evaluate the performance of our model in the next section.\n\n\n# Build the decision tree model\ntree = DecisionTreeClassifier(max_depth=3, random_state=42)\ntree.fit(X_train, y_train)\n\n# Make predictions on the test set\n\nDecisionTreeClassifier(max_depth=3, random_state=42)\n\ny_pred = tree.predict(X_test)"
  },
  {
    "objectID": "posts/2023-03-dectree_python/index.html#performance",
    "href": "posts/2023-03-dectree_python/index.html#performance",
    "title": "Model 2 - Decision Trees in Python with Penguin dataset",
    "section": "Performance",
    "text": "Performance\nAfter building our decision tree model, it is important to evaluate its performance. We can use various metrics to evaluate the model, including accuracy, precision, recall, and the confusion matrix. Before getting to the code, let’s understand the metrics.\nThe accuracy score tells us the percentage of correctly classified instances out of all instances.\naccuracy = (number of correct predictions) / (total number of predictions)\nThe precision score tells us the percentage of instances that were correctly classified as a certain class out of all instances classified as that class.\nPrecision = (true positives) / (true positives + false positives)\nThe recall score tells us the percentage of instances that were correctly classified as a certain class out of all instances of that class.\nRecall = (true positives) / (true positives + false negatives)\nF1 score is the harmonic mean of precision and recall. It is a measure of the balance between precision and recall and is a useful metric when dealing with imbalanced datasets.\nF1 score = 2 * ((precision * recall) / (precision + recall))\nIn these formulas, true positives are the number of correctly predicted instances of a particular class, false positives are the number of instances that were incorrectly predicted as that class, and false negatives are the number of instances of that class that were incorrectly predicted as another class.\nThe confusion matrix is a table that shows the number of true positives, true negatives, false positives, and false negatives for each class.\n\n\n# Evaluate the model\nprint(\"Accuracy: {:.2f}%\".format(100 * accuracy_score(y_test, y_pred)))\n\nAccuracy: 97.01%\n\nprint(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred, average='weighted')))\n\nPrecision: 97.20%\n\nprint(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred, average='weighted')))\n\nRecall: 97.01%\n\nprint(\"F1 score: {:.2f}%\".format(100 * f1_score(y_test, y_pred, average='macro')))\n\nF1 score: 97.00%\n\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n\nConfusion Matrix:\n [[31  0  0]\n [ 2 16  0]\n [ 0  0 18]]\n\n\nWe can see that our model has an accuracy of 97%, which means it correctly classified 97% of instances. The precision and recall scores are also high, indicating that our model performed well in classifying instances for each class. The confusion matrix provides us with more detailed information about the number of correctly and incorrectly classified instances for each class.\nLet’s take a look at the decision tree we made.\n\n\n# Visualize the decision tree\ndot_data = export_graphviz(tree, out_file=None, \n                           feature_names=data_final_vars,  \n                           class_names=['Adelie', 'Chinstrap', 'Gentoo'],  \n                           filled=True, rounded=True,  \n                           special_characters=True)  \n\ngraph = graphviz.Source(dot_data)  \n\n# create the plot as a png file on your directory\n#graph.render(\"penguins_decision_tree.png\") \n\n\n\n\n\nEach box is called ‘node’, the node shows the rules and the subsets created, it comes together with Gini index, so we can track how pure each node gets.\nLet’s take a better look at the Gini index: \\[\nGini = 1 - \\sum_{i=1}^c (p_i)^2\n\\]\nWhere:\n\n\\(c\\) is the number of classes in the target variable\n\\(p_i\\) is the proportion of observations belonging to class i in the node\n\nThe Gini index ranges from 0 to 1, where a Gini index of 0 means all observations in the node belong to the same class (pure node), and a Gini index of 1 means that the node contains an equal proportion of observations from each class (impure node). In general, a lower Gini index indicates a better split for the decision tree.\nYou got it! We’ve covered a lot of ground in this post, from exploring our dataset and identifying key features, to preprocessing our data and training a decision tree model to predict penguin species. And the best part? Our model performed really well. Next we will talk about random forest, until then, keep coding!"
  },
  {
    "objectID": "posts/2023-03-dectree_python/index.html#dataset-description",
    "href": "posts/2023-03-dectree_python/index.html#dataset-description",
    "title": "Model 2 - Decision Trees in Python with Penguin dataset",
    "section": "Dataset Description",
    "text": "Dataset Description\nThe Penguin dataset is a well-known dataset that contains information about the physical characteristics and species of penguins. It consists of 344 samples with 8 input features and 1 target variable (the species of the penguin). In this section, we will describe the structure of the dataset and the importance of data preprocessing in the context of decision trees.\nThe first step in any machine learning project is to load and explore the data. To load the Palmer Penguins dataset, we will use the load_penguins function from the palmerpenguins library. We will also import pandas to create a dataframe to store the data, and seaborn and matplotlib to visualize the data.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.tree import export_graphviz\nimport graphviz\nimport seaborn as sns \nfrom palmerpenguins import load_penguins\nfrom sklearn.preprocessing import StandardScaler\n\n\nimport pandas as pd\nimport seaborn as sns \nfrom palmerpenguins import load_penguins\nsns.set_style('whitegrid')\npenguins = load_penguins()\npenguins.head()\n\n  species     island  bill_length_mm  ...  body_mass_g     sex  year\n0  Adelie  Torgersen            39.1  ...       3750.0    male  2007\n1  Adelie  Torgersen            39.5  ...       3800.0  female  2007\n2  Adelie  Torgersen            40.3  ...       3250.0  female  2007\n3  Adelie  Torgersen             NaN  ...          NaN     NaN  2007\n4  Adelie  Torgersen            36.7  ...       3450.0  female  2007\n\n[5 rows x 8 columns]\n\n\nThe head function is used to display the first few rows of the dataset. This is useful to check that the data has been loaded correctly and to get a quick overview of the data. Now let’s look for missing values.\n\n\n# Check for missing values\nprint(penguins.isnull().sum())\n\nspecies               0\nisland                0\nbill_length_mm        2\nbill_depth_mm         2\nflipper_length_mm     2\nbody_mass_g           2\nsex                  11\nyear                  0\ndtype: int64\n\n\nwe don’t always just drop na, but sckitlearn classifiers aren’t able to handle missing values plus we will lose only a few rows, so let’s take the easy way out here. An alternative could be `filling NA`, but it can be dangerous specially if you rely in simple methods like, just using the average value to fill the gaps. Note that, in the real world, we usually deal with many more missing values and the answer could be trying to enrich the dataset with external information, testing another classification model which can deal with missing values or checking more advanced methods to fill the gaps, how would you like a post about that?\n\n# drop missing values \npenguins = penguins.dropna()\n# Double Check for missing values\nprint(penguins.isnull().sum())\n\nspecies              0\nisland               0\nbill_length_mm       0\nbill_depth_mm        0\nflipper_length_mm    0\nbody_mass_g          0\nsex                  0\nyear                 0\ndtype: int64\n\n\nNow that we have checked for missing values, we can visualize the distribution of the target variable using a histogram. In this case, the target variable is the species of the penguin.\n\n# Plot the distribution of the target variable\nplt.hist(penguins['species'])\n\n(array([146.,   0.,   0.,   0.,   0., 119.,   0.,   0.,   0.,  68.]), array([0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8, 2. ]), <BarContainer object of 10 artists>)\n\nplt.show()\n\n\n\n\n\n\n\nWe can see that the dataset contains three species of penguins: Adelie, Chinstrap, and Gentoo.\nFinally, we can use a scatter matrix plot to visualize the pairwise relationships between the features. This is useful to understand the relationships between the features and to identify any potential correlations, or even if there’s a variable worth removing.\n\n# Plot the pairwise relationships between the features\nsns.pairplot(data=penguins, hue='species')\n\n\n\n\n\n\n\nThis is a very revealing plot, we can see, for example, that some features are very correlated, meaning that we could probably remove flipper_length_mm for example, and leave body_mass, because they are correlated, you can see what I’m talking about if you look to the graph at row 3 column 4. We also can see how the features interact with the species of the penguins, look at row 4 column 4, for example and you will find out that most of Gentoo penguins are heavier than the others, this indicates that body mass may be important predictor for our decision tree model.\nOk, now we’re read to prep the data"
  },
  {
    "objectID": "posts/2023-04-daily_life_ds/index.html",
    "href": "posts/2023-04-daily_life_ds/index.html",
    "title": "Model 3 - Random Forest in R with Breast Cancer Wisconsin (Diagnostic) dataset",
    "section": "",
    "text": "Welcome to the third instalment in our series of posts designed to introduce statistical and machine learning models. Having explored Linear Models (LM) and Decision Trees in our previous posts, we are now moving on to Random Forests.\nIn this tutorial, we will apply a Random Forest model to the Breast Cancer Wisconsin (Diagnostic) dataset. The aim is to predict whether a breast mass is malignant or benign based on measurements from cell nuclei. Let’s get started with a brief description of each attribute:"
  },
  {
    "objectID": "posts/2023-04-daily_life_ds/index.html#conclusion",
    "href": "posts/2023-04-daily_life_ds/index.html#conclusion",
    "title": "Data Detective Chronicles: Solving the Puzzles of Daily Data Science",
    "section": "Conclusion",
    "text": "Conclusion\nJoining datasets without a proper key column can be a challenging task for data scientists. However, by using string similarity measures like the Jaro-Winkler distance, you can tackle this issue and find the best matches between columns with slight discrepancies. Stay tuned for the next instalment of our series, where we will continue to explore the daily challenges faced by data scientists and share\nSee ya!"
  },
  {
    "objectID": "posts/2023-04-daily_life_ds2/index.html",
    "href": "posts/2023-04-daily_life_ds2/index.html",
    "title": "DDC: Tackling Missing or Inconsistent Data",
    "section": "",
    "text": "In the world of data science, dealing with missing or inconsistent data is an everyday challenge. The quality of your insights, predictions, and models heavily depends on the quality of the data you use. In this second post of our series on data science daily life challenges, we’ll explore various strategies for handling missing or inconsistent data using R, and how to make informed decisions about the best approach for your specific situation.\n\nUnderstand the nature of the missing or inconsistent data\n\nBefore diving into any solutions, it’s essential to understand the nature of the missing or inconsistent data you’re dealing with. In R, you can use the summary() function to get an overview of your dataset, including the number of missing values:\n\n# load these packages\n#library(dplyr)\n#library(stringdist)\n#library(tidyverse)\n#library(mice)\n#library(DMwR2)\n\ndataset <- data.frame(col1 = c(1:3, NA),\n                 col2 = c(\"one\", NA,\"cool\", \"text\"), \n                 col3 = c(TRUE, FALSE, TRUE, TRUE), \n                 col4 = c(0.5, 4.7, 3.2, NA),\n                 date_column = c(\"2000/1/1\",\"2000/2/1\" ,\"2000/3/1\" ,\"2023/13/40\"),                 stringsAsFactors = FALSE)\n\nsummary(dataset)\n\n      col1         col2              col3              col4     \n Min.   :1.0   Length:4           Mode :logical   Min.   :0.50  \n 1st Qu.:1.5   Class :character   FALSE:1         1st Qu.:1.85  \n Median :2.0   Mode  :character   TRUE :3         Median :3.20  \n Mean   :2.0                                      Mean   :2.80  \n 3rd Qu.:2.5                                      3rd Qu.:3.95  \n Max.   :3.0                                      Max.   :4.70  \n NA's   :1                                        NA's   :1     \n date_column       \n Length:4          \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n\n\nData Imputation\n\nOne common approach for dealing with missing data is imputation. Imputation involves estimating the missing values based on other available data. Some popular imputation methods in R include:\n\nMean, median, or mode imputation: Replace missing values with the mean, median, or mode of the column.\n\n\ndataset <- dataset %>%\n  mutate(col4 = if_else(is.na(col4), mean(col4, na.rm = TRUE), col4))\n\n\nLinear regression imputation: Use a linear regression model to estimate missing values based on other variables in the dataset.\n\n\nimputed_data <- mice(dataset, method = 'norm.predict', m = 5)\n\n\n iter imp variable\n  1   1  col1\n  1   2  col1\n  1   3  col1\n  1   4  col1\n  1   5  col1\n  2   1  col1\n  2   2  col1\n  2   3  col1\n  2   4  col1\n  2   5  col1\n  3   1  col1\n  3   2  col1\n  3   3  col1\n  3   4  col1\n  3   5  col1\n  4   1  col1\n  4   2  col1\n  4   3  col1\n  4   4  col1\n  4   5  col1\n  5   1  col1\n  5   2  col1\n  5   3  col1\n  5   4  col1\n  5   5  col1\n\ncomplete_data <- complete(imputed_data)\ncomplete_data\n\n      col1 col2  col3 col4 date_column\n1 1.000000  one  TRUE  0.5    2000/1/1\n2 2.000000 <NA> FALSE  4.7    2000/2/1\n3 3.000000 cool  TRUE  3.2    2000/3/1\n4 2.703704 text  TRUE  2.8  2023/13/40\n\n\n\nK-Nearest Neighbours (KNN) imputation: Fill in missing values by averaging the values of the k-nearest neighbours. I will give an example of the code below, but you need a bigger dataset for that approach, that’s why the code is commented.\n\n\n#imputed_data <- knnImputation(dataset, k = 5)\n\nIt’s important to note that imputation can introduce bias or distort the underlying data distribution, so it should be used with caution.\n\nRemoving missing or inconsistent data\n\nIn some cases, it may be appropriate to remove rows or columns containing missing or inconsistent data. This can be done using techniques such as:\n\nListwise deletion: Remove any rows containing missing values.\n\n\ndataset <- na.omit(dataset)\n\n\n\nDropping columns: Remove columns with a high proportion of missing or inconsistent data.\n\ncolumn_with_missing_data <- sapply(dataset,function(x)sum(is.na(x)))\ncolumn_with_missing_data <- column_with_missing_data[column_with_missing_data == 0]\n\ndataset <- dataset %>% select(-column_with_missing_data)\ndataset\n\n  col1 col2 col3 col4 date_column\n1    1  one TRUE  0.5    2000/1/1\n3    3 cool TRUE  3.2    2000/3/1\n\n\nKeep in mind that removing data can lead to loss of information and may introduce bias if the data is not missing at random.\n\nData Standardisation and Transformation\n\nInconsistent data often results from variations in data entry, formats, or units. To address this issue, you can standardise and transform the data using R functions like:\n\nEstablishing consistent formats for dates ( in case it is of type character and there’s inconsistences like “13/40/2023” the return would be NA, so it will help you to recognise inconsistences.\n\n\ndataset$date_column <- as.Date(dataset$date_column, format = \"%Y/%m/%d\")\ndataset\n\n  col1 col2 col3 col4 date_column\n1    1  one TRUE  0.5  2000-01-01\n3    3 cool TRUE  3.2  2000-03-01\n\n\nDealing with missing or inconsistent data is a common challenge for data scientists, but it’s also an opportunity to refine your skills and improve your dataset’s quality. By using R to understand the nature of the missing or inconsistent data and applying appropriate strategies, you can make more informed decisions and produce more accurate and reliable insights. In the next post of our series on data science daily life challenges, we’ll explore the intricacies of handling high-dimensional data and the techniques used to simplify analyses using R. Stay tuned!"
  },
  {
    "objectID": "posts/2023-04-daily_life_ds2/index.html#conclusion",
    "href": "posts/2023-04-daily_life_ds2/index.html#conclusion",
    "title": "Data Detective Chronicles: Solving the Puzzles of Daily Data Science",
    "section": "Conclusion",
    "text": "Conclusion\nJoining datasets without a proper key column can be a challenging task for data scientists. However, by using string similarity measures like the Jaro-Winkler distance, you can tackle this issue and find the best matches between columns with slight discrepancies. Stay tuned for the next instalment of our series, where we will continue to explore the daily challenges faced by data scientists and share\nSee ya!"
  },
  {
    "objectID": "posts/2023-04-daily_life_ds/index.html#building-our-random-forest-model",
    "href": "posts/2023-04-daily_life_ds/index.html#building-our-random-forest-model",
    "title": "Model 3 - Random Forest in R with Breast Cancer Wisconsin (Diagnostic) dataset",
    "section": "Building Our Random Forest Model",
    "text": "Building Our Random Forest Model\nNow that we’ve got a thorough understanding of our data, let’s move forward and build our Random Forest model. In this step, we will use the randomForest package in R.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Split the data into training and test sets\ntraining_rows <- sample(1:nrow(BreastCancer), 0.7*nrow(BreastCancer))\ntraining_set <- BreastCancer[training_rows, -1]\ntest_set <- BreastCancer[-training_rows,-1 ]\n\n# Train the model\n rf_model <- cforest(Class ~ ., data = training_set, controls=cforest_unbiased(ntree=5, mtry=3))\n\n# Print the model\nrf_model\n\n\n     Random Forest using Conditional Inference Trees\n\nNumber of trees:  5 \n\nResponse:  Class \nInputs:  Cl.thickness, Cell.size, Cell.shape, Marg.adhesion, Epith.c.size, Bare.nuclei, Bl.cromatin, Normal.nucleoli, Mitoses \nNumber of observations:  489 \n\n\nThe randomForest() function builds the Random Forest model, with the formula Class ~ . telling it to use all other variables to predict Class. The ntree argument is the number of trees to grow in the forest - in this case, 10. The importance argument, when set to TRUE, computes a measure of variable importance.\nNow, let’s evaluate our model’s performance on the test data.\n\n# Make predictions on the test data\npredictions <- predict(rf_model, newdata = test_set)\n\n# Print confusion matrix\ntable(observed = test_set$Class, predicted = predictions)\n\n           predicted\nobserved    benign malignant\n  benign       143         7\n  malignant      0        60\n\n\nThe confusion matrix gives us a detailed breakdown of the model’s performance. The diagonal elements represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabelled by the classifier.\nIn addition to a confusion matrix, there are several other metrics and methods that can provide a more detailed understanding of a model’s performance. These include:\n\n\nROC Curve and AUC: Receiver Operating Characteristic (ROC) curve is a plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold is varied. Area Under the Curve (AUC) provides an aggregate measure of performance across all possible classification thresholds. An AUC of 1 indicates a perfect classifier.\n\n\npred_prob <- predict(rf_model, newdata = test_set, type = \"prob\")\npred_prob <- sapply(pred_prob, `[[`, 2)\nroc_obj <- roc(test_set$Class, pred_prob)\n\nSetting levels: control = benign, case = malignant\n\n\nSetting direction: controls < cases\n\ncat(\"AUC: \", auc(roc_obj), \"\\n\")\n\nAUC:  0.9866667 \n\n\n\nplot(roc_obj)\n\n\n\n\n\n\n\n\n\nCross-validation: This technique provides a better assessment of the model’s expected performance on unseen data by reducing the variance associated with a single train-test split.\n\n\n# Load necessary library\nlibrary(caret)\n\nCarregando pacotes exigidos: lattice\n\n# Define training control\ntrain_control <- trainControl(method = \"cv\", number = 10)\n\n# Train the model using cross-validation\nrf_model2 <- train(Class ~ ., data = BreastCancer[,-1] %>% na.omit(), trControl = train_control, method = \"rf\")\n\n# Print the model\nprint(rf_model2)\n\nRandom Forest \n\n683 samples\n  9 predictor\n  2 classes: 'benign', 'malignant' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 615, 615, 614, 614, 615, 615, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n   2    0.9647679  0.9232216\n  41    0.9633186  0.9200402\n  80    0.9574576  0.9065592\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.\n\n\nLastly, we can inspect the importance of each feature in making predictions.\n\n# Get variable importance\nvarimp(rf_model)\n\n   Cl.thickness       Cell.size      Cell.shape   Marg.adhesion    Epith.c.size \n    0.000000000     0.054748603     0.005586592     0.000000000     0.011173184 \n    Bare.nuclei     Bl.cromatin Normal.nucleoli         Mitoses \n    0.026815642     0.079329609     0.016759777     0.000000000 \n\n\nEach row of the output corresponds to one of the predictor variables. The higher the value, the more important the predictor variable in classifying the tumour as benign or malignant.\nVariable importance gives us insight into which features are most influential in predicting the target variable in our Random Forest model. It’s an important tool for understanding and interpreting the model. So cell size and shape are the strongest variables for prediction.\nAnd there we have it - a complete Random Forest model in R, built on the Breast Cancer Wisconsin (Diagnostic) dataset. In this tutorial, we’ve covered a lot of ground, from the initial exploratory data analysis through to the creation and evaluation of our model. We hope you’ve found it helpful and enlightening!\nIn our upcoming posts, we’ll dive into more sophisticated machine learning algorithms and their applications. Stay tuned!"
  },
  {
    "objectID": "posts/2023-05-randomforest/index.html",
    "href": "posts/2023-05-randomforest/index.html",
    "title": "DDC: Tackling Missing or Inconsistent Data",
    "section": "",
    "text": "In the world of data science, dealing with missing or inconsistent data is an everyday challenge. The quality of your insights, predictions, and models heavily depends on the quality of the data you use. In this second post of our series on data science daily life challenges, we’ll explore various strategies for handling missing or inconsistent data using R, and how to make informed decisions about the best approach for your specific situation.\n\nUnderstand the nature of the missing or inconsistent data\n\nBefore diving into any solutions, it’s essential to understand the nature of the missing or inconsistent data you’re dealing with. In R, you can use the summary() function to get an overview of your dataset, including the number of missing values:\n\n# load these packages\n#library(dplyr)\n#library(stringdist)\n#library(tidyverse)\n#library(mice)\n#library(DMwR2)\n\ndataset <- data.frame(col1 = c(1:3, NA),\n                 col2 = c(\"one\", NA,\"cool\", \"text\"), \n                 col3 = c(TRUE, FALSE, TRUE, TRUE), \n                 col4 = c(0.5, 4.7, 3.2, NA),\n                 date_column = c(\"2000/1/1\",\"2000/2/1\" ,\"2000/3/1\" ,\"2023/13/40\"),                 stringsAsFactors = FALSE)\n\nsummary(dataset)\n\n      col1         col2              col3              col4     \n Min.   :1.0   Length:4           Mode :logical   Min.   :0.50  \n 1st Qu.:1.5   Class :character   FALSE:1         1st Qu.:1.85  \n Median :2.0   Mode  :character   TRUE :3         Median :3.20  \n Mean   :2.0                                      Mean   :2.80  \n 3rd Qu.:2.5                                      3rd Qu.:3.95  \n Max.   :3.0                                      Max.   :4.70  \n NA's   :1                                        NA's   :1     \n date_column       \n Length:4          \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n\n\nData Imputation\n\nOne common approach for dealing with missing data is imputation. Imputation involves estimating the missing values based on other available data. Some popular imputation methods in R include:\n\nMean, median, or mode imputation: Replace missing values with the mean, median, or mode of the column.\n\n\ndataset <- dataset %>%\n  mutate(col4 = if_else(is.na(col4), mean(col4, na.rm = TRUE), col4))\n\n\nLinear regression imputation: Use a linear regression model to estimate missing values based on other variables in the dataset.\n\n\nimputed_data <- mice(dataset, method = 'norm.predict', m = 5)\n\n\n iter imp variable\n  1   1  col1\n  1   2  col1\n  1   3  col1\n  1   4  col1\n  1   5  col1\n  2   1  col1\n  2   2  col1\n  2   3  col1\n  2   4  col1\n  2   5  col1\n  3   1  col1\n  3   2  col1\n  3   3  col1\n  3   4  col1\n  3   5  col1\n  4   1  col1\n  4   2  col1\n  4   3  col1\n  4   4  col1\n  4   5  col1\n  5   1  col1\n  5   2  col1\n  5   3  col1\n  5   4  col1\n  5   5  col1\n\ncomplete_data <- complete(imputed_data)\ncomplete_data\n\n      col1 col2  col3 col4 date_column\n1 1.000000  one  TRUE  0.5    2000/1/1\n2 2.000000 <NA> FALSE  4.7    2000/2/1\n3 3.000000 cool  TRUE  3.2    2000/3/1\n4 2.703704 text  TRUE  2.8  2023/13/40\n\n\n\nK-Nearest Neighbours (KNN) imputation: Fill in missing values by averaging the values of the k-nearest neighbours. I will give an example of the code below, but you need a bigger dataset for that approach, that’s why the code is commented.\n\n\n#imputed_data <- knnImputation(dataset, k = 5)\n\nIt’s important to note that imputation can introduce bias or distort the underlying data distribution, so it should be used with caution.\n\nRemoving missing or inconsistent data\n\nIn some cases, it may be appropriate to remove rows or columns containing missing or inconsistent data. This can be done using techniques such as:\n\nListwise deletion: Remove any rows containing missing values.\n\n\ndataset <- na.omit(dataset)\n\n\n\nDropping columns: Remove columns with a high proportion of missing or inconsistent data.\n\ncolumn_with_missing_data <- sapply(dataset,function(x)sum(is.na(x)))\ncolumn_with_missing_data <- column_with_missing_data[column_with_missing_data == 0]\n\ndataset <- dataset %>% select(-column_with_missing_data)\ndataset\n\n  col1 col2 col3 col4 date_column\n1    1  one TRUE  0.5    2000/1/1\n3    3 cool TRUE  3.2    2000/3/1\n\n\nKeep in mind that removing data can lead to loss of information and may introduce bias if the data is not missing at random.\n\nData Standardisation and Transformation\n\nInconsistent data often results from variations in data entry, formats, or units. To address this issue, you can standardise and transform the data using R functions like:\n\nEstablishing consistent formats for dates ( in case it is of type character and there’s inconsistences like “13/40/2023” the return would be NA, so it will help you to recognise inconsistences.\n\n\ndataset$date_column <- as.Date(dataset$date_column, format = \"%Y/%m/%d\")\ndataset\n\n  col1 col2 col3 col4 date_column\n1    1  one TRUE  0.5  2000-01-01\n3    3 cool TRUE  3.2  2000-03-01\n\n\nDealing with missing or inconsistent data is a common challenge for data scientists, but it’s also an opportunity to refine your skills and improve your dataset’s quality. By using R to understand the nature of the missing or inconsistent data and applying appropriate strategies, you can make more informed decisions and produce more accurate and reliable insights. In the next post of our series on data science daily life challenges, we’ll explore the intricacies of handling high-dimensional data and the techniques used to simplify analyses using R. Stay tuned!"
  }
]