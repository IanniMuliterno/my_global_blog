[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ianní Muliterno",
    "section": "",
    "text": "Machine learning\n\n\nStatistics\n\n\nData Science\n\n\nLinear Regression\n\n\nPython\n\n\nKaggle\n\n\n \n\n\n\n\nMar 6, 2023\n\n\nI. Muliterno\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine learning\n\n\nStatistics\n\n\nData Science\n\n\n \n\n\n\n\nMar 1, 2023\n\n\nIanní Muliterno\n\n\n4 min\n\n\n\n\n\n\nNo matching items\n\n\n See all"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n \n\n\n\nModel 1 - Linear Regression in Python with Kaggle Data\n\n\n\nMar 6, 2023\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nModeling Approaches\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-03-models_and_applications/index.html",
    "href": "posts/2023-03-models_and_applications/index.html",
    "title": "Modeling Approaches",
    "section": "",
    "text": "As a senior data scientist, one of the key responsibilities is to identify the right modeling approach for a given problem. Different modeling approaches have different strengths and weaknesses, and choosing the right approach is crucial to building an effective and accurate model. This post is the first in a series of posts on modeling approaches, I will discuss the strengths and weaknesses of some popular modeling approaches and when to apply different combinations.\nThis post is the first in a series of posts on modeling approaches"
  },
  {
    "objectID": "posts/2023-03-models_and_applications/index.html#linear-regression",
    "href": "posts/2023-03-models_and_applications/index.html#linear-regression",
    "title": "Modeling Approaches",
    "section": "1. Linear Regression",
    "text": "1. Linear Regression\nLinear regression is a popular modeling approach used for predicting continuous values. It is a simple yet powerful modeling approach that works well when the relationship between the predictor and response variable is linear. Linear regression models are easy to interpret and can be used for both simple and complex problems.\n\nStrengths:\n\nEasy to interpret and explain.\nWorks well when the relationship between predictor and response variable is linear.\nCan handle both simple and complex problems.\n\n\n\nWeaknesses:\n\nAssumes a linear relationship between predictor and response variable.\nSensitive to outliers and multicollinearity.\n\n\n\nWhen to use:\n\nWhen the relationship between predictor and response variable is linear.\nWhen the data has few predictors.\nWhen the data has no multicollinearity or outliers."
  },
  {
    "objectID": "posts/2023-03-models_and_applications/index.html#decision-trees",
    "href": "posts/2023-03-models_and_applications/index.html#decision-trees",
    "title": "Modeling Approaches",
    "section": "2. Decision Trees",
    "text": "2. Decision Trees\nDecision trees are a popular modeling approach used for classification and regression. They work by partitioning the data into smaller subsets based on the values of the predictors. Decision trees are easy to interpret and can handle both categorical and continuous predictors. Personally, I prefer to apply only for categorical predictors.\n\nStrengths:\n\nEasy to interpret and explain.\nCan handle both categorical and continuous predictors.\nCan handle interactions between predictors.\n\n\n\nWeaknesses:\n\nCan overfit the data.\nSensitive to small changes in the data.\n\n\n\nWhen to use:\n\nWhen the data has many predictors.\nWhen the data has interactions between predictors.\nWhen the data has both categorical and continuous predictors."
  },
  {
    "objectID": "posts/2023-03-models_and_applications/index.html#random-forests",
    "href": "posts/2023-03-models_and_applications/index.html#random-forests",
    "title": "Modeling Approaches",
    "section": "3. Random Forests",
    "text": "3. Random Forests\nRandom forests are an extension of decision trees that work by combining multiple decision trees to make a final prediction. They are a popular modeling approach used for classification and regression. Random forests are robust to overfitting and can handle both categorical and continuous predictors.\n\nStrengths:\n\nRobust to overfitting.\nCan handle both categorical and continuous predictors.\nCan handle interactions between predictors.\n\n\n\nWeaknesses:\n\nCan be slow to train and predict.\nCan be difficult to interpret.\n\n\n\nWhen to use:\n\nWhen the data has many predictors.\nWhen the data has interactions between predictors.\nWhen the data has both categorical and continuous predictors.\nWhen the data has outliers or missing values."
  },
  {
    "objectID": "posts/2023-03-models_and_applications/index.html#neural-networks",
    "href": "posts/2023-03-models_and_applications/index.html#neural-networks",
    "title": "Modeling Approaches",
    "section": "4. Neural Networks",
    "text": "4. Neural Networks\nNeural networks are a popular modeling approach used for classification and regression. They work by mimicking the structure of the human brain to identify complex patterns in the data. Neural networks are powerful and can handle both linear and nonlinear relationships between predictors and response variables.\n\nStrengths:\n\nCan handle both linear and nonlinear relationships between predictors and response variables.\nCan identify complex patterns in the data.\nCan handle large and complex datasets.\n\n\n\nWeaknesses:\n\nCan be slow to train and predict.\nCan overfit the data.\n\n\n\nWhen to use:\n\nWhen the data has many predictors.\nWhen the data has complex relationships between predictors and response variables.\nWhen the data has large and complex datasets."
  },
  {
    "objectID": "posts/2023-03-models_and_applications/index.html#support-vector-machines",
    "href": "posts/2023-03-models_and_applications/index.html#support-vector-machines",
    "title": "Modeling Approaches",
    "section": "5. Support Vector Machines",
    "text": "5. Support Vector Machines\nSupport Vector Machines (SVMs) are a popular modeling approach used for classification and regression. They work by finding the hyperplane that best separates the data into different classes. SVMs are powerful and can handle both linear and nonlinear relationships between predictors and response variables.\n\nStrengths:\n\nCan handle both linear and nonlinear relationships between predictors and response variables.\nCan handle high-dimensional datasets.\nCan handle outliers.\n\n\n\nWeaknesses:\n\nCan be sensitive to the choice of kernel function.\nCan be slow to train and predict.\n\n\n\nWhen to use:\n\nWhen the data has many predictors.\n\n\n\nWhen the data has both linear and nonlinear relationships between predictors and response variables.\nWhen the data has outliers.\n\nIn practice, different modeling approaches may be combined to build more accurate and robust models. For example, a decision tree model may be combined with a random forest model to improve accuracy and reduce overfitting. Or, a linear regression model may be combined with a support vector machine model to handle both linear and nonlinear relationships between predictors and response variables.\nWhen choosing a modeling approach or combination of approaches, it is important to consider the strengths and weaknesses of each approach in relation to the specific problem at hand. It is also important to consider factors such as the size and complexity of the data, the level of interpretability required, and the computational resources available.\nIn summary, there are a variety of modeling approaches that can be used in data science, each with its own strengths and weaknesses. By carefully considering the specific problem at hand and choosing the right combination of modeling approaches, data scientists can build accurate and robust models that provide valuable insights and predictions."
  },
  {
    "objectID": "posts/2023-03-lm_python/index.html",
    "href": "posts/2023-03-lm_python/index.html",
    "title": "Model 1 - Linear Regression in Python with Kaggle Data",
    "section": "",
    "text": "In our previous post, we discussed different modeling approaches and their applications. Today, we will delve deeper into linear regression, one of the most commonly used modeling techniques in data science. By the end, you will learn when to use linear regression and how to code it from start to finish using a dataset from Kaggle."
  },
  {
    "objectID": "posts/2023-03-lm_python/index.html#getting-to-know-linear-regression",
    "href": "posts/2023-03-lm_python/index.html#getting-to-know-linear-regression",
    "title": "Model 1 - Linear Regression in Python with Kaggle Data",
    "section": "1. Getting to know Linear Regression",
    "text": "1. Getting to know Linear Regression\nLinear regression is a popular modeling approach used for predicting continuous values. It is a simple yet powerful modeling approach that works well when the relationship between the predictor and response variable is linear. It is important to ensure that the assumptions of linear regression are met, including linearity, independence, normality, and equal variance. These assumptions can be tested using various techniques, such as residual plots and statistical tests. Linear regression models are easy to interpret and can be used for both simple and complex problems.\nSimple linear regression involves a single independent variable, while multiple linear regression involves two or more independent variables. Examples of where linear regression is commonly used include predicting housing prices, analyzing stock prices, and estimating crop yields."
  },
  {
    "objectID": "posts/2023-03-lm_python/index.html#data-cleaning-and-visualization",
    "href": "posts/2023-03-lm_python/index.html#data-cleaning-and-visualization",
    "title": "Model 1 - Linear Regression in Python with Kaggle Data",
    "section": "2. Data Cleaning and Visualization",
    "text": "2. Data Cleaning and Visualization\nWhen working with real-world data, it is common to encounter missing or erroneous values, inconsistent formatting, and other issues. Data cleaning is the process of detecting and correcting these problems in the data to ensure that it is accurate and reliable for analysis. Visualization, on the other hand, is the process of representing data in a visual format such as graphs, charts, or maps, to help analysts identify patterns and trends.\nFor this blog post, we will be using a data set from Kaggle’s House Prices: Advanced Regression Techniques competition. This data set contains information on various attributes of residential homes in Ames, Iowa, including their sale prices. The goal of the competition is to build a model that can accurately predict the sale prices of homes based on these attributes.\nTo start, we will import the necessary libraries in Python, including Pandas for data manipulation and Matplotlib for visualization. We will then load the data set using the read_csv() function from Pandas.\n\n\n\nNow that we have loaded the data, we can begin the data cleaning process. The first step is to check for missing values in the data. We can use the isnull() function from Pandas to check for missing values and the sum() function to count the number of missing values in each column.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# remove the comment to load the dataset\n#df = pd.read_csv(\"train.csv\")\n\n# Check for missing values\nmissing_values = df.isnull().sum()\nprint(missing_values)\n\nId                 0\nMSSubClass         0\nMSZoning           0\nLotFrontage      259\nLotArea            0\n                ... \nMoSold             0\nYrSold             0\nSaleType           0\nSaleCondition      0\nSalePrice          0\nLength: 81, dtype: int64\n\n\nThis will give us a count of missing values in each column of the data set. We can then decide how to handle these missing values, depending on the amount of missing data and the nature of the problem. In this case, we will simply drop the columns with more than 50% missing values.\nNext, we can check for any duplicate rows in the data set using the duplicated() function from Pandas. If there are any duplicate rows, we can drop them using the drop_duplicates() function.\n\n\n\n# Drop columns with more than 50% missing values\ndf = df.dropna(thresh=len(df)*0.5, axis=1)\n\n# Check for duplicates\nduplicates = df.duplicated()\nprint(duplicates.sum())\n\n# Drop duplicates\n\n0\n\ndf = df.drop_duplicates()\n\nNow that we have cleaned the data, we can move on to visualization. One common visualization for exploring the relationship between two variables is a scatter plot. We can create a scatter plot of the sale prices and the living area of the homes using Matplotlib.\n\n# Create a histogram of sale prices\nplt.hist(df[\"SalePrice\"], bins=20)\n\n(array([ 22., 126., 380., 343., 229., 144.,  86.,  49.,  28.,  23.,  12.,\n         7.,   3.,   1.,   2.,   1.,   2.,   0.,   0.,   2.]), array([ 34900.,  70905., 106910., 142915., 178920., 214925., 250930.,\n       286935., 322940., 358945., 394950., 430955., 466960., 502965.,\n       538970., 574975., 610980., 646985., 682990., 718995., 755000.]), <BarContainer object of 20 artists>)\n\nplt.xlabel(\"Sale Price ($)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\n\n\n\n\nThis will give us a visual representation of the distribution of sale prices in the data set. We can see that the distribution is skewed to the right, with a few homes having very high sale prices.\nBy cleaning and visualizing the data, we can gain a better understanding of its properties and identify any potential issues that may need to be addressed before building a linear regression model."
  },
  {
    "objectID": "posts/2023-03-lm_python/index.html#summary",
    "href": "posts/2023-03-lm_python/index.html#summary",
    "title": "Model 1 - Linear Regression in Python with Kaggle Data",
    "section": "Summary",
    "text": "Summary\nIn this post, we have explored the process of using linear regression to predict the sale prices of homes based on their attributes. We started by cleaning and visualizing the data to gain insights into the relationships between the input variables and the sale prices. We then built a linear regression model using the Scikit-learn library and evaluated its performance using the mean squared error and coefficient of determination.\nBy following this process, we can make accurate predictions about the sale prices of homes based on their attributes. This information can be valuable for a variety of applications, including real estate valuation, mortgage underwriting, and investment analysis.\nAs with any predictive model, it is important to continually evaluate and refine the model over time to ensure that it is accurately predicting the outcome of interest. With continued effort, we can refine our understanding of the relationship between the input variables and the sale prices of homes, and improve the accuracy of our predictions."
  },
  {
    "objectID": "posts/2023-03-lm_python/index.html#building-a-linear-regression-model",
    "href": "posts/2023-03-lm_python/index.html#building-a-linear-regression-model",
    "title": "Model 1 - Linear Regression in Python with Kaggle Data",
    "section": "3. Building a Linear Regression Model",
    "text": "3. Building a Linear Regression Model\nNow that we have cleaned and visualized the data, we can start building a linear regression model to predict the sale prices of homes based on their attributes. Linear regression is a statistical technique that is commonly used for predicting a numeric value based on one or more input variables. In this case, we will use the input variables in the data set to predict the sale price of each home.\nTo start, we will split the data set into a training set and a validation set using the train_test_split() function from Scikit-learn. The training set will be used to train the model, while the validation set will be used to evaluate the model’s performance.\n\n\nfrom sklearn.model_selection import train_test_split\n\n# Split the data set into training and validation sets\ntrain, val = train_test_split(df, test_size=0.2, random_state=42)\n\nNext, we will select the input variables that we want to use in the model. In this case, we will use the living area, number of bedrooms, and number of bathrooms as input variables.\n\n# Select the input variables\nX_train = train[[\"GrLivArea\", \"BedroomAbvGr\", \"FullBath\"]]\ny_train = train[\"SalePrice\"]\n\nX_val = val[[\"GrLivArea\", \"BedroomAbvGr\", \"FullBath\"]]\ny_val = val[\"SalePrice\"]\n\nWe can then build a linear regression model using the LinearRegression() function from Scikit-learn. We can use it to predict the sale prices of homes in the validation set using the predict() function.\n\n\nfrom sklearn.linear_model import LinearRegression\n\n# Build the linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predict the sale prices of homes in the validation set\n\nLinearRegression()\n\ny_pred = model.predict(X_val)\n\nTo evaluate the performance of the model, we can calculate the mean squared error (MSE) and the coefficient of determination (R-squared) between the predicted sale prices and the actual sale prices in the validation set using the mean_squared_error() and r2_score() functions from Scikit-learn.\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Calculate the mean squared error and R-squared\nmse = mean_squared_error(y_val, y_pred)\nr2 = r2_score(y_val, y_pred)\nprint(\"MSE:\", mse)\n\nMSE: 2806426667.247853\n\nprint(\"R-squared:\", r2)\n\nR-squared: 0.6341189942328371\n\n\nA low mean squared error and a high coefficient of determination indicate that the model is accurately predicting the sale prices of homes based on their attributes.\nBy building a linear regression model, we can use the input variables in the data set to predict the sale prices of homes with a high degree of accuracy. This information can be useful for real estate professionals, home buyers, and sellers looking to estimate the value of a residential property."
  }
]